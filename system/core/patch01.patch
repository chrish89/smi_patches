diff --git a/libpixelflinger/Android.mk b/libpixelflinger/Android.mk
index 484cf50..5541b39 100644
--- a/libpixelflinger/Android.mk
+++ b/libpixelflinger/Android.mk
@@ -6,25 +6,29 @@ include $(CLEAR_VARS)
 #
 
 include $(CLEAR_VARS)
-PIXELFLINGER_SRC_FILES:= \
+PIXELFLINGER_SRC_FILES += \
+    codeflinger/CodeCache.cpp \
+    codeflinger/tinyutils/SharedBuffer.cpp \
+    codeflinger/tinyutils/VectorImpl.cpp \
+    format.cpp \
+    clear.cpp \
+    raster.cpp \
+    buffer.cpp
+
+ifneq ($(TARGET_ARCH),x86)
+PIXELFLINGER_SRC_FILES += \
 	codeflinger/ARMAssemblerInterface.cpp \
 	codeflinger/ARMAssemblerProxy.cpp \
-	codeflinger/CodeCache.cpp \
 	codeflinger/GGLAssembler.cpp \
 	codeflinger/load_store.cpp \
 	codeflinger/blending.cpp \
 	codeflinger/texturing.cpp \
-	codeflinger/tinyutils/SharedBuffer.cpp \
-	codeflinger/tinyutils/VectorImpl.cpp \
 	fixed.cpp.arm \
 	picker.cpp.arm \
 	pixelflinger.cpp.arm \
 	trap.cpp.arm \
-	scanline.cpp.arm \
-	format.cpp \
-	clear.cpp \
-	raster.cpp \
-	buffer.cpp
+	scanline.cpp.arm
+endif
 
 PIXELFLINGER_CFLAGS := -fstrict-aliasing -fomit-frame-pointer
 
@@ -44,11 +48,27 @@ PIXELFLINGER_SRC_FILES_arm64 := \
 	arch-arm64/col32cb16blend.S \
 	arch-arm64/t32cb16blend.S \
 
+PIXELFLINGER_SRC_FILES_x86 := \
+	codeflinger/x86/X86Assembler.cpp \
+	codeflinger/x86/GGLX86Assembler.cpp \
+	codeflinger/x86/load_store.cpp \
+	codeflinger/x86/blending.cpp \
+	codeflinger/x86/texturing.cpp \
+	fixed.cpp \
+	picker.cpp \
+	pixelflinger.cpp \
+	trap.cpp \
+	scanline.cpp
+
 PIXELFLINGER_SRC_FILES_mips := \
 	codeflinger/MIPSAssembler.cpp \
 	codeflinger/mips_disassem.c \
 	arch-mips/t32cb16blend.S \
 
+PIXELFLINGER_C_INCLUDES_x86 := \
+	$(TARGET_OUT_HEADERS)/libenc
+PIXELFLINGER_STATIC_LIBRARIES_x86 := libenc
+
 #
 # Shared library
 #
@@ -57,8 +77,11 @@ LOCAL_MODULE:= libpixelflinger
 LOCAL_SRC_FILES := $(PIXELFLINGER_SRC_FILES)
 LOCAL_SRC_FILES_arm := $(PIXELFLINGER_SRC_FILES_arm)
 LOCAL_SRC_FILES_arm64 := $(PIXELFLINGER_SRC_FILES_arm64)
+LOCAL_SRC_FILES_x86 := $(PIXELFLINGER_SRC_FILES_x86)
 LOCAL_SRC_FILES_mips := $(PIXELFLINGER_SRC_FILES_mips)
 LOCAL_CFLAGS := $(PIXELFLINGER_CFLAGS)
+LOCAL_C_INCLUDES_x86 := $(PIXELFLINGER_C_INCLUDES_x86)
+LOCAL_STATIC_LIBRARIES_x86 := $(PIXELFLINGER_STATIC_LIBRARIES_x86)
 LOCAL_SHARED_LIBRARIES := libcutils liblog
 
 ifneq ($(BUILD_TINY_ANDROID),true)
@@ -78,8 +101,11 @@ LOCAL_MODULE:= libpixelflinger_static
 LOCAL_SRC_FILES := $(PIXELFLINGER_SRC_FILES)
 LOCAL_SRC_FILES_arm := $(PIXELFLINGER_SRC_FILES_arm)
 LOCAL_SRC_FILES_arm64 := $(PIXELFLINGER_SRC_FILES_arm64)
+LOCAL_SRC_FILES_x86 := $(PIXELFLINGER_SRC_FILES_x86)
 LOCAL_SRC_FILES_mips := $(PIXELFLINGER_SRC_FILES_mips)
 LOCAL_CFLAGS := $(PIXELFLINGER_CFLAGS)
+LOCAL_C_INCLUDES_x86 := $(PIXELFLINGER_C_INCLUDES_x86)
+LOCAL_STATIC_LIBRARIES_x86 := $(PIXELFLINGER_STATIC_LIBRARIES_x86)
 include $(BUILD_STATIC_LIBRARY)
 
 
diff --git a/libpixelflinger/codeflinger/x86/GGLX86Assembler.cpp b/libpixelflinger/codeflinger/x86/GGLX86Assembler.cpp
index e69de29..bd69fa2 100644
--- a/libpixelflinger/codeflinger/x86/GGLX86Assembler.cpp
+++ b/libpixelflinger/codeflinger/x86/GGLX86Assembler.cpp
@@ -0,0 +1,1508 @@
+/* libs/pixelflinger/codeflinger/x86/GGLX86Assembler.cpp
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#define LOG_TAG "GGLX86Assembler"
+
+#include <assert.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <sys/types.h>
+#include <cutils/log.h>
+
+#include "codeflinger/x86/GGLX86Assembler.h"
+
+namespace android {
+
+// ----------------------------------------------------------------------------
+
+GGLX86Assembler::GGLX86Assembler(const sp<Assembly>& assembly)
+    : X86Assembler(assembly), X86RegisterAllocator(), mOptLevel(7)
+{
+}
+
+GGLX86Assembler::~GGLX86Assembler()
+{
+}
+
+void GGLX86Assembler::reset(int opt_level)
+{
+    X86Assembler::reset();
+    X86RegisterAllocator::reset();
+    mOptLevel = opt_level;
+}
+
+// ---------------------------------------------------------------------------
+
+int GGLX86Assembler::scanline(const needs_t& needs, context_t const* c)
+{
+    int err = 0;
+    err = scanline_core(needs, c);
+    if (err != 0)
+        ALOGE("scanline_core failed probably due to running out of the registers: %d\n", err);
+
+    // XXX: in theory, pcForLabel is not valid before generate()
+    char* fragment_start_pc = pcForLabel("fragment_loop");
+    char* fragment_end_pc = pcForLabel("fragment_end");
+    const int per_fragment_ins_size = int(fragment_end_pc - fragment_start_pc);
+
+    // build a name for our pipeline
+    char name[128];
+    sprintf(name,
+            "scanline__%08X:%08X_%08X_%08X [%3d ipp ins size]",
+            needs.p, needs.n, needs.t[0], needs.t[1], per_fragment_ins_size);
+
+    if (err) {
+        ALOGE("Error while generating ""%s""\n", name);
+        disassemble(name);
+        return -1;
+    }
+
+    return generate(name);
+}
+
+int GGLX86Assembler::scanline_core(const needs_t& needs, context_t const* c)
+{
+    int64_t duration = ggl_system_time();
+
+    mBlendFactorCached = 0;
+    mBlending = 0;
+    mMasking = 0;
+    mAA        = GGL_READ_NEEDS(P_AA, needs.p);
+    mDithering = GGL_READ_NEEDS(P_DITHER, needs.p);
+    mAlphaTest = GGL_READ_NEEDS(P_ALPHA_TEST, needs.p) + GGL_NEVER;
+    mDepthTest = GGL_READ_NEEDS(P_DEPTH_TEST, needs.p) + GGL_NEVER;
+    mFog       = GGL_READ_NEEDS(P_FOG, needs.p) != 0;
+    mSmooth    = GGL_READ_NEEDS(SHADE, needs.n) != 0;
+    mBuilderContext.needs = needs;
+    mBuilderContext.c = c;
+    mBuilderContext.Rctx = obtainReg(); //dynamically obtain if used and then immediately recycle it if not used
+    mCbFormat = c->formats[ GGL_READ_NEEDS(CB_FORMAT, needs.n) ];
+
+    // ------------------------------------------------------------------------
+
+    decodeLogicOpNeeds(needs);
+
+    decodeTMUNeeds(needs, c);
+
+    mBlendSrc  = ggl_needs_to_blendfactor(GGL_READ_NEEDS(BLEND_SRC, needs.n));
+    mBlendDst  = ggl_needs_to_blendfactor(GGL_READ_NEEDS(BLEND_DST, needs.n));
+    mBlendSrcA = ggl_needs_to_blendfactor(GGL_READ_NEEDS(BLEND_SRCA, needs.n));
+    mBlendDstA = ggl_needs_to_blendfactor(GGL_READ_NEEDS(BLEND_DSTA, needs.n));
+
+    if (!mCbFormat.c[GGLFormat::ALPHA].h) {
+        if ((mBlendSrc == GGL_ONE_MINUS_DST_ALPHA) ||
+                (mBlendSrc == GGL_DST_ALPHA)) {
+            mBlendSrc = GGL_ONE;
+        }
+        if ((mBlendSrcA == GGL_ONE_MINUS_DST_ALPHA) ||
+                (mBlendSrcA == GGL_DST_ALPHA)) {
+            mBlendSrcA = GGL_ONE;
+        }
+        if ((mBlendDst == GGL_ONE_MINUS_DST_ALPHA) ||
+                (mBlendDst == GGL_DST_ALPHA)) {
+            mBlendDst = GGL_ONE;
+        }
+        if ((mBlendDstA == GGL_ONE_MINUS_DST_ALPHA) ||
+                (mBlendDstA == GGL_DST_ALPHA)) {
+            mBlendDstA = GGL_ONE;
+        }
+    }
+
+    // if we need the framebuffer, read it now
+    const int blending =    blending_codes(mBlendSrc, mBlendDst) |
+                            blending_codes(mBlendSrcA, mBlendDstA);
+
+    // XXX: handle special cases, destination not modified...
+    if ((mBlendSrc==GGL_ZERO) && (mBlendSrcA==GGL_ZERO) &&
+            (mBlendDst==GGL_ONE) && (mBlendDstA==GGL_ONE)) {
+        // Destination unmodified (beware of logic ops)
+    } else if ((mBlendSrc==GGL_ZERO) && (mBlendSrcA==GGL_ZERO) &&
+               (mBlendDst==GGL_ZERO) && (mBlendDstA==GGL_ZERO)) {
+        // Destination is zero (beware of logic ops)
+    }
+
+    int fbComponents = 0;
+    const int masking = GGL_READ_NEEDS(MASK_ARGB, needs.n);
+    for (int i=0 ; i<4 ; i++) {
+        const int mask = 1<<i;
+        component_info_t& info = mInfo[i];
+        int fs = i==GGLFormat::ALPHA ? mBlendSrcA : mBlendSrc;
+        int fd = i==GGLFormat::ALPHA ? mBlendDstA : mBlendDst;
+        if (fs==GGL_SRC_ALPHA_SATURATE && i==GGLFormat::ALPHA)
+            fs = GGL_ONE;
+        info.masked =   !!(masking & mask);
+        info.inDest =   !info.masked && mCbFormat.c[i].h &&
+                        ((mLogicOp & LOGIC_OP_SRC) || (!mLogicOp));
+        if (mCbFormat.components >= GGL_LUMINANCE &&
+                (i==GGLFormat::GREEN || i==GGLFormat::BLUE)) {
+            info.inDest = false;
+        }
+        info.needed =   (i==GGLFormat::ALPHA) &&
+                        (isAlphaSourceNeeded() || mAlphaTest != GGL_ALWAYS);
+        info.replaced = !!(mTextureMachine.replaced & mask);
+        info.iterated = (!info.replaced && (info.inDest || info.needed));
+        info.smooth =   mSmooth && info.iterated;
+        info.fog =      mFog && info.inDest && (i != GGLFormat::ALPHA);
+        info.blend =    (fs != int(GGL_ONE)) || (fd > int(GGL_ZERO));
+
+        mBlending |= (info.blend ? mask : 0);
+        mMasking |= (mCbFormat.c[i].h && info.masked) ? mask : 0;
+        fbComponents |= mCbFormat.c[i].h ? mask : 0;
+    }
+
+    mAllMasked = (mMasking == fbComponents);
+    if (mAllMasked) {
+        mDithering = 0;
+    }
+
+    fragment_parts_t parts;
+
+    // ------------------------------------------------------------------------
+    callee_work();
+    // ------------------------------------------------------------------------
+
+    mCurSp = -12; // %ebx, %edi, %esi
+    prepare_esp(0);
+    build_scanline_preparation(parts, needs);
+    recycleReg(mBuilderContext.Rctx);
+
+    if (registerFile().status())
+        return registerFile().status();
+
+    // ------------------------------------------------------------------------
+    label("fragment_loop");
+    // ------------------------------------------------------------------------
+    {
+        Scratch regs(registerFile());
+        int temp_reg = -1;
+
+        if (mDithering) {
+            // update the dither index.
+            temp_reg = regs.obtain();
+            //To load to register and calculate should be fast than the memory operations
+            MOV_MEM_TO_REG(parts.count.offset_ebp, PhysicalReg_EBP, temp_reg);
+            ROR(GGL_DITHER_ORDER_SHIFT, temp_reg);
+            ADD_IMM_TO_REG(1 << (32 - GGL_DITHER_ORDER_SHIFT), temp_reg);
+            ROR(32 - GGL_DITHER_ORDER_SHIFT, temp_reg);
+            MOV_REG_TO_MEM(temp_reg, parts.count.offset_ebp, PhysicalReg_EBP);
+            regs.recycle(temp_reg);
+
+        }
+
+        // XXX: could we do an early alpha-test here in some cases?
+        // It would probaly be used only with smooth-alpha and no texture
+        // (or no alpha component in the texture).
+
+        // Early z-test
+        if (mAlphaTest==GGL_ALWAYS) {
+            build_depth_test(parts, Z_TEST|Z_WRITE);
+        } else {
+            // we cannot do the z-write here, because
+            // it might be killed by the alpha-test later
+            build_depth_test(parts, Z_TEST);
+        }
+
+        {   // texture coordinates
+            Scratch scratches(registerFile());
+
+            // texel generation
+            build_textures(parts, regs);
+
+        }
+
+        if ((blending & (FACTOR_DST|BLEND_DST)) ||
+                (mMasking && !mAllMasked) ||
+                (mLogicOp & LOGIC_OP_DST))
+        {
+            // blending / logic_op / masking need the framebuffer
+            mDstPixel.setTo(regs.obtain(), &mCbFormat);
+
+            // load the framebuffer pixel
+            comment("fetch color-buffer");
+            parts.cbPtr.reg = regs.obtain();
+            MOV_MEM_TO_REG(parts.cbPtr.offset_ebp, PhysicalReg_EBP, parts.cbPtr.reg);
+            load(parts.cbPtr, mDstPixel);
+            mCurSp = mCurSp - 4;
+            mDstPixel.offset_ebp = mCurSp;
+            MOV_REG_TO_MEM(mDstPixel.reg, mDstPixel.offset_ebp, EBP);
+            regs.recycle(mDstPixel.reg);
+            regs.recycle(parts.cbPtr.reg);
+            mDstPixel.reg = -1;
+        }
+
+        if (registerFile().status())
+            return registerFile().status();
+
+        pixel_t pixel;
+        int directTex = mTextureMachine.directTexture;
+        if (directTex | parts.packed) {
+            // note: we can't have both here
+            // iterated color or direct texture
+            if(directTex) {
+                pixel.offset_ebp = parts.texel[directTex-1].offset_ebp;
+            }
+            else
+                pixel.offset_ebp = parts.iterated.offset_ebp;
+            pixel.reg = regs.obtain();
+            MOV_MEM_TO_REG(pixel.offset_ebp, EBP, pixel.reg);
+            //pixel = directTex ? parts.texel[directTex-1] : parts.iterated;
+            pixel.flags &= ~CORRUPTIBLE;
+        } else {
+            if (mDithering) {
+                mBuilderContext.Rctx = regs.obtain();
+                temp_reg = regs.obtain();
+                const int ctxtReg = mBuilderContext.Rctx;
+                MOV_MEM_TO_REG(8, EBP, ctxtReg);
+                const int mask = GGL_DITHER_SIZE-1;
+                parts.dither = reg_t(regs.obtain());
+                MOV_MEM_TO_REG(parts.count.offset_ebp, EBP, parts.dither.reg);
+                AND_IMM_TO_REG(mask, parts.dither.reg);
+                ADD_REG_TO_REG(ctxtReg, parts.dither.reg);
+                MOVZX_MEM_TO_REG(OpndSize_8, parts.dither.reg, GGL_OFFSETOF(ditherMatrix), temp_reg);
+                MOV_REG_TO_REG(temp_reg, parts.dither.reg);
+                mCurSp = mCurSp - 4;
+                parts.dither.offset_ebp = mCurSp;
+                MOV_REG_TO_MEM(parts.dither.reg, parts.dither.offset_ebp, EBP);
+                regs.recycle(parts.dither.reg);
+                regs.recycle(temp_reg);
+                regs.recycle(mBuilderContext.Rctx);
+
+            }
+
+            // allocate a register for the resulting pixel
+            pixel.setTo(regs.obtain(), &mCbFormat, FIRST);
+
+            build_component(pixel, parts, GGLFormat::ALPHA,    regs);
+
+            if (mAlphaTest!=GGL_ALWAYS) {
+                // only handle the z-write part here. We know z-test
+                // was successful, as well as alpha-test.
+                build_depth_test(parts, Z_WRITE);
+            }
+
+            build_component(pixel, parts, GGLFormat::RED,      regs);
+            build_component(pixel, parts, GGLFormat::GREEN,    regs);
+            build_component(pixel, parts, GGLFormat::BLUE,     regs);
+
+            pixel.flags |= CORRUPTIBLE;
+        }
+
+        if (registerFile().status()) {
+            return registerFile().status();
+        }
+
+        if (pixel.reg == -1) {
+            // be defensive here. if we're here it's probably
+            // that this whole fragment is a no-op.
+            pixel = mDstPixel;
+        }
+
+        if (!mAllMasked) {
+            // logic operation
+            build_logic_op(pixel, regs);
+
+            // masking
+            build_masking(pixel, regs);
+
+            comment("store");
+            parts.cbPtr.reg = regs.obtain();
+            MOV_MEM_TO_REG(parts.cbPtr.offset_ebp, EBP, parts.cbPtr.reg);
+            store(parts.cbPtr, pixel, WRITE_BACK);
+            MOV_REG_TO_MEM(parts.cbPtr.reg, parts.cbPtr.offset_ebp, EBP);
+            regs.recycle(parts.cbPtr.reg);
+            regs.recycle(pixel.reg);
+        }
+    }
+
+    if (registerFile().status())
+        return registerFile().status();
+
+    // update the iterated color...
+    if (parts.reload != 3) {
+        build_smooth_shade(parts);
+    }
+
+    // update iterated z
+    build_iterate_z(parts);
+
+    // update iterated fog
+    build_iterate_f(parts);
+
+    //SUB_IMM_TO_REG(1<<16, parts.count.reg);
+    SUB_IMM_TO_MEM(1<<16, parts.count.offset_ebp, EBP);
+
+    JCC(Mnemonic_JNS, "fragment_loop");
+    label("fragment_end");
+    int update_esp_offset, shrink_esp_offset;
+    update_esp_offset = shrink_esp_offset = -mCurSp - 12; // 12 is ebx, esi, edi
+    update_esp(update_esp_offset);
+    shrink_esp(shrink_esp_offset);
+    return_work();
+
+    if ((mAlphaTest!=GGL_ALWAYS) || (mDepthTest!=GGL_ALWAYS)) {
+        if (mDepthTest!=GGL_ALWAYS) {
+            label("discard_before_textures");
+            build_iterate_texture_coordinates(parts);
+        }
+        label("discard_after_textures");
+        build_smooth_shade(parts);
+        build_iterate_z(parts);
+        build_iterate_f(parts);
+        if (!mAllMasked) {
+            //ADD_IMM_TO_REG(parts.cbPtr.size>>3, parts.cbPtr.reg);
+            ADD_IMM_TO_MEM(parts.cbPtr.size>>3, parts.cbPtr.offset_ebp, EBP);
+        }
+        SUB_IMM_TO_MEM(1<<16, parts.count.offset_ebp, EBP);
+        //SUB_IMM_TO_REG(1<<16, parts.count.reg);
+        JCC(Mnemonic_JNS, "fragment_loop");
+        update_esp_offset = shrink_esp_offset = -mCurSp - 12; // 12 is ebx, esi, edi
+        update_esp(update_esp_offset);
+        shrink_esp(shrink_esp_offset);
+        return_work();
+    }
+
+    return registerFile().status();
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_scanline_preparation(
+    fragment_parts_t& parts, const needs_t& needs)
+{
+    Scratch scratches(registerFile());
+
+    // compute count
+    comment("compute ct (# of pixels to process)");
+    int temp_reg;
+    parts.count.setTo(obtainReg());
+    int Rx = scratches.obtain();
+    int Ry = scratches.obtain();
+    // the only argument is +8 bytes relative to the current EBP
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(Rx, iterators.xl);
+    CONTEXT_LOAD(parts.count.reg, iterators.xr);
+    CONTEXT_LOAD(Ry, iterators.y);
+
+    // parts.count = iterators.xr - Rx
+    SUB_REG_TO_REG(Rx, parts.count.reg);
+    SUB_IMM_TO_REG(1, parts.count.reg);
+
+    if (mDithering) {
+        // parts.count.reg = 0xNNNNXXDD
+        // NNNN = count-1
+        // DD   = dither offset
+        // XX   = 0xxxxxxx (x = garbage)
+        Scratch scratches(registerFile());
+        int tx = scratches.obtain();
+        int ty = scratches.obtain();
+
+        MOV_REG_TO_REG(Rx,tx);
+        AND_IMM_TO_REG(GGL_DITHER_MASK, tx);
+        MOV_REG_TO_REG(Ry,ty);
+        AND_IMM_TO_REG(GGL_DITHER_MASK, ty);
+        SHL(GGL_DITHER_ORDER_SHIFT, ty);
+        ADD_REG_TO_REG(ty, tx);
+        SHL(16, parts.count.reg);
+        OR_REG_TO_REG(tx, parts.count.reg);
+        scratches.recycle(tx);
+        scratches.recycle(ty);
+    } else {
+        // parts.count.reg = 0xNNNN0000
+        // NNNN = count-1
+        SHL(16, parts.count.reg);
+    }
+    mCurSp = mCurSp - 4;
+    parts.count.offset_ebp = mCurSp; //ebx, esi, edi, parts.count.reg
+    MOV_REG_TO_MEM(parts.count.reg, parts.count.offset_ebp, EBP);
+    //PUSH(parts.count.reg);
+    recycleReg(parts.count.reg);
+    parts.count.reg=-1;
+    if (!mAllMasked) {
+        // compute dst ptr
+        comment("compute color-buffer pointer");
+        const int cb_bits = mCbFormat.size*8;
+        int Rs = scratches.obtain();
+        temp_reg = scratches.obtain();
+        CONTEXT_LOAD(Rs, state.buffers.color.stride);
+        MOVSX_REG_TO_REG(OpndSize_16, Ry, temp_reg);
+        MOVSX_REG_TO_REG(OpndSize_16, Rs, Rs);
+        IMUL(temp_reg, Rs);
+        scratches.recycle(temp_reg);
+        ADD_REG_TO_REG(Rx, Rs);
+
+        parts.cbPtr.setTo(obtainReg(), cb_bits);
+        CONTEXT_LOAD(parts.cbPtr.reg, state.buffers.color.data);
+        reg_t temp_reg_t;
+        temp_reg_t.setTo(Rs);
+        base_offset(parts.cbPtr, parts.cbPtr, temp_reg_t);
+
+        mCurSp = mCurSp - 4;
+        parts.cbPtr.offset_ebp = mCurSp; //ebx, esi, edi, parts.count.reg, parts.cbPtr.reg
+        MOV_REG_TO_MEM(parts.cbPtr.reg, parts.cbPtr.offset_ebp, EBP);
+        //PUSH(parts.cbPtr.reg);
+        recycleReg(parts.cbPtr.reg);
+        parts.cbPtr.reg=-1;
+        scratches.recycle(Rs);
+    }
+
+    // init fog
+    const int need_fog = GGL_READ_NEEDS(P_FOG, needs.p);
+    if (need_fog) {
+        comment("compute initial fog coordinate");
+        Scratch scratches(registerFile());
+        int ydfdy = scratches.obtain();
+        int dfdx = scratches.obtain();
+        CONTEXT_LOAD(dfdx,  generated_vars.dfdx);
+        IMUL(Rx, dfdx);
+        CONTEXT_LOAD(ydfdy, iterators.ydfdy);
+        ADD_REG_TO_REG(ydfdy, dfdx); // Rx * dfdx + ydfdy
+        CONTEXT_STORE(dfdx, generated_vars.f);
+        scratches.recycle(dfdx);
+        scratches.recycle(ydfdy);
+    }
+
+    // init Z coordinate
+    if ((mDepthTest != GGL_ALWAYS) || GGL_READ_NEEDS(P_MASK_Z, needs.p)) {
+        parts.z = reg_t(obtainReg());
+        comment("compute initial Z coordinate");
+        Scratch scratches(registerFile());
+        int dzdx = scratches.obtain();
+        int ydzdy = parts.z.reg;
+        CONTEXT_LOAD(dzdx,  generated_vars.dzdx);   // 1.31 fixed-point
+        IMUL(Rx, dzdx);
+        CONTEXT_LOAD(ydzdy, iterators.ydzdy);       // 1.31 fixed-point
+        ADD_REG_TO_REG(dzdx, ydzdy);  // parts.z.reg = Rx * dzdx + ydzdy
+
+        mCurSp = mCurSp - 4;
+        parts.z.offset_ebp = mCurSp; //ebx, esi, edi, parts.count.reg, parts.cbPtr.reg, parts.z.reg
+        MOV_REG_TO_MEM(ydzdy, parts.z.offset_ebp, EBP);
+        //PUSH(ydzdy);
+        recycleReg(ydzdy);
+        parts.z.reg=-1;
+
+        // we're going to index zbase of parts.count
+        // zbase = base + (xl-count + stride*y)*2 by arm
+        // !!! Actually, zbase = base + (xl + stride*y)*2
+        int Rs = dzdx;
+        int zbase = scratches.obtain();
+        temp_reg = zbase;
+        CONTEXT_LOAD(Rs, state.buffers.depth.stride);
+        MOVSX_REG_TO_REG(OpndSize_16, Rs, Rs);
+        MOV_REG_TO_REG(Ry, temp_reg);
+        MOVSX_REG_TO_REG(OpndSize_16, temp_reg, temp_reg);
+        IMUL(temp_reg, Rs);
+        ADD_REG_TO_REG(Rx, Rs);
+        // load parts.count.reg
+        MOV_MEM_TO_REG(parts.count.offset_ebp, EBP, temp_reg);
+        SHR(16, temp_reg);
+        ADD_REG_TO_REG(temp_reg, Rs);
+        SHL(1, Rs);
+        CONTEXT_LOAD(zbase, state.buffers.depth.data);
+        ADD_REG_TO_REG(Rs, zbase);
+        CONTEXT_STORE(zbase, generated_vars.zbase);
+        scratches.recycle(zbase);
+        scratches.recycle(dzdx);
+    }
+    // the rgisters are all used up
+
+    // init texture coordinates
+    init_textures(parts.coords, reg_t(Rx), reg_t(Ry));
+    scratches.recycle(Ry);
+
+    // iterated color
+    init_iterated_color(parts, reg_t(Rx));
+
+    // init coverage factor application (anti-aliasing)
+    if (mAA) {
+        parts.covPtr.setTo(obtainReg(), 16);
+        CONTEXT_LOAD(parts.covPtr.reg, state.buffers.coverage);
+        SHL(1, Rx);
+        ADD_REG_TO_REG(Rx, parts.covPtr.reg);
+
+        mCurSp = mCurSp - 4;
+        parts.covPtr.offset_ebp = mCurSp;
+        MOV_REG_TO_MEM(parts.covPtr.reg, parts.covPtr.offset_ebp, EBP);
+        //PUSH(parts.covPtr.reg);
+        recycleReg(parts.covPtr.reg);
+        parts.covPtr.reg=-1;
+    }
+    scratches.recycle(Rx);
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_component( pixel_t& pixel,
+                                       fragment_parts_t& parts,
+                                       int component,
+                                       Scratch& regs)
+{
+    static char const * comments[] = {"alpha", "red", "green", "blue"};
+    comment(comments[component]);
+
+    // local register file
+    Scratch scratches(registerFile());
+    const int dst_component_size = pixel.component_size(component);
+
+    component_t temp(-1);
+    build_incoming_component( temp, dst_component_size,
+                              parts, component, scratches, regs);
+
+    if (mInfo[component].inDest) {
+        // blending...
+        build_blending( temp, mDstPixel, component, scratches );
+
+        // downshift component and rebuild pixel...
+        downshift(pixel, component, temp, parts.dither);
+    }
+}
+
+void GGLX86Assembler::build_incoming_component(
+    component_t& temp,
+    int dst_size,
+    fragment_parts_t& parts,
+    int component,
+    Scratch& scratches,
+    Scratch& global_regs)
+{
+    const uint32_t component_mask = 1<<component;
+
+    // Figure out what we need for the blending stage...
+    int fs = component==GGLFormat::ALPHA ? mBlendSrcA : mBlendSrc;
+    int fd = component==GGLFormat::ALPHA ? mBlendDstA : mBlendDst;
+    if (fs==GGL_SRC_ALPHA_SATURATE && component==GGLFormat::ALPHA) {
+        fs = GGL_ONE;
+    }
+
+    // Figure out what we need to extract and for what reason
+    const int blending = blending_codes(fs, fd);
+
+    // Are we actually going to blend?
+    const int need_blending = (fs != int(GGL_ONE)) || (fd > int(GGL_ZERO));
+
+    // expand the source if the destination has more bits
+    int need_expander = false;
+    for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT-1 ; i++) {
+        texture_unit_t& tmu = mTextureMachine.tmu[i];
+        if ((tmu.format_idx) &&
+                (parts.texel[i].component_size(component) < dst_size)) {
+            need_expander = true;
+        }
+    }
+
+    // do we need to extract this component?
+    const bool multiTexture = mTextureMachine.activeUnits > 1;
+    const int blend_needs_alpha_source = (component==GGLFormat::ALPHA) &&
+                                         (isAlphaSourceNeeded());
+    int need_extract = mInfo[component].needed;
+    if (mInfo[component].inDest)
+    {
+        need_extract |= ((need_blending ?
+                          (blending & (BLEND_SRC|FACTOR_SRC)) : need_expander));
+        need_extract |= (mTextureMachine.mask != mTextureMachine.replaced);
+        need_extract |= mInfo[component].smooth;
+        need_extract |= mInfo[component].fog;
+        need_extract |= mDithering;
+        need_extract |= multiTexture;
+    }
+
+    if (need_extract) {
+        Scratch& regs = blend_needs_alpha_source ? global_regs : scratches;
+        component_t fragment;
+
+        // iterated color
+        fragment.setTo( regs.obtain(), 0, 32, CORRUPTIBLE);
+        build_iterated_color(fragment, parts, component, regs);
+
+        // texture environment (decal, modulate, replace)
+        build_texture_environment(fragment, parts, component, regs);
+
+        // expand the source if the destination has more bits
+        if (need_expander && (fragment.size() < dst_size)) {
+            // we're here only if we fetched a texel
+            // (so we know for sure fragment is CORRUPTIBLE)
+            //fragment is stored on the stack
+            expand(fragment, fragment, dst_size);
+        }
+
+        mCurSp = mCurSp - 4;
+        fragment.offset_ebp = mCurSp;
+        MOV_REG_TO_MEM(fragment.reg, fragment.offset_ebp, EBP);
+        regs.recycle(fragment.reg);
+
+        // We have a few specific things to do for the alpha-channel
+        if ((component==GGLFormat::ALPHA) &&
+                (mInfo[component].needed || fragment.size()<dst_size))
+        {
+            // convert to integer_t first and make sure
+            // we don't corrupt a needed register
+            if (fragment.l) {
+                //component_t incoming(fragment);
+                // actually fragment is not corruptible
+                //modify(fragment, regs);
+                //MOV_REG_TO_REG(incoming.reg, fragment.reg);
+                SHR(fragment.l, fragment.offset_ebp, EBP);
+                fragment.h -= fragment.l;
+                fragment.l = 0;
+            }
+
+            // I haven't found any case to trigger coverage and the following alpha test (mAlphaTest != GGL_ALWAYS)
+            fragment.reg = regs.obtain();
+            MOV_MEM_TO_REG(fragment.offset_ebp, EBP, fragment.reg);
+
+            // coverage factor application
+            build_coverage_application(fragment, parts, regs);
+            // alpha-test
+            build_alpha_test(fragment, parts);
+
+            MOV_REG_TO_MEM(fragment.reg, fragment.offset_ebp, EBP);
+            regs.recycle(fragment.reg);
+
+            if (blend_needs_alpha_source) {
+                // We keep only 8 bits for the blending stage
+                const int shift = fragment.h <= 8 ? 0 : fragment.h-8;
+
+                if (fragment.flags & CORRUPTIBLE) {
+                    fragment.flags &= ~CORRUPTIBLE;
+                    mAlphaSource.setTo(fragment.reg,
+                                       fragment.size(), fragment.flags, fragment.offset_ebp);
+                    //mCurSp = mCurSp - 4;
+                    //mAlphaSource.offset_ebp = mCurSp;
+                    if (shift) {
+                        SHR(shift, mAlphaSource.offset_ebp, EBP);
+                    }
+                } else {
+                    // XXX: it would better to do this in build_blend_factor()
+                    // so we can avoid the extra MOV below.
+                    mAlphaSource.setTo(regs.obtain(),
+                                       fragment.size(), CORRUPTIBLE);
+                    mCurSp = mCurSp - 4;
+                    mAlphaSource.offset_ebp = mCurSp;
+                    if (shift) {
+                        MOV_MEM_TO_REG(fragment.offset_ebp, EBP, mAlphaSource.reg);
+                        SHR(shift, mAlphaSource.reg);
+                    } else {
+                        MOV_MEM_TO_REG(fragment.offset_ebp, EBP, mAlphaSource.reg);
+                    }
+                    MOV_REG_TO_MEM(mAlphaSource.reg, mAlphaSource.offset_ebp, EBP);
+                    regs.recycle(mAlphaSource.reg);
+                }
+                mAlphaSource.s -= shift;
+
+            }
+        }
+
+        // fog...
+        build_fog( fragment, component, regs );
+
+        temp = fragment;
+    } else {
+        if (mInfo[component].inDest) {
+            // extraction not needed and replace
+            // we just select the right component
+            if ((mTextureMachine.replaced & component_mask) == 0) {
+                // component wasn't replaced, so use it!
+                temp = component_t(parts.iterated, component);
+            }
+            for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT ; i++) {
+                const texture_unit_t& tmu = mTextureMachine.tmu[i];
+                if ((tmu.mask & component_mask) &&
+                        ((tmu.replaced & component_mask) == 0)) {
+                    temp = component_t(parts.texel[i], component);
+                }
+            }
+        }
+    }
+}
+
+bool GGLX86Assembler::isAlphaSourceNeeded() const
+{
+    // XXX: also needed for alpha-test
+    const int bs = mBlendSrc;
+    const int bd = mBlendDst;
+    return  bs==GGL_SRC_ALPHA_SATURATE ||
+            bs==GGL_SRC_ALPHA || bs==GGL_ONE_MINUS_SRC_ALPHA ||
+            bd==GGL_SRC_ALPHA || bd==GGL_ONE_MINUS_SRC_ALPHA ;
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_smooth_shade(fragment_parts_t& parts)
+{
+    if (mSmooth && !parts.iterated_packed) {
+        // update the iterated color in a pipelined way...
+        comment("update iterated color");
+        Scratch scratches(registerFile());
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+
+        const int reload = parts.reload;
+        for (int i=0 ; i<4 ; i++) {
+            if (!mInfo[i].iterated)
+                continue;
+
+            int dx = parts.argb_dx[i].reg;
+            int c = parts.argb[i].reg;
+            dx = scratches.obtain();
+            c = scratches.obtain();
+            CONTEXT_LOAD(dx, generated_vars.argb[i].dx);
+            CONTEXT_LOAD(c, generated_vars.argb[i].c);
+
+            //if (reload & 1) {
+            //    c = scratches.obtain();
+            //    CONTEXT_LOAD(c, generated_vars.argb[i].c);
+            //}
+            //if (reload & 2) {
+            //    dx = scratches.obtain();
+            //    CONTEXT_LOAD(dx, generated_vars.argb[i].dx);
+            //}
+
+            if (mSmooth) {
+                ADD_REG_TO_REG(dx, c);
+            }
+
+            CONTEXT_STORE(c, generated_vars.argb[i].c);
+            scratches.recycle(c);
+            scratches.recycle(dx);
+            //if (reload & 1) {
+            //    CONTEXT_STORE(c, generated_vars.argb[i].c);
+            //    scratches.recycle(c);
+            //}
+            //if (reload & 2) {
+            //    scratches.recycle(dx);
+            //}
+        }
+        scratches.recycle(mBuilderContext.Rctx);
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_coverage_application(component_t& fragment,
+        fragment_parts_t& parts, Scratch& regs)
+{
+    // here fragment.l is guarenteed to be 0
+    if (mAA) {
+        // coverages are 1.15 fixed-point numbers
+        comment("coverage application");
+
+        component_t incoming(fragment);
+        modify(fragment, regs);
+
+        Scratch scratches(registerFile());
+        int cf = scratches.obtain();
+        parts.covPtr.reg = scratches.obtain();
+        MOV_MEM_TO_REG(parts.covPtr.offset_ebp, EBP, parts.covPtr.reg);
+        MOVZX_MEM_TO_REG(OpndSize_16, parts.covPtr.reg, 2, cf); // refer to LDRH definition
+        scratches.recycle(parts.covPtr.reg);
+        if (fragment.h > 31) {
+            fragment.h--;
+
+            int flag_push_edx = 0;
+            int flag_reserve_edx = 0;
+            int temp_reg2 = -1;
+            int edx_offset_ebp = 0;
+            if(scratches.isUsed(EDX) == 1) {
+                if(incoming.reg != EDX && cf != EDX) {
+                    flag_push_edx = 1;
+                    mCurSp = mCurSp - 4;
+                    edx_offset_ebp = mCurSp;
+                    MOV_REG_TO_MEM(EDX, edx_offset_ebp, EBP);
+                }
+            }
+            else {
+                flag_reserve_edx = 1;
+                scratches.reserve(EDX);
+            }
+            if(scratches.isUsed(EAX)) {
+                if( cf == EAX || incoming.reg == EAX) {
+                    MOVSX_REG_TO_REG(OpndSize_16, cf, cf);
+                    if(cf == EAX)
+                        IMUL(incoming.reg);
+                    else
+                        IMUL(cf);
+                    SHL(16, EDX);
+                    SHR(16, EAX);
+                    MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                    MOV_REG_TO_REG(EDX, incoming.reg);
+                }
+                else {
+                    int eax_offset_ebp = 0;
+                    if(scratches.countFreeRegs() > 0) {
+                        temp_reg2 = scratches.obtain();
+                        MOV_REG_TO_REG(EAX, temp_reg2);
+                    }
+                    else {
+                        mCurSp = mCurSp - 4;
+                        eax_offset_ebp = mCurSp;
+                        MOV_REG_TO_MEM(EAX, eax_offset_ebp, EBP);
+                    }
+                    MOV_REG_TO_REG(cf, EAX);
+                    MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                    IMUL(incoming.reg);
+                    SHL(16, EDX);
+                    SHR(16, EAX);
+                    MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                    MOV_REG_TO_REG(EDX, incoming.reg);
+                    if(temp_reg2 > -1) {
+                        MOV_REG_TO_REG(temp_reg2, EAX);
+                        scratches.recycle(temp_reg2);
+                    }
+                    else {
+                        MOV_MEM_TO_REG(eax_offset_ebp, EBP, EAX);
+                    }
+                }
+            }
+            else {
+                MOV_REG_TO_REG(cf, EAX);
+                MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                IMUL(incoming.reg);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, incoming.reg);
+            }
+            if(flag_push_edx == 1) {
+                MOV_MEM_TO_REG(edx_offset_ebp, EBP, EDX);
+            }
+            if(flag_reserve_edx ==1)
+                scratches.recycle(EDX);
+
+            MOV_REG_TO_REG(incoming.reg, fragment.reg);
+
+            //IMUL(cf, incoming.reg);
+        } else {
+            MOV_REG_TO_REG(incoming.reg, fragment.reg);
+            SHL(1, fragment.reg);
+
+            int flag_push_edx = 0;
+            int flag_reserve_edx = 0;
+            int temp_reg2 = -1;
+            int edx_offset_ebp = 0;
+            if(scratches.isUsed(EDX) == 1) {
+                if(fragment.reg != EDX && cf != EDX) {
+                    flag_push_edx = 1;
+                    mCurSp = mCurSp - 4;
+                    edx_offset_ebp = mCurSp;
+                    MOV_REG_TO_MEM(EDX, edx_offset_ebp, EBP);
+                }
+            }
+            else {
+                flag_reserve_edx = 1;
+                scratches.reserve(EDX);
+            }
+            if(scratches.isUsed(EAX)) {
+                if( cf == EAX || fragment.reg == EAX) {
+                    MOVSX_REG_TO_REG(OpndSize_16, cf, cf);
+                    if(cf == EAX)
+                        IMUL(fragment.reg);
+                    else
+                        IMUL(cf);
+                    SHL(16, EDX);
+                    SHR(16, EAX);
+                    MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                    MOV_REG_TO_REG(EDX, fragment.reg);
+                }
+                else {
+                    int eax_offset_ebp = 0;
+                    if(scratches.countFreeRegs() > 0) {
+                        temp_reg2 = scratches.obtain();
+                        MOV_REG_TO_REG(EAX, temp_reg2);
+                    }
+                    else {
+                        mCurSp = mCurSp - 4;
+                        eax_offset_ebp = mCurSp;
+                        MOV_REG_TO_MEM(EAX, eax_offset_ebp, EBP);
+                    }
+                    MOV_REG_TO_REG(cf, EAX);
+                    MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                    IMUL(fragment.reg);
+                    SHL(16, EDX);
+                    SHR(16, EAX);
+                    MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                    MOV_REG_TO_REG(EDX, fragment.reg);
+                    if(temp_reg2 > -1) {
+                        MOV_REG_TO_REG(temp_reg2, EAX);
+                        scratches.recycle(temp_reg2);
+                    }
+                    else {
+                        MOV_MEM_TO_REG(eax_offset_ebp, EBP, EAX);
+                    }
+                }
+            }
+            else {
+                MOV_REG_TO_REG(cf, EAX);
+                MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                IMUL(fragment.reg);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, fragment.reg);
+            }
+            if(flag_push_edx == 1) {
+                MOV_MEM_TO_REG(edx_offset_ebp, EBP, EDX);
+            }
+            if(flag_reserve_edx ==1)
+                scratches.recycle(EDX);
+
+            //IMUL(cf, fragment.reg);
+        }
+        scratches.recycle(cf);
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_alpha_test(component_t& fragment,
+                                       const fragment_parts_t& parts)
+{
+    if (mAlphaTest != GGL_ALWAYS) {
+        comment("Alpha Test");
+        Scratch scratches(registerFile());
+        int ref = scratches.obtain();
+        mBuilderContext.Rctx  = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        const int shift = GGL_COLOR_BITS-fragment.size();
+        CONTEXT_LOAD(ref, state.alpha_test.ref);
+        scratches.recycle(mBuilderContext.Rctx);
+        if (shift) {
+            SHR(shift, ref);
+            CMP_REG_TO_REG(ref, fragment.reg);
+        } else   CMP_REG_TO_REG(ref, fragment.reg);
+        Mnemonic cc = Mnemonic_NULL;
+        //int cc = NV;
+        switch (mAlphaTest) {
+        case GGL_NEVER:
+            JMP("discard_after_textures");
+            return;
+            break;
+        case GGL_LESS:
+            cc = Mnemonic_JNL;
+            break;
+        case GGL_EQUAL:
+            cc = Mnemonic_JNE;
+            break;
+        case GGL_LEQUAL:
+            cc = Mnemonic_JB;
+            break;
+        case GGL_GREATER:
+            cc = Mnemonic_JLE;
+            break;
+        case GGL_NOTEQUAL:
+            cc = Mnemonic_JE;
+            break;
+        case GGL_GEQUAL:
+            cc = Mnemonic_JNC;
+            break;
+        }
+        JCC(cc, "discard_after_textures");
+        //B(cc^1, "discard_after_textures");
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_depth_test(
+    const fragment_parts_t& parts, uint32_t mask)
+{
+    mask &= Z_TEST|Z_WRITE;
+    int store_flag = 0;
+    const needs_t& needs = mBuilderContext.needs;
+    const int zmask = GGL_READ_NEEDS(P_MASK_Z, needs.p);
+    Scratch scratches(registerFile());
+
+    if (mDepthTest != GGL_ALWAYS || zmask) {
+        Mnemonic ic = Mnemonic_NULL;
+        switch (mDepthTest) {
+        case GGL_LESS:
+            ic = Mnemonic_JBE;
+            break;
+        case GGL_EQUAL:
+            ic = Mnemonic_JNE;
+            break;
+        case GGL_LEQUAL:
+            ic = Mnemonic_JB;
+            break;
+        case GGL_GREATER:
+            ic = Mnemonic_JGE;
+            break;
+        case GGL_NOTEQUAL:
+            ic = Mnemonic_JE;
+            break;
+        case GGL_GEQUAL:
+            ic = Mnemonic_JA;
+            break;
+        case GGL_NEVER:
+            // this never happens, because it's taken care of when
+            // computing the needs. but we keep it for completness.
+            comment("Depth Test (NEVER)");
+            JMP("discard_before_textures");
+            return;
+        case GGL_ALWAYS:
+            // we're here because zmask is enabled
+            mask &= ~Z_TEST;    // test always passes.
+            break;
+        }
+
+
+        if ((mask & Z_WRITE) && !zmask) {
+            mask &= ~Z_WRITE;
+        }
+
+        if (!mask)
+            return;
+
+        comment("Depth Test");
+
+        int zbase = scratches.obtain();
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        CONTEXT_LOAD(zbase, generated_vars.zbase);  // stall
+        scratches.recycle(mBuilderContext.Rctx);
+
+        int temp_reg1 = scratches.obtain();
+        int depth = scratches.obtain();
+        int z = parts.z.reg;
+        MOV_MEM_TO_REG(parts.count.offset_ebp, PhysicalReg_EBP, temp_reg1);
+        SHR(15, temp_reg1);
+        SUB_REG_TO_REG(temp_reg1, zbase);
+
+        // above does zbase = zbase + ((count >> 16) << 1)
+
+        if (mask & Z_TEST) {
+            MOVZX_MEM_TO_REG(OpndSize_16, zbase, 0, depth);
+            MOV_MEM_TO_REG(parts.z.offset_ebp, PhysicalReg_EBP, temp_reg1);
+            SHR(16, temp_reg1);
+            CMP_REG_TO_REG(temp_reg1, depth);
+            JCC(ic, "discard_before_textures");
+
+        }
+        if (mask & Z_WRITE) {
+            if (mask == Z_WRITE) {
+                // only z-write asked, cc is meaningless
+                store_flag = 1;
+            }
+            // actually it must be stored since the above branch is not taken
+            MOV_REG_TO_MEM(temp_reg1, 0, zbase, OpndSize_16);
+        }
+        scratches.recycle(temp_reg1);
+        scratches.recycle(zbase);
+        scratches.recycle(depth);
+    }
+}
+
+void GGLX86Assembler::build_iterate_z(const fragment_parts_t& parts)
+{
+    const needs_t& needs = mBuilderContext.needs;
+    if ((mDepthTest != GGL_ALWAYS) || GGL_READ_NEEDS(P_MASK_Z, needs.p)) {
+        Scratch scratches(registerFile());
+        int dzdx = scratches.obtain();
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        CONTEXT_LOAD(dzdx, generated_vars.dzdx);    // stall
+        scratches.recycle(mBuilderContext.Rctx);
+        ADD_REG_TO_MEM(dzdx, EBP, parts.z.offset_ebp);
+        scratches.recycle(dzdx);
+    }
+}
+
+void GGLX86Assembler::build_iterate_f(const fragment_parts_t& parts)
+{
+    const needs_t& needs = mBuilderContext.needs;
+    if (GGL_READ_NEEDS(P_FOG, needs.p)) {
+        Scratch scratches(registerFile());
+        int dfdx = scratches.obtain();
+        int f = scratches.obtain();
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        CONTEXT_LOAD(f,     generated_vars.f);
+        CONTEXT_LOAD(dfdx,  generated_vars.dfdx);   // stall
+        ADD_REG_TO_REG(dfdx, f);
+        CONTEXT_STORE(f,    generated_vars.f);
+        scratches.recycle(mBuilderContext.Rctx);
+        scratches.recycle(dfdx);
+        scratches.recycle(f);
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_logic_op(pixel_t& pixel, Scratch& regs)
+{
+    const needs_t& needs = mBuilderContext.needs;
+    const int opcode = GGL_READ_NEEDS(LOGIC_OP, needs.n) | GGL_CLEAR;
+    if (opcode == GGL_COPY)
+        return;
+
+    comment("logic operation");
+
+    pixel_t s(pixel);
+    if (!(pixel.flags & CORRUPTIBLE)) {
+        pixel.reg = regs.obtain();
+        pixel.flags |= CORRUPTIBLE;
+    }
+
+    pixel_t d(mDstPixel);
+    d.reg = regs.obtain();
+    MOV_MEM_TO_REG(mDstPixel.offset_ebp, EBP, d.reg);
+    switch(opcode) {
+    case GGL_CLEAR:
+        MOV_IMM_TO_REG(0, pixel.reg);
+        break;
+    case GGL_AND:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        AND_REG_TO_REG(s.reg, pixel.reg);
+        break;
+    case GGL_AND_REVERSE:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        NOT(pixel.reg);
+        AND_REG_TO_REG(s.reg, pixel.reg);
+        break;
+    case GGL_COPY:
+        break;
+    case GGL_AND_INVERTED:
+        MOV_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        AND_REG_TO_REG(d.reg, pixel.reg);
+        break;
+    case GGL_NOOP:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        break;
+    case GGL_XOR:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        XOR(s.reg, pixel.reg);
+        break;
+    case GGL_OR:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        OR_REG_TO_REG(s.reg, pixel.reg);
+        break;
+    case GGL_NOR:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        OR_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_EQUIV:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        XOR(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_INVERT:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_OR_REVERSE:    // s | ~d == ~(~s & d)
+        MOV_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        AND_REG_TO_REG(d.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_COPY_INVERTED:
+        MOV_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_OR_INVERTED:   // ~s | d == ~(s & ~d)
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        NOT(pixel.reg);
+        AND_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_NAND:
+        MOV_REG_TO_REG(d.reg, pixel.reg);
+        AND_REG_TO_REG(s.reg, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    case GGL_SET:
+        MOV_IMM_TO_REG(0, pixel.reg);
+        NOT(pixel.reg);
+        break;
+    };
+    regs.recycle(d.reg);
+}
+
+// ---------------------------------------------------------------------------
+
+
+void GGLX86Assembler::build_and_immediate(int d, int s, uint32_t mask, int bits)
+{
+    uint32_t rot;
+    uint32_t size = ((bits>=32) ? 0 : (1LU << bits)) - 1;
+    mask &= size;
+
+    if (mask == size) {
+        if (d != s)
+            MOV_REG_TO_REG(s, d);
+        return;
+    }
+
+    MOV_REG_TO_REG(s, d);
+    AND_IMM_TO_REG(mask, d);
+}
+
+void GGLX86Assembler::build_masking(pixel_t& pixel, Scratch& regs)
+{
+    if (!mMasking || mAllMasked) {
+        return;
+    }
+
+    comment("color mask");
+
+    pixel_t fb(mDstPixel);
+    fb.reg = regs.obtain();
+    MOV_MEM_TO_REG(mDstPixel.offset_ebp, EBP, fb.reg);
+    pixel_t s(pixel);
+    if (!(pixel.flags & CORRUPTIBLE)) {
+        pixel.reg = regs.obtain();
+        pixel.flags |= CORRUPTIBLE;
+    }
+
+    int mask = 0;
+    for (int i=0 ; i<4 ; i++) {
+        const int component_mask = 1<<i;
+        const int h = fb.format.c[i].h;
+        const int l = fb.format.c[i].l;
+        if (h && (!(mMasking & component_mask))) {
+            mask |= ((1<<(h-l))-1) << l;
+        }
+    }
+
+    // There is no need to clear the masked components of the source
+    // (unless we applied a logic op), because they're already zeroed
+    // by construction (masked components are not computed)
+
+    if (mLogicOp) {
+        const needs_t& needs = mBuilderContext.needs;
+        const int opcode = GGL_READ_NEEDS(LOGIC_OP, needs.n) | GGL_CLEAR;
+        if (opcode != GGL_CLEAR) {
+            // clear masked component of source
+            build_and_immediate(pixel.reg, s.reg, mask, fb.size());
+            s = pixel;
+        }
+    }
+
+    // clear non masked components of destination
+    build_and_immediate(fb.reg, fb.reg, ~mask, fb.size());
+
+    // or back the channels that were masked
+    if (s.reg == fb.reg) {
+        // this is in fact a MOV
+        if (s.reg == pixel.reg) {
+            // ugh. this in in fact a nop
+        } else {
+            MOV_REG_TO_REG(fb.reg, pixel.reg);
+        }
+    } else {
+        MOV_REG_TO_REG(fb.reg, pixel.reg);
+        OR_REG_TO_REG(s.reg, pixel.reg);
+    }
+    MOV_REG_TO_MEM(fb.reg, mDstPixel.offset_ebp, EBP);
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::base_offset(pointer_t& d, pointer_t& b, const reg_t& o)
+{
+// d and b are the same reference
+    Scratch scratches(registerFile());
+    int temp_reg = scratches.obtain();
+    switch (b.size) {
+    case 32:
+        MOV_REG_TO_REG(b.reg, temp_reg);
+        MOV_REG_TO_REG(o.reg, d.reg);
+        SHL(2,d.reg);
+        ADD_REG_TO_REG(temp_reg, d.reg);
+        break;
+    case 24:
+        if (d.reg == b.reg) {
+            MOV_REG_TO_REG(b.reg, temp_reg);
+            MOV_REG_TO_REG(o.reg, d.reg);
+            SHL(1,d.reg);
+            ADD_REG_TO_REG(temp_reg, d.reg);
+            ADD_REG_TO_REG(o.reg, d.reg);
+        } else {
+            MOV_REG_TO_REG(o.reg, temp_reg);
+            SHL(1,temp_reg);
+            MOV_REG_TO_REG(temp_reg, d.reg);
+            ADD_REG_TO_REG(o.reg, d.reg);
+            ADD_REG_TO_REG(b.reg, d.reg);
+        }
+        break;
+    case 16:
+        MOV_REG_TO_REG(b.reg, temp_reg);
+        MOV_REG_TO_REG(o.reg, d.reg);
+        SHL(1,d.reg);
+        ADD_REG_TO_REG(temp_reg, d.reg);
+        break;
+    case 8:
+        MOV_REG_TO_REG(b.reg, temp_reg);
+        MOV_REG_TO_REG(o.reg, d.reg);
+        ADD_REG_TO_REG(temp_reg, d.reg);
+        break;
+    }
+    scratches.recycle(temp_reg);
+}
+
+// ----------------------------------------------------------------------------
+// cheezy register allocator...
+// ----------------------------------------------------------------------------
+
+void X86RegisterAllocator::reset()
+{
+    mRegs.reset();
+}
+
+int X86RegisterAllocator::reserveReg(int reg)
+{
+    return mRegs.reserve(reg);
+}
+
+int X86RegisterAllocator::obtainReg()
+{
+    return mRegs.obtain();
+}
+
+void X86RegisterAllocator::recycleReg(int reg)
+{
+    mRegs.recycle(reg);
+}
+
+X86RegisterAllocator::RegisterFile& X86RegisterAllocator::registerFile()
+{
+    return mRegs;
+}
+
+// ----------------------------------------------------------------------------
+
+X86RegisterAllocator::RegisterFile::RegisterFile()
+    : mRegs(0), mTouched(0), mStatus(0)
+{
+    //reserve(PhysicalReg_EBP);
+    //reserve(PhysicalReg_ESP);
+}
+
+X86RegisterAllocator::RegisterFile::RegisterFile(const RegisterFile& rhs)
+    : mRegs(rhs.mRegs), mTouched(rhs.mTouched)
+{
+}
+
+X86RegisterAllocator::RegisterFile::~RegisterFile()
+{
+}
+
+bool X86RegisterAllocator::RegisterFile::operator == (const RegisterFile& rhs) const
+{
+    return (mRegs == rhs.mRegs);
+}
+
+void X86RegisterAllocator::RegisterFile::reset()
+{
+    mRegs = mTouched = mStatus = 0;
+}
+
+int X86RegisterAllocator::RegisterFile::reserve(int reg)
+{
+    LOG_ALWAYS_FATAL_IF(isUsed(reg),
+                        "reserving register %d, but already in use",
+                        reg);
+    if(isUsed(reg)) return -1;
+    mRegs |= (1<<reg);
+    mTouched |= mRegs;
+    return reg;
+}
+
+void X86RegisterAllocator::RegisterFile::reserveSeveral(uint32_t regMask)
+{
+    mRegs |= regMask;
+    mTouched |= regMask;
+}
+
+int X86RegisterAllocator::RegisterFile::isUsed(int reg) const
+{
+    LOG_ALWAYS_FATAL_IF(reg>=6, "invalid register %d", reg);
+    return mRegs & (1<<reg);
+}
+
+int X86RegisterAllocator::RegisterFile::obtain()
+{
+//multiplication result is in edx:eax
+//ebx, ecx, edi, esi, eax, edx
+    const char priorityList[6] = { PhysicalReg_EBX, PhysicalReg_ECX,PhysicalReg_EDI, PhysicalReg_ESI, PhysicalReg_EAX, PhysicalReg_EDX };
+
+    const int nbreg = sizeof(priorityList);
+    int i, r;
+    for (i=0 ; i<nbreg ; i++) {
+        r = priorityList[i];
+        if (!isUsed(r)) {
+            break;
+        }
+    }
+    // this is not an error anymore because, we'll try again with
+    // a lower optimization level.
+    ALOGE_IF(i >= nbreg, "pixelflinger ran out of registers\n");
+    if (i >= nbreg) {
+        mStatus |= OUT_OF_REGISTERS;
+        // we return SP so we can more easily debug things
+        // the code will never be run anyway.
+        printf("pixelflinger ran out of registers\n");
+        return PhysicalReg_ESP;
+        //return -1;
+    }
+    reserve(r);
+    return r;
+}
+
+bool X86RegisterAllocator::RegisterFile::hasFreeRegs() const
+{
+    return ((mRegs & 0x3F) == 0x3F) ? false : true;
+}
+
+int X86RegisterAllocator::RegisterFile::countFreeRegs() const
+{
+    int f = ~mRegs & 0x3F;
+    // now count number of 1
+    f = (f & 0x5555) + ((f>>1) & 0x5555);
+    f = (f & 0x3333) + ((f>>2) & 0x3333);
+    f = (f & 0x0F0F) + ((f>>4) & 0x0F0F);
+    f = (f & 0x00FF) + ((f>>8) & 0x00FF);
+    return f;
+}
+
+void X86RegisterAllocator::RegisterFile::recycle(int reg)
+{
+    LOG_FATAL_IF(!isUsed(reg),
+                 "recycling unallocated register %d",
+                 reg);
+    mRegs &= ~(1<<reg);
+}
+
+void X86RegisterAllocator::RegisterFile::recycleSeveral(uint32_t regMask)
+{
+    LOG_FATAL_IF((mRegs & regMask)!=regMask,
+                 "recycling unallocated registers "
+                 "(recycle=%08x, allocated=%08x, unallocated=%08x)",
+                 regMask, mRegs, mRegs&regMask);
+    mRegs &= ~regMask;
+}
+
+uint32_t X86RegisterAllocator::RegisterFile::touched() const
+{
+    return mTouched;
+}
+
+// ----------------------------------------------------------------------------
+
+}; // namespace android
+
diff --git a/libpixelflinger/codeflinger/x86/GGLX86Assembler.h b/libpixelflinger/codeflinger/x86/GGLX86Assembler.h
index e69de29..1960cfc 100644
--- a/libpixelflinger/codeflinger/x86/GGLX86Assembler.h
+++ b/libpixelflinger/codeflinger/x86/GGLX86Assembler.h
@@ -0,0 +1,563 @@
+/* libs/pixelflinger/codeflinger/x86/GGLX86Assembler.h
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+
+#ifndef ANDROID_GGLX86ASSEMBLER_H
+#define ANDROID_GGLX86ASSEMBLER_H
+
+#include <stdint.h>
+#include <sys/types.h>
+
+#include <private/pixelflinger/ggl_context.h>
+
+#include "codeflinger/x86/X86Assembler.h"
+
+
+namespace android {
+
+// ----------------------------------------------------------------------------
+
+#define CONTEXT_LOAD(REG, FIELD) \
+    MOV_MEM_TO_REG(GGL_OFFSETOF(FIELD), mBuilderContext.Rctx, REG)
+
+#define CONTEXT_STORE(REG, FIELD) \
+    MOV_REG_TO_MEM(REG, GGL_OFFSETOF(FIELD), mBuilderContext.Rctx)
+
+class X86RegisterAllocator
+{
+public:
+    class RegisterFile;
+
+    RegisterFile&   registerFile();
+    int             reserveReg(int reg);
+    int             obtainReg();
+    void            recycleReg(int reg);
+    void            reset();
+
+    class RegisterFile
+    {
+    public:
+                            RegisterFile();
+                            RegisterFile(const RegisterFile& rhs);
+                            ~RegisterFile();
+
+                void        reset();
+
+                bool operator == (const RegisterFile& rhs) const;
+                bool operator != (const RegisterFile& rhs) const {
+                    return !operator == (rhs);
+                }
+
+                int         reserve(int reg);
+                void        reserveSeveral(uint32_t regMask);
+
+                void        recycle(int reg);
+                void        recycleSeveral(uint32_t regMask);
+
+                int         obtain();
+        inline  int         isUsed(int reg) const;
+
+                bool        hasFreeRegs() const;
+                int         countFreeRegs() const;
+
+                uint32_t    touched() const;
+        inline  uint32_t    status() const { return mStatus; }
+
+        enum {
+            OUT_OF_REGISTERS = 0x1
+        };
+
+    private:
+        uint32_t    mRegs;
+        uint32_t    mTouched;
+        uint32_t    mStatus;
+    };
+
+    class Scratch
+    {
+    public:
+            Scratch(RegisterFile& regFile)
+                : mRegFile(regFile), mScratch(0) {
+            }
+            ~Scratch() {
+                mRegFile.recycleSeveral(mScratch);
+            }
+        int obtain() {
+            int reg = mRegFile.obtain();
+            mScratch |= 1<<reg;
+            return reg;
+        }
+        void reserve(int reg) {
+            mRegFile.reserve(reg);
+            mScratch |= 1<<reg;
+        }
+        void recycle(int reg) {
+            mRegFile.recycle(reg);
+            mScratch &= ~(1<<reg);
+        }
+        bool isUsed(int reg) {
+            return (mScratch & (1<<reg));
+        }
+        int countFreeRegs() {
+            return mRegFile.countFreeRegs();
+        }
+    private:
+        RegisterFile&   mRegFile;
+        uint32_t        mScratch;
+    };
+
+/*
+// currently we don't use it
+
+    class Spill
+    {
+    public:
+        Spill(RegisterFile& regFile, X86Assembler& gen, uint32_t reglist)
+            : mRegFile(regFile), mGen(gen), mRegList(reglist), mCount(0)
+        {
+            if (reglist) {
+                int count = 0;
+                while (reglist) {
+                    count++;
+                    reglist &= ~(1 << (31 - __builtin_clz(reglist)));
+                }
+                if (count == 1) {
+                    int reg = 31 - __builtin_clz(mRegList);
+                    // move to the stack
+                } else {
+                    // move to the stack
+                }
+                mRegFile.recycleSeveral(mRegList);
+                mCount = count;
+            }
+        }
+        ~Spill() {
+            if (mRegList) {
+                if (mCount == 1) {
+                    int reg = 31 - __builtin_clz(mRegList);
+                    // move to the stack
+                } else {
+                }
+                mRegFile.reserveSeveral(mRegList);
+            }
+        }
+    private:
+        RegisterFile&           mRegFile;
+        X86Assembler&           mGen;
+        uint32_t                mRegList;
+        int                     mCount;
+    };
+*/
+
+private:
+    RegisterFile    mRegs;
+};
+
+// ----------------------------------------------------------------------------
+
+class GGLX86Assembler : public X86Assembler, public X86RegisterAllocator
+{
+public:
+
+            GGLX86Assembler(const sp<Assembly>& assembly);
+            ~GGLX86Assembler();
+
+    char*   base() const { return 0; } // XXX
+    char*   pc() const { return 0; } // XXX
+
+    void    reset(int opt_level);
+
+
+        // generate scanline code for given needs
+    int     scanline(const needs_t& needs, context_t const* c);
+    int     scanline_core(const needs_t& needs, context_t const* c);
+
+    enum {
+        CLEAR_LO    = 0x0001,
+        CLEAR_HI    = 0x0002,
+        CORRUPTIBLE = 0x0004,
+        FIRST       = 0x0008
+    };
+
+    enum { //load/store flags
+        WRITE_BACK  = 0x0001
+    };
+
+    struct reg_t {
+        reg_t() : reg(-1), flags(0), offset_ebp(0) {
+        }
+        reg_t(int r, int f=0, int offset=0)
+            : reg(r), flags(f), offset_ebp(offset) {
+        }
+        void setTo(int r, int f=0, int offset=0) {
+            reg=r; flags=f; offset_ebp=offset;
+        }
+        int         reg;
+        uint16_t    flags;
+        int         offset_ebp;
+    };
+
+    struct integer_t : public reg_t {
+        integer_t() : reg_t(), s(0) {
+        }
+        integer_t(int r, int sz=32, int f=0, int offset=0)
+            : reg_t(r, f, offset), s(sz) {
+        }
+        void setTo(int r, int sz=32, int f=0, int offset=0) {
+            reg_t::setTo(r, f, offset); s=sz;
+        }
+        int8_t s;
+        inline int size() const { return s; }
+    };
+
+    struct pixel_t : public reg_t {
+        pixel_t() : reg_t() {
+            memset(&format, 0, sizeof(GGLFormat));
+        }
+        pixel_t(int r, const GGLFormat* fmt, int f=0, int offset=0)
+            : reg_t(r, f, offset), format(*fmt) {
+        }
+        void setTo(int r, const GGLFormat* fmt, int f=0, int offset=0) {
+            reg_t::setTo(r, f, offset); format = *fmt;
+        }
+        GGLFormat format;
+        inline int hi(int c) const { return format.c[c].h; }
+        inline int low(int c) const { return format.c[c].l; }
+        inline int mask(int c) const { return ((1<<size(c))-1) << low(c); }
+        inline int size() const { return format.size*8; }
+        inline int size(int c) const { return component_size(c); }
+        inline int component_size(int c) const { return hi(c) - low(c); }
+    };
+
+    struct component_t : public reg_t {
+        component_t() : reg_t(), h(0), l(0) {
+        }
+        component_t(int r, int f=0, int offset=0)
+            : reg_t(r, f, offset), h(0), l(0) {
+        }
+        component_t(int r, int lo, int hi, int f=0, int offset=0)
+            : reg_t(r, f, offset), h(hi), l(lo) {
+        }
+        explicit component_t(const integer_t& rhs)
+            : reg_t(rhs.reg, rhs.flags, rhs.offset_ebp), h(rhs.s), l(0) {
+        }
+        explicit component_t(const pixel_t& rhs, int component) {
+            setTo(  rhs.reg,
+                    rhs.format.c[component].l,
+                    rhs.format.c[component].h,
+                    rhs.flags|CLEAR_LO|CLEAR_HI, rhs.offset_ebp);
+        }
+        void setTo(int r, int lo=0, int hi=0, int f=0, int offset=0) {
+            reg_t::setTo(r, f, offset); h=hi; l=lo;
+        }
+        int8_t h;
+        int8_t l;
+        inline int size() const { return h-l; }
+    };
+
+    struct pointer_t : public reg_t {
+        pointer_t() : reg_t(), size(0) {
+        }
+        pointer_t(int r, int s, int f=0, int offset=0)
+            : reg_t(r, f, offset), size(s) {
+        }
+        void setTo(int r, int s, int f=0, int offset=0) {
+            reg_t::setTo(r, f, offset); size=s;
+        }
+        int8_t size;
+    };
+
+
+private:
+    struct tex_coord_t {
+        reg_t       s;
+        reg_t       t;
+        pointer_t   ptr;
+    };
+
+    struct fragment_parts_t {
+        uint32_t    packed  : 1;
+        uint32_t    reload  : 2;
+        uint32_t    iterated_packed  : 1;
+        pixel_t     iterated;
+        pointer_t   cbPtr;
+        pointer_t   covPtr;
+        reg_t       count;
+        reg_t       argb[4];
+        reg_t       argb_dx[4];
+        reg_t       z;
+        reg_t       dither;
+        pixel_t     texel[GGL_TEXTURE_UNIT_COUNT];
+        tex_coord_t coords[GGL_TEXTURE_UNIT_COUNT];
+    };
+
+    struct texture_unit_t {
+        int         format_idx;
+        GGLFormat   format;
+        int         bits;
+        int         swrap;
+        int         twrap;
+        int         env;
+        int         pot;
+        int         linear;
+        uint8_t     mask;
+        uint8_t     replaced;
+    };
+
+    struct texture_machine_t {
+        texture_unit_t  tmu[GGL_TEXTURE_UNIT_COUNT];
+        uint8_t         mask;
+        uint8_t         replaced;
+        uint8_t         directTexture;
+        uint8_t         activeUnits;
+    };
+
+    struct component_info_t {
+        bool    masked      : 1;
+        bool    inDest      : 1;
+        bool    needed      : 1;
+        bool    replaced    : 1;
+        bool    iterated    : 1;
+        bool    smooth      : 1;
+        bool    blend       : 1;
+        bool    fog         : 1;
+    };
+
+    struct builder_context_t {
+        context_t const*    c;
+        needs_t             needs;
+        int                 Rctx;
+    };
+
+    template <typename T>
+    void modify(T& r, Scratch& regs)
+    {
+        if (!(r.flags & CORRUPTIBLE)) {
+            r.reg = regs.obtain();
+            r.flags |= CORRUPTIBLE;
+        }
+    }
+
+    // helpers
+    void    base_offset(pointer_t& d, pointer_t& b, const reg_t& o);
+
+    // texture environement
+    void    modulate(   component_t& dest,
+                        const component_t& incoming,
+                        const pixel_t& texel, int component);
+
+    void    decal(  component_t& dest,
+                    const component_t& incoming,
+                    const pixel_t& texel, int component);
+
+    void    blend(  component_t& dest,
+                    const component_t& incoming,
+                    const pixel_t& texel, int component, int tmu);
+
+    void    add(  component_t& dest,
+                    const component_t& incoming,
+                    const pixel_t& texel, int component);
+
+    // load/store stuff
+    void    store(const pointer_t& addr, const pixel_t& src, uint32_t flags=0);
+    void    load(pointer_t& addr, const pixel_t& dest, uint32_t flags=0);
+
+    void    extract(integer_t& d, const pixel_t& s, int component);
+    void    extract(component_t& d, const pixel_t& s, int component);
+    void    extract(integer_t& d, int s, int h, int l, int bits=32);
+    void    expand(integer_t& d, const integer_t& s, int dbits);
+    void    expand(integer_t& d, const component_t& s, int dbits);
+    void    expand(component_t& d, const component_t& s, int dbits);
+    void    downshift(pixel_t& d, int component, component_t s, reg_t& dither);
+
+
+    void    mul_factor( component_t& d,
+                        const integer_t& v,
+                        const integer_t& f, Scratch& scratches);
+
+    void    mul_factor_add( component_t& d,
+                            const integer_t& v,
+                            const integer_t& f,
+                            const component_t& a);
+
+    void    component_add(  component_t& d,
+                            const integer_t& dst,
+                            const integer_t& src);
+
+    void    component_sat(  const component_t& v, const int temp_reg);
+
+
+    void    build_scanline_preparation(fragment_parts_t& parts,
+                                    const needs_t& needs);
+
+    void    build_smooth_shade(fragment_parts_t& parts);
+
+    void    build_component(    pixel_t& pixel,
+                                fragment_parts_t& parts,
+                                int component,
+                                Scratch& global_scratches);
+
+    void    build_incoming_component(
+                                component_t& temp,
+                                int dst_size,
+                                fragment_parts_t& parts,
+                                int component,
+                                Scratch& scratches,
+                                Scratch& global_scratches);
+
+    void    init_iterated_color(fragment_parts_t& parts, const reg_t& x);
+
+    void    build_iterated_color(   component_t& fragment,
+                                    fragment_parts_t& parts,
+                                    int component,
+                                    Scratch& regs);
+
+    void    decodeLogicOpNeeds(const needs_t& needs);
+
+    void    decodeTMUNeeds(const needs_t& needs, context_t const* c);
+
+    void    init_textures(  tex_coord_t* coords,
+                            const reg_t& x,
+                            const reg_t& y);
+
+    void    build_textures( fragment_parts_t& parts,
+                            Scratch& regs);
+
+    void    filter8(   const fragment_parts_t& parts,
+                        pixel_t& texel, const texture_unit_t& tmu,
+                        reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+                        int FRAC_BITS, Scratch& scratches);
+
+    void    filter16(   const fragment_parts_t& parts,
+                        pixel_t& texel, const texture_unit_t& tmu,
+                        reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+                        int FRAC_BITS, Scratch& scratches);
+
+    void    filter24(   const fragment_parts_t& parts,
+                        pixel_t& texel, const texture_unit_t& tmu,
+                        int U, int V, pointer_t& txPtr,
+                        int FRAC_BITS);
+
+    void    filter32(   const fragment_parts_t& parts,
+                        pixel_t& texel, const texture_unit_t& tmu,
+                        reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+                        int FRAC_BITS, Scratch& scratches);
+
+    void    build_texture_environment(  component_t& fragment,
+                                        fragment_parts_t& parts,
+                                        int component,
+                                        Scratch& regs);
+
+    void    wrapping(   int d,
+                        int coord, int size,
+                        int tx_wrap, int tx_linear, Scratch& scratches);
+
+    void    build_fog(  component_t& temp,
+                        int component,
+                        Scratch& parent_scratches);
+
+    void    build_blending(     component_t& in_out,
+                                pixel_t& pixel,
+                                int component,
+                                Scratch& parent_scratches);
+
+    void    build_blend_factor(
+                integer_t& factor, int f, int component,
+                const pixel_t& dst_pixel,
+                integer_t& fragment,
+                integer_t& fb,
+                Scratch& scratches);
+
+    void    build_blendFOneMinusF(  component_t& temp,
+                                    const integer_t& factor,
+                                    const integer_t& fragment,
+                                    const integer_t& fb);
+
+    void    build_blendOneMinusFF(  component_t& temp,
+                                    const integer_t& factor,
+                                    const integer_t& fragment,
+                                    const integer_t& fb);
+
+    void build_coverage_application(component_t& fragment,
+                                    fragment_parts_t& parts,
+                                    Scratch& regs);
+
+    void build_alpha_test(component_t& fragment, const fragment_parts_t& parts);
+
+    enum { Z_TEST=1, Z_WRITE=2 };
+    void build_depth_test(const fragment_parts_t& parts, uint32_t mask);
+    void build_iterate_z(const fragment_parts_t& parts);
+    void build_iterate_f(const fragment_parts_t& parts);
+    void build_iterate_texture_coordinates(const fragment_parts_t& parts);
+
+    void build_logic_op(pixel_t& pixel, Scratch& regs);
+
+    void build_masking(pixel_t& pixel, Scratch& regs);
+
+    void build_and_immediate(int d, int s, uint32_t mask, int bits);
+
+    bool    isAlphaSourceNeeded() const;
+
+    enum {
+        FACTOR_SRC=1, FACTOR_DST=2, BLEND_SRC=4, BLEND_DST=8
+    };
+
+    enum {
+        LOGIC_OP=1, LOGIC_OP_SRC=2, LOGIC_OP_DST=4
+    };
+
+    static int blending_codes(int fs, int fd);
+
+    builder_context_t   mBuilderContext;
+    texture_machine_t   mTextureMachine;
+    component_info_t    mInfo[4];
+    int                 mBlending;
+    int                 mMasking;
+    int                 mAllMasked;
+    int                 mLogicOp;
+    int                 mAlphaTest;
+    int                 mAA;
+    int                 mDithering;
+    int                 mDepthTest;
+
+    int             mSmooth;
+    int             mFog;
+    pixel_t         mDstPixel;
+
+    GGLFormat       mCbFormat;
+
+    int             mBlendFactorCached;
+    integer_t       mAlphaSource;
+
+    int             mBaseRegister;
+
+    int             mBlendSrc;
+    int             mBlendDst;
+    int             mBlendSrcA;
+    int             mBlendDstA;
+
+    int             mOptLevel;
+
+    // to stretch esp and shrink esp
+    int             mCurSp;
+};
+
+// ----------------------------------------------------------------------------
+
+}; // namespace android
+
+#endif // ANDROID_GGLX86ASSEMBLER_H
diff --git a/libpixelflinger/codeflinger/x86/X86Assembler.cpp b/libpixelflinger/codeflinger/x86/X86Assembler.cpp
index e69de29..edcb769 100644
--- a/libpixelflinger/codeflinger/x86/X86Assembler.cpp
+++ b/libpixelflinger/codeflinger/x86/X86Assembler.cpp
@@ -0,0 +1,619 @@
+/* libs/pixelflinger/codeflinger/x86/X86Assembler.cpp
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#define LOG_TAG "X86Assembler"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <cutils/log.h>
+#include <cutils/properties.h>
+#include <string.h>
+
+#if defined(WITH_LIB_HARDWARE)
+#include <hardware_legacy/qemu_tracing.h>
+#endif
+
+#include <private/pixelflinger/ggl_context.h>
+
+#include "codeflinger/CodeCache.h"
+#include "codeflinger/x86/X86Assembler.h"
+
+// ----------------------------------------------------------------------------
+
+namespace android {
+
+// ----------------------------------------------------------------------------
+
+X86Assembler::X86Assembler(const sp<Assembly>& assembly)
+    :  mAssembly(assembly)
+{
+    mBase = mStream = (char *)assembly->base();
+    mDuration = ggl_system_time();
+#if defined(WITH_LIB_HARDWARE)
+    mQemuTracing = true;
+#endif
+}
+
+X86Assembler::~X86Assembler()
+{
+}
+
+char* X86Assembler::pc() const
+{
+    return mStream;
+}
+
+char* X86Assembler::base() const
+{
+    return mBase;
+}
+
+void X86Assembler::reset()
+{
+    mBase = mStream = (char *)mAssembly->base();
+    mBranchTargets.clear();
+    mLabels.clear();
+    mLabelsInverseMapping.clear();
+    mComments.clear();
+}
+
+// ----------------------------------------------------------------------------
+
+void X86Assembler::disassemble(const char* name)
+{
+    if (name) {
+        printf("%s:\n", name);
+    }
+    size_t count = pc()-base();
+    unsigned insLength;
+    unsigned insSize;
+    char* curStream = (char*)base();
+    while (count>0) {
+        ssize_t label = mLabelsInverseMapping.indexOfKey(curStream);
+        if (label >= 0) {
+            printf("%s:\n", mLabelsInverseMapping.valueAt(label));
+        }
+        ssize_t comment = mComments.indexOfKey(curStream);
+        if (comment >= 0) {
+            printf("; %s\n", mComments.valueAt(comment));
+        }
+        insLength = decodeThenPrint(curStream);
+        curStream = curStream + insLength;
+        count = count - insLength;
+    }
+}
+
+void X86Assembler::comment(const char* string)
+{
+    mComments.add(mStream, string);
+}
+
+void X86Assembler::label(const char* theLabel)
+{
+    mLabels.add(theLabel, mStream);
+    mLabelsInverseMapping.add(mStream, theLabel);
+}
+
+//the conditional jump
+void X86Assembler::JCC(Mnemonic cc, const char* label) {
+    switch (cc) {
+    case Mnemonic_JO:
+        encoder_imm(Mnemonic_JO, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNO:
+        encoder_imm(Mnemonic_JNO, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JB:
+        encoder_imm(Mnemonic_JB, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNB:
+        encoder_imm(Mnemonic_JNB, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JZ:
+        encoder_imm(Mnemonic_JZ, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNZ:
+        encoder_imm(Mnemonic_JNZ, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JBE:
+        encoder_imm(Mnemonic_JBE, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNBE:
+        encoder_imm(Mnemonic_JNBE, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JS:
+        encoder_imm(Mnemonic_JS, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNS:
+        encoder_imm(Mnemonic_JNS, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JP:
+        encoder_imm(Mnemonic_JP, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNP:
+        encoder_imm(Mnemonic_JNP, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JL:
+        encoder_imm(Mnemonic_JL, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNL:
+        encoder_imm(Mnemonic_JNL, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JLE:
+        encoder_imm(Mnemonic_JLE, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    case Mnemonic_JNLE:
+        encoder_imm(Mnemonic_JNLE, OpndSize_32,  0/*imm*/, mStream);
+        break;
+    default :
+        printf("the condition is not supported.\n");
+        return;
+    }
+    mStreamNext = mStream + encoder_get_inst_size(mStream);
+    //the offset is relative to the next instruction of the current PC
+    mBranchTargets.add(branch_target_t(label, mStream, mStreamNext));
+    mStream = mStreamNext;
+}
+
+void X86Assembler::JMP(const char* label) {
+    encoder_imm(Mnemonic_JMP, OpndSize_32,  0/*imm*/, mStream);
+    mStreamNext = mStream + encoder_get_inst_size(mStream);
+    mBranchTargets.add(branch_target_t(label, mStream, mStreamNext));
+    mStream = mStreamNext;
+}
+
+void X86Assembler::prepare_esp(int old_offset)
+{
+    mStreamUpdate = mStream;
+    SUB_IMM_TO_REG(old_offset, ESP);
+}
+
+void X86Assembler::update_esp(int new_offset)
+{
+    encoder_update_imm_rm(new_offset, mStreamUpdate);
+}
+
+void X86Assembler::shrink_esp(int shrink_offset)
+{
+    ADD_IMM_TO_REG(shrink_offset, ESP);
+}
+
+void X86Assembler::callee_work()
+{
+    //push EBX, ESI, EDI which need to be done in callee
+    /*
+    push %ebp
+    mov  %esp,%ebp
+    push %ebx
+    push %esi
+    push %edi
+    */
+    PUSH(EBP);
+    MOV_REG_TO_REG(ESP, EBP);
+    PUSH(EBX);
+    PUSH(ESI);
+    PUSH(EDI);
+}
+
+void X86Assembler::return_work()
+{
+// pop  %esi
+// pop  %edi
+// pop  %ebx
+// movl %ebp,%esp
+// pop  %ebp
+// ret
+// ret is equivalent to below
+// pop  %eax  // the return address
+// jmp  *%eax
+    POP(EDI);
+    POP(ESI);
+    POP(EBX);
+    POP(EBP);
+    encoder_return(mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+int X86Assembler::generate(const char* name)
+{
+    // fixup all the branches
+    size_t count = mBranchTargets.size();
+    while (count--) {
+        const branch_target_t& bt = mBranchTargets[count];
+        char* target_pc = mLabels.valueFor(bt.label);
+        LOG_ALWAYS_FATAL_IF(!target_pc,
+                            "error resolving branch targets, target_pc is null");
+        //the offset is relative to the next instruction of the current PC
+        int32_t offset = int32_t(target_pc - bt.next_pc);
+        encoder_update_imm(offset, bt.pc);
+    }
+
+    mAssembly->resize((int)(pc()-base()));
+
+    // the instruction cache is flushed by CodeCache
+    const int64_t duration = ggl_system_time() - mDuration;
+    const char * const format = "generated %s (%d ins size) at [%p:%p] in %lld ns\n";
+    ALOGI(format, name, int(pc()-base()), base(), pc(), duration);
+
+#if defined(WITH_LIB_HARDWARE)
+    if (__builtin_expect(mQemuTracing, 0)) {
+        int err = qemu_add_mapping(int(base()), name);
+        mQemuTracing = (err >= 0);
+    }
+#endif
+
+    char value[PROPERTY_VALUE_MAX];
+    property_get("debug.pf.disasm", value, "0");
+    if (atoi(value) != 0) {
+        printf(format, name, int(pc()-base()), base(), pc(), duration);
+        disassemble(name);
+    }
+
+    return NO_ERROR;
+}
+
+char* X86Assembler::pcForLabel(const char* label)
+{
+    return mLabels.valueFor(label);
+}
+
+// ----------------------------------------------------------------------------
+
+void X86Assembler::PUSH(int reg) {
+    encoder_reg(Mnemonic_PUSH, OpndSize_32, reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::POP(int reg) {
+    encoder_reg(Mnemonic_POP, OpndSize_32, reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+//arithmetic
+void X86Assembler::ADD_REG_TO_REG(int src, int dst) {
+    encoder_reg_reg(Mnemonic_ADD, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ADD_IMM_TO_REG(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_ADD, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ADD_IMM_TO_MEM(int imm, int disp, int dst) {
+    encoder_imm_mem(Mnemonic_ADD, OpndSize_32, imm, disp, dst, 0/*isBasePhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ADD_MEM_TO_REG(int base_reg, int disp, int dst) {
+    encoder_mem_reg(Mnemonic_ADD, OpndSize_32, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ADD_REG_TO_MEM(int src, int base_reg, int disp) {
+    encoder_reg_mem(Mnemonic_ADD, OpndSize_32, src, 0/*isPhysical*/, disp, base_reg, 0/*isBasePhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SUB_REG_TO_REG(int src, int dst) {
+    encoder_reg_reg(Mnemonic_SUB, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SUB_IMM_TO_REG(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_SUB, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SUB_IMM_TO_MEM(int imm, int disp, int dst) {
+    encoder_imm_mem(Mnemonic_SUB, OpndSize_32, imm, disp, dst, 0/*isBasePhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SUB_REG_TO_MEM(int src, int base_reg, int disp) {
+    encoder_reg_mem(Mnemonic_SUB, OpndSize_32, src, 0/*isPhysical*/, disp, base_reg, 0/*isBasePhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+//test
+void X86Assembler::TEST_REG_TO_REG(int src, int dst, OpndSize size) {
+    encoder_reg_reg(Mnemonic_TEST, size, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+//compare
+void X86Assembler::CMP_REG_TO_REG(int src, int dst, OpndSize size) {
+    encoder_reg_reg(Mnemonic_CMP, size, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::CMP_IMM_TO_REG(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_CMP, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::CMP_MEM_TO_REG(int base_reg, int disp, int dst, OpndSize size) {
+    encoder_mem_reg(Mnemonic_CMP, size, disp, base_reg, 0/*isBasePhysical*/,
+                    dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::CMP_REG_TO_MEM(int reg, int disp, int base_reg, OpndSize size)
+{
+    encoder_reg_mem(Mnemonic_CMP, size, reg, 0/*isPhysical*/, disp, base_reg, 0/*isBasePhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+//logical
+void X86Assembler::AND_REG_TO_REG(int src, int dst) {
+    encoder_reg_reg(Mnemonic_AND, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::AND_IMM_TO_REG(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_AND, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::OR_REG_TO_REG(int src, int dst) {
+    encoder_reg_reg(Mnemonic_OR, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::XOR(int src, int dst) {
+    encoder_reg_reg(Mnemonic_XOR, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::OR_IMM_TO_REG(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_OR, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::NOT(int dst) {
+    encoder_reg(Mnemonic_NOT, OpndSize_32, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::NEG(int dst) {
+    encoder_reg(Mnemonic_NEG, OpndSize_32, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+//shift
+void X86Assembler::SHL(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_SHL, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SHL(int imm, int disp, int dst) {
+    encoder_imm_mem(Mnemonic_SHL, OpndSize_32, imm, disp, dst, 0/*isBasePhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SHR(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_SHR, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SHR(int imm, int disp, int dst) {
+    encoder_imm_mem(Mnemonic_SHR, OpndSize_32, imm, disp, dst, 0/*isBasePhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::SAR(int imm, int dst) {
+    encoder_imm_reg(Mnemonic_SAR, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ROR(const int imm, int dst) {
+    encoder_imm_reg(Mnemonic_ROR, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::ROR(int imm, int disp, int dst) {
+    encoder_imm_mem(Mnemonic_ROR, OpndSize_32, imm, disp, dst, 0/*isBasePhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+//signed extension
+void X86Assembler::MOVSX_MEM_TO_REG(OpndSize size, int base_reg, int disp, int dst) {
+    encoder_moves_mem_to_reg(size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOVSX_REG_TO_REG(OpndSize size, int src, int dst) {
+    encoder_moves_reg_to_reg(size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+//zero entension
+void X86Assembler::MOVZX_MEM_TO_REG(OpndSize size, int base_reg, int disp, int dst) {
+    encoder_movez_mem_to_reg(size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOVZX_REG_TO_REG(OpndSize size, int src, int dst) {
+    encoder_movez_reg_to_reg(size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+// multiply...
+// the first source operand is placed in EAX
+void X86Assembler::IMUL(int reg) {
+    encoder_reg(Mnemonic_IMUL, OpndSize_32, reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::IMUL(int src, int dst) {
+    encoder_reg_reg(Mnemonic_IMUL, OpndSize_32, src, 0/*isPhysical*/, dst/*dst is the destination*/, 0/*isPhysical2*/,LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MUL(int reg) {
+    encoder_reg(Mnemonic_MUL, OpndSize_32, reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+
+// data transfer...
+void X86Assembler::MOV_IMM_TO_REG(int32_t imm, int dst) {
+    encoder_imm_reg(Mnemonic_MOV, OpndSize_32, imm, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOV_REG_TO_REG(int src, int dst, OpndSize size)
+{
+    if(src == dst) return;
+    encoder_reg_reg(Mnemonic_MOV, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOV_REG_TO_MEM(int reg, int disp, int base_reg, OpndSize size)
+{
+    encoder_reg_mem(Mnemonic_MOV, size, reg, 0/*isPhysical*/, disp, base_reg, 0/*isBasePhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOV_MEM_TO_REG(int disp, int base_reg, int reg, OpndSize size)
+{
+    encoder_mem_reg(Mnemonic_MOV, size, disp, base_reg, 0/*isBasePhysical*/,
+                    reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::MOV_MEM_SCALE_TO_REG(int base_reg, int index_reg, int scale, int reg, OpndSize size)
+{
+    encoder_mem_scale_reg(Mnemonic_MOV, size, base_reg, 0/*isBasePhysical*/, index_reg, 0/*isIndexPhysical*/, scale, reg, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+// the conditional move
+void X86Assembler::CMOV_REG_TO_REG(Mnemonic cc, int src, int dst, OpndSize size)
+{
+    switch (cc) {
+    case Mnemonic_CMOVO:
+        encoder_reg_reg(Mnemonic_CMOVO, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNO:
+        encoder_reg_reg(Mnemonic_CMOVNO, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVB:
+        encoder_reg_reg(Mnemonic_CMOVB, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNB:
+        encoder_reg_reg(Mnemonic_CMOVNB, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVZ:
+        encoder_reg_reg(Mnemonic_CMOVZ, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNZ:
+        encoder_reg_reg(Mnemonic_CMOVNZ, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVBE:
+        encoder_reg_reg(Mnemonic_CMOVBE, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNBE:
+        encoder_reg_reg(Mnemonic_CMOVNBE, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVS:
+        encoder_reg_reg(Mnemonic_CMOVS, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNS:
+        encoder_reg_reg(Mnemonic_CMOVNS, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVP:
+        encoder_reg_reg(Mnemonic_CMOVP, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNP:
+        encoder_reg_reg(Mnemonic_CMOVNP, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVL:
+        encoder_reg_reg(Mnemonic_CMOVL, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNL:
+        encoder_reg_reg(Mnemonic_CMOVNL, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVLE:
+        encoder_reg_reg(Mnemonic_CMOVLE, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNLE:
+        encoder_reg_reg(Mnemonic_CMOVNLE, size, src, 0/*isPhysical*/, dst, 0/*isPhysical2*/, LowOpndRegType_gp, mStream);
+        break;
+    default :
+        printf("the condition is not supported.\n");
+        return;
+    }
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+void X86Assembler::CMOV_MEM_TO_REG(Mnemonic cc, int disp, int base_reg, int dst, OpndSize size)
+{
+    switch (cc) {
+    case Mnemonic_CMOVO:
+        encoder_mem_reg(Mnemonic_CMOVO, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNO:
+        encoder_mem_reg(Mnemonic_CMOVNO, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVB:
+        encoder_mem_reg(Mnemonic_CMOVB, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNB:
+        encoder_mem_reg(Mnemonic_CMOVNB, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVZ:
+        encoder_mem_reg(Mnemonic_CMOVZ, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNZ:
+        encoder_mem_reg(Mnemonic_CMOVNZ, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVBE:
+        encoder_mem_reg(Mnemonic_CMOVBE, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNBE:
+        encoder_mem_reg(Mnemonic_CMOVNBE, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVS:
+        encoder_mem_reg(Mnemonic_CMOVS, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNS:
+        encoder_mem_reg(Mnemonic_CMOVNS, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVP:
+        encoder_mem_reg(Mnemonic_CMOVP, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNP:
+        encoder_mem_reg(Mnemonic_CMOVNP, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVL:
+        encoder_mem_reg(Mnemonic_CMOVL, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNL:
+        encoder_mem_reg(Mnemonic_CMOVNL, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVLE:
+        encoder_mem_reg(Mnemonic_CMOVLE, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    case Mnemonic_CMOVNLE:
+        encoder_mem_reg(Mnemonic_CMOVNLE, size, disp, base_reg, 0/*isBasePhysical*/, dst, 0/*isPhysical*/, LowOpndRegType_gp, mStream);
+        break;
+    default :
+        printf("the condition is not supported.\n");
+        return;
+    }
+    mStream = mStream + encoder_get_inst_size(mStream);
+}
+
+}; // namespace android
+
diff --git a/libpixelflinger/codeflinger/x86/X86Assembler.h b/libpixelflinger/codeflinger/x86/X86Assembler.h
index e69de29..69efcc3 100644
--- a/libpixelflinger/codeflinger/x86/X86Assembler.h
+++ b/libpixelflinger/codeflinger/x86/X86Assembler.h
@@ -0,0 +1,164 @@
+/* libs/pixelflinger/codeflinger/x86/X86Assembler.h
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#ifndef ANDROID_X86ASSEMBLER_H
+#define ANDROID_X86ASSEMBLER_H
+
+#include <stdint.h>
+#include <sys/types.h>
+
+#include "codeflinger/tinyutils/Vector.h"
+#include "codeflinger/tinyutils/KeyedVector.h"
+#include "codeflinger/tinyutils/smartpointer.h"
+
+#include "codeflinger/tinyutils/smartpointer.h"
+#include "codeflinger/CodeCache.h"
+#include "enc_wrapper.h"
+
+namespace android {
+
+// ----------------------------------------------------------------------------
+
+class X86Assembler
+{
+public:
+
+    enum {
+        EAX = PhysicalReg_EAX, EBX = PhysicalReg_EBX, ECX = PhysicalReg_ECX,
+        EDX = PhysicalReg_EDX, EDI = PhysicalReg_EDI, ESI = PhysicalReg_ESI,
+        ESP = PhysicalReg_ESP, EBP = PhysicalReg_EBP
+    };
+
+    X86Assembler(const sp<Assembly>& assembly);
+    ~X86Assembler();
+
+    char*   base() const;
+    char*   pc() const;
+
+
+    void        disassemble(const char* name);
+
+    // ------------------------------------------------------------------------
+    // X86AssemblerInterface...
+    // ------------------------------------------------------------------------
+
+    void    reset();
+
+    int     generate(const char* name);
+
+    void    comment(const char* string);
+
+    void    label(const char* theLabel);
+
+    void    JCC(Mnemonic cc, const char* label);
+
+    void    JMP(const char* label);
+
+    void    prepare_esp(int old_offset);
+
+    void    update_esp(int new_offset);
+
+    void    shrink_esp(int shrink_offset);
+
+    void    callee_work();
+
+    void    return_work();
+
+    char*   pcForLabel(const char* label);
+
+    void    PUSH(int reg);
+
+    void    POP(int reg);
+
+    void    ADD_REG_TO_REG(int src, int dst);
+    void    ADD_IMM_TO_REG(int imm, int dst);
+    void    ADD_IMM_TO_MEM(int imm, int disp, int dst);
+    void    ADD_MEM_TO_REG(int base_reg, int disp, int dst);
+    void    ADD_REG_TO_MEM(int src, int base_reg, int disp);
+    void    SUB_REG_TO_REG(int src, int dst);
+    void    SUB_IMM_TO_REG(int imm, int dst);
+    void    SUB_IMM_TO_MEM(int imm, int disp, int dst);
+    void    SUB_REG_TO_MEM(int src, int base_reg, int disp);
+
+    void    TEST_REG_TO_REG(int src, int dst, OpndSize size=OpndSize_32);
+    void    CMP_REG_TO_REG(int src, int dst, OpndSize size=OpndSize_32);
+    void    CMP_MEM_TO_REG(int base_reg, int disp, int dst, OpndSize size=OpndSize_32);
+    void    CMP_REG_TO_MEM(int reg, int disp, int base_reg, OpndSize size=OpndSize_32);
+    void    CMP_IMM_TO_REG(int imm, int dst);
+
+    void    AND_REG_TO_REG(int src, int dst);
+    void    AND_IMM_TO_REG(int imm, int dst);
+    void    OR_REG_TO_REG(int src, int dst);
+    void    XOR(int src, int dst);
+    void    OR_IMM_TO_REG(int imm, int dst);
+    void    NOT(int dst);
+    void    NEG(int dst);
+    void    SHL(int imm, int dst);
+    void    SHL(int imm, int disp, int dst);
+    void    SHR(int imm, int dst);
+    void    SHR(int imm, int disp, int dst);
+    void    SAR(int imm, int dst);
+    void    ROR(const int imm, int dst);
+    void    ROR(int imm, int disp, int dst);
+    void    IMUL(int reg);
+    void    IMUL(int src, int dst);
+    void    MUL(int reg);
+
+    void    MOVSX_MEM_TO_REG(OpndSize size, int base_reg, int disp, int dst);
+    void    MOVSX_REG_TO_REG(OpndSize size, int src, int dst);
+    void    MOVZX_MEM_TO_REG(OpndSize size, int base_reg, int disp, int dst);
+    void    MOVZX_REG_TO_REG(OpndSize size, int src, int dst);
+    void    MOV_IMM_TO_REG(int32_t imm, int dst);
+    void    MOV_REG_TO_REG(int src, int dst, OpndSize size=OpndSize_32);
+    void    MOV_MEM_TO_REG(int disp, int base_reg, int reg, OpndSize size=OpndSize_32);
+    void    MOV_REG_TO_MEM(int reg, int disp, int base_reg, OpndSize size=OpndSize_32);
+    void    MOV_MEM_SCALE_TO_REG(int base_reg, int index_reg, int scale, int reg, OpndSize size=OpndSize_32);
+    void    CMOV_REG_TO_REG(Mnemonic cc, int src, int dst, OpndSize size=OpndSize_32);
+    void    CMOV_MEM_TO_REG(Mnemonic cc, int disp, int base_reg, int dst, OpndSize size=OpndSize_32);
+
+
+    sp<Assembly>    mAssembly;
+    char*           mBase;
+    char*           mStream;
+    //branch target offset is relative to the next instruction
+    char*           mStreamNext;
+    //updating esp after iterating the loop
+    char*           mStreamUpdate;
+
+    int64_t         mDuration;
+#if defined(WITH_LIB_HARDWARE)
+    bool            mQemuTracing;
+#endif
+
+    struct branch_target_t {
+        inline branch_target_t() : label(0), pc(0), next_pc(0) { }
+        inline branch_target_t(const char* l, char* p, char* next_p)
+            : label(l), pc(p), next_pc(next_p) { }
+        const char* label;
+        char*   pc;
+        char*   next_pc;
+    };
+
+    Vector<branch_target_t>             mBranchTargets;
+    KeyedVector< const char*, char* >   mLabels;
+    KeyedVector< char*, const char* >   mLabelsInverseMapping;
+    KeyedVector< char*, const char* >   mComments;
+};
+
+}; // namespace android
+
+#endif //ANDROID_X86ASSEMBLER_H
diff --git a/libpixelflinger/codeflinger/x86/blending.cpp b/libpixelflinger/codeflinger/x86/blending.cpp
index e69de29..7bba3a2 100644
--- a/libpixelflinger/codeflinger/x86/blending.cpp
+++ b/libpixelflinger/codeflinger/x86/blending.cpp
@@ -0,0 +1,975 @@
+/* libs/pixelflinger/codeflinger/x86/blending.cpp
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#include <assert.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <sys/types.h>
+
+#include <cutils/log.h>
+
+#include "codeflinger/x86/GGLX86Assembler.h"
+
+
+namespace android {
+
+void GGLX86Assembler::build_fog(
+    component_t& temp,      // incomming fragment / output
+    int component,
+    Scratch& regs)
+{
+    if (mInfo[component].fog) {
+        Scratch scratches(registerFile());
+        comment("fog");
+
+        temp.reg = scratches.obtain();
+        MOV_MEM_TO_REG(temp.offset_ebp, EBP, temp.reg);
+        integer_t fragment(temp.reg, temp.h, temp.flags, temp.offset_ebp);
+        if (!(temp.flags & CORRUPTIBLE)) {
+            temp.reg = regs.obtain();
+            temp.flags |= CORRUPTIBLE;
+        }
+
+        integer_t fogColor(scratches.obtain(), 8, CORRUPTIBLE);
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        MOVZX_MEM_TO_REG(OpndSize_8, mBuilderContext.Rctx, GGL_OFFSETOF(state.fog.color[component]), fogColor.reg);
+
+        integer_t factor(scratches.obtain(), 16, CORRUPTIBLE);
+        CONTEXT_LOAD(factor.reg, generated_vars.f);
+        scratches.recycle(mBuilderContext.Rctx);
+
+        // clamp fog factor (TODO: see if there is a way to guarantee
+        // we won't overflow, when setting the iterators)
+        int temp_reg = scratches.obtain();
+        MOV_REG_TO_REG(factor.reg, temp_reg);
+        SAR(31, temp_reg);
+        NOT(temp_reg);
+        AND_REG_TO_REG(temp_reg, factor.reg);
+        MOV_IMM_TO_REG(0x10000, temp_reg);
+        CMP_IMM_TO_REG(0x10000, factor.reg);
+        CMOV_REG_TO_REG(Mnemonic_CMOVAE, temp_reg, factor.reg);
+        scratches.recycle(temp_reg);
+
+        //we will resue factor.reg
+        build_blendFOneMinusF(temp, factor, fragment, fogColor);
+        MOV_REG_TO_MEM(temp.reg, temp.offset_ebp, EBP);
+        scratches.recycle(temp.reg);
+    }
+}
+
+void GGLX86Assembler::build_blending(
+    component_t& temp,      // incomming fragment / output
+    pixel_t& pixel,   // framebuffer
+    int component,
+    Scratch& regs)
+{
+    if (!mInfo[component].blend)
+        return;
+
+    int fs = component==GGLFormat::ALPHA ? mBlendSrcA : mBlendSrc;
+    int fd = component==GGLFormat::ALPHA ? mBlendDstA : mBlendDst;
+    if (fs==GGL_SRC_ALPHA_SATURATE && component==GGLFormat::ALPHA)
+        fs = GGL_ONE;
+    const int blending = blending_codes(fs, fd);
+    if (!temp.size()) {
+        // here, blending will produce something which doesn't depend on
+        // that component (eg: GL_ZERO:GL_*), so the register has not been
+        // allocated yet. Will never be used as a source.
+        //temp = component_t(regs.obtain(), CORRUPTIBLE, temp_offset_ebp);
+        temp.reg = regs.obtain();
+        temp.flags = CORRUPTIBLE;
+        temp.h = temp.l = 0;
+    } else {
+        temp.reg = regs.obtain();
+    }
+    MOV_MEM_TO_REG(temp.offset_ebp, EBP, temp.reg);
+    // we are doing real blending...
+    // fb:          extracted dst
+    // fragment:    extracted src
+    // temp:        component_t(fragment) and result
+
+    // scoped register allocator
+    Scratch scratches(registerFile());
+    comment("blending");
+
+    // we can optimize these cases a bit...
+    // (1) saturation is not needed
+    // (2) we can use only one multiply instead of 2
+    // (3) we can reduce the register pressure
+    //      R = S*f + D*(1-f) = (S-D)*f + D
+    //      R = S*(1-f) + D*f = (D-S)*f + S
+
+    const bool same_factor_opt1 =
+        (fs==GGL_DST_COLOR && fd==GGL_ONE_MINUS_DST_COLOR) ||
+        (fs==GGL_SRC_COLOR && fd==GGL_ONE_MINUS_SRC_COLOR) ||
+        (fs==GGL_DST_ALPHA && fd==GGL_ONE_MINUS_DST_ALPHA) ||
+        (fs==GGL_SRC_ALPHA && fd==GGL_ONE_MINUS_SRC_ALPHA);
+
+    const bool same_factor_opt2 =
+        (fs==GGL_ONE_MINUS_DST_COLOR && fd==GGL_DST_COLOR) ||
+        (fs==GGL_ONE_MINUS_SRC_COLOR && fd==GGL_SRC_COLOR) ||
+        (fs==GGL_ONE_MINUS_DST_ALPHA && fd==GGL_DST_ALPHA) ||
+        (fs==GGL_ONE_MINUS_SRC_ALPHA && fd==GGL_SRC_ALPHA);
+
+
+    // XXX: we could also optimize these cases:
+    // R = S*f + D*f = (S+D)*f
+    // R = S*(1-f) + D*(1-f) = (S+D)*(1-f)
+    // R = S*D + D*S = 2*S*D
+
+
+    pixel.reg = scratches.obtain();
+    MOV_MEM_TO_REG(pixel.offset_ebp, EBP, pixel.reg);
+    // see if we need to extract 'component' from the destination (fb)
+    integer_t fb;
+    if (blending & (BLEND_DST|FACTOR_DST)) {
+        fb.setTo(scratches.obtain(), 32);
+        extract(fb, pixel, component);
+        if (mDithering) {
+            // XXX: maybe what we should do instead, is simply
+            // expand fb -or- fragment to the larger of the two
+            if (fb.size() < temp.size()) {
+                // for now we expand 'fb' to min(fragment, 8)
+                int new_size = temp.size() < 8 ? temp.size() : 8;
+                expand(fb, fb, new_size);
+            }
+        }
+    }
+
+    // convert input fragment to integer_t
+    if (temp.l && (temp.flags & CORRUPTIBLE)) {
+        SHR(temp.l, temp.reg);
+        temp.h -= temp.l;
+        temp.l = 0;
+    }
+    integer_t fragment(temp.reg, temp.size(), temp.flags, temp.offset_ebp);
+
+    // if not done yet, convert input fragment to integer_t
+    if (temp.l) {
+        // here we know temp is not CORRUPTIBLE
+        fragment.reg = scratches.obtain();
+        MOV_REG_TO_REG(temp.reg, fragment.reg);
+        SHR(temp.l, fragment.reg);
+        fragment.flags |= CORRUPTIBLE;
+    }
+
+    if (!(temp.flags & CORRUPTIBLE)) {
+        // temp is not corruptible, but since it's the destination it
+        // will be modified, so we need to allocate a new register.
+        temp.reg = regs.obtain();
+        temp.flags &= ~CORRUPTIBLE;
+        fragment.flags &= ~CORRUPTIBLE;
+    }
+
+    if ((blending & BLEND_SRC) && !same_factor_opt1) {
+        // source (fragment) is needed for the blending stage
+        // so it's not CORRUPTIBLE (unless we're doing same_factor_opt1)
+        fragment.flags &= ~CORRUPTIBLE;
+    }
+
+
+    if (same_factor_opt1) {
+        //  R = S*f + D*(1-f) = (S-D)*f + D
+        integer_t factor;
+        build_blend_factor(factor, fs,
+                           component, pixel, fragment, fb, scratches);
+        // fb is always corruptible from this point
+        fb.flags |= CORRUPTIBLE;
+        //we will reuse factor in mul_factor_add of build_blendFOneMinusF, unless factor.reg == fragment.reg == temp.reg or factor.reg == fb.reg in build_blend_factor
+        if(factor.reg == fragment.reg || factor.reg == fb.reg)
+            MOV_REG_TO_REG(factor.reg, pixel.reg);
+        else
+            scratches.recycle(pixel.reg);
+        build_blendFOneMinusF(temp, factor, fragment, fb);
+        if(factor.reg == fragment.reg || factor.reg == fb.reg) {
+            MOV_REG_TO_REG(pixel.reg, factor.reg);
+            scratches.recycle(pixel.reg);
+        }
+        scratches.recycle(fb.reg);
+        //scratches.recycle(factor.reg);
+    } else if (same_factor_opt2) {
+        //  R = S*(1-f) + D*f = (D-S)*f + S
+        integer_t factor;
+        // fb is always corrruptible here
+        fb.flags |= CORRUPTIBLE;
+        build_blend_factor(factor, fd,
+                           component, pixel, fragment, fb, scratches);
+        //we will reuse factor in mul_factor_add of build_blendFOneMinusFF, unless factor.reg == fragment.reg == temp.reg or factor.reg == fb.reg in build_blend_factor
+        if(factor.reg == fragment.reg || factor.reg == fb.reg)
+            MOV_REG_TO_REG(factor.reg, pixel.reg);
+        else
+            scratches.recycle(pixel.reg);
+        build_blendOneMinusFF(temp, factor, fragment, fb);
+        if(factor.reg == fragment.reg || factor.reg == fb.reg) {
+            MOV_REG_TO_REG(pixel.reg, factor.reg);
+            scratches.recycle(pixel.reg);
+        }
+        scratches.recycle(fb.reg);
+    } else {
+        integer_t src_factor;
+        integer_t dst_factor;
+
+        // if destination (fb) is not needed for the blending stage,
+        // then it can be marked as CORRUPTIBLE
+        if (!(blending & BLEND_DST)) {
+            fb.flags |= CORRUPTIBLE;
+        }
+
+        // XXX: try to mark some registers as CORRUPTIBLE
+        // in most case we could make those corruptible
+        // when we're processing the last component
+        // but not always, for instance
+        //    when fragment is constant and not reloaded
+        //    when fb is needed for logic-ops or masking
+        //    when a register is aliased (for instance with mAlphaSource)
+
+        // blend away...
+        if (fs==GGL_ZERO) {
+            if (fd==GGL_ZERO) {         // R = 0
+                // already taken care of
+            } else if (fd==GGL_ONE) {   // R = D
+                // already taken care of
+            } else {                    // R = D*fd
+                // compute fd
+                build_blend_factor(dst_factor, fd,
+                                   component, pixel, fragment, fb, scratches);
+                scratches.recycle(pixel.reg);
+                mul_factor(temp, fb, dst_factor, regs);
+                scratches.recycle(fb.reg);
+            }
+        } else if (fs==GGL_ONE) {
+            int temp_reg;
+            if (fd==GGL_ZERO) {     // R = S
+                // NOP, taken care of
+            } else if (fd==GGL_ONE) {   // R = S + D
+                component_add(temp, fb, fragment); // args order matters
+                temp_reg = scratches.obtain();
+                component_sat(temp, temp_reg);
+                scratches.recycle(temp_reg);
+            } else {                    // R = S + D*fd
+                // compute fd
+                build_blend_factor(dst_factor, fd,
+                                   component, pixel, fragment, fb, scratches);
+                //we will probably change src_factor in mul_factor_add, unless factor.reg == fragment.reg == temp.reg or factor.reg == fb.reg in build_blend_factor
+                if(dst_factor.reg == fragment.reg || dst_factor.reg == fb.reg)
+                    MOV_REG_TO_REG(dst_factor.reg, pixel.reg);
+                else
+                    scratches.recycle(pixel.reg);
+                mul_factor_add(temp, fb, dst_factor, component_t(fragment));
+                if(dst_factor.reg == fragment.reg || dst_factor.reg == fb.reg) {
+                    MOV_REG_TO_REG(pixel.reg, dst_factor.reg);
+                    scratches.recycle(pixel.reg);
+                }
+                temp_reg = fb.reg;
+                component_sat(temp, temp_reg);
+                scratches.recycle(fb.reg);
+            }
+        } else {
+            // compute fs
+            int temp_reg;
+            build_blend_factor(src_factor, fs,
+                               component, pixel, fragment, fb, scratches);
+            if (fd==GGL_ZERO) {         // R = S*fs
+                mul_factor(temp, fragment, src_factor, regs);
+                if (scratches.isUsed(src_factor.reg))
+                    scratches.recycle(src_factor.reg);
+            } else if (fd==GGL_ONE) {   // R = S*fs + D
+                //we will probably change src_factor in mul_factor_add, unless factor.reg == fragment.reg == temp.reg or factor.reg == fb.reg in build_blend_factor
+                if(src_factor.reg == fragment.reg || src_factor.reg == fb.reg)
+                    MOV_REG_TO_REG(src_factor.reg, pixel.reg);
+                else
+                    scratches.recycle(pixel.reg);
+                mul_factor_add(temp, fragment, src_factor, component_t(fb));
+                if(src_factor.reg == fragment.reg || src_factor.reg == fb.reg) {
+                    MOV_REG_TO_REG(pixel.reg, src_factor.reg);
+                    scratches.recycle(pixel.reg);
+                }
+                temp_reg = fb.reg;
+                component_sat(temp, temp_reg);
+                scratches.recycle(fb.reg);
+            } else {                    // R = S*fs + D*fd
+                mul_factor(temp, fragment, src_factor, regs);
+                if (scratches.isUsed(src_factor.reg))
+                    scratches.recycle(src_factor.reg);
+                // compute fd
+                build_blend_factor(dst_factor, fd,
+                                   component, pixel, fragment, fb, scratches);
+                //we will probably change dst_factor in mul_factor_add, unless factor.reg == fragment.reg == temp.reg or factor.reg == fb.reg
+                if(dst_factor.reg == fragment.reg || dst_factor.reg == fb.reg)
+                    MOV_REG_TO_REG(dst_factor.reg, pixel.reg);
+                else
+                    scratches.recycle(pixel.reg);
+                mul_factor_add(temp, fb, dst_factor, temp);
+                if(dst_factor.reg == fragment.reg || dst_factor.reg == fb.reg) {
+                    MOV_REG_TO_REG(pixel.reg, dst_factor.reg);
+                    scratches.recycle(pixel.reg);
+                }
+                if (!same_factor_opt1 && !same_factor_opt2) {
+                    temp_reg = fb.reg;
+                    component_sat(temp, temp_reg);
+                }
+                scratches.recycle(fb.reg);
+            }
+            if(scratches.isUsed(pixel.reg))
+                scratches.recycle(pixel.reg);
+        }
+    }
+    // temp is modified, but it will be used immediately in downshift
+    //printf("temp.offset_ebp: %d \n", temp.offset_ebp);
+    //below will be triggered on CDK for surfaceflinger
+    if(temp.offset_ebp == mAlphaSource.offset_ebp) {
+        mCurSp = mCurSp - 4;
+        temp.offset_ebp = mCurSp;
+    }
+    // the r, g, b value must be stored, otherwise the color of globaltime is incorrect.
+    MOV_REG_TO_MEM(temp.reg, temp.offset_ebp, EBP);
+    regs.recycle(temp.reg);
+
+    // now we can be corrupted (it's the dest)
+    temp.flags |= CORRUPTIBLE;
+}
+
+void GGLX86Assembler::build_blend_factor(
+    integer_t& factor, int f, int component,
+    const pixel_t& dst_pixel,
+    integer_t& fragment,
+    integer_t& fb,
+    Scratch& scratches)
+{
+    integer_t src_alpha(fragment);
+
+    // src_factor/dst_factor won't be used after blending,
+    // so it's fine to mark them as CORRUPTIBLE (if not aliased)
+    factor.flags |= CORRUPTIBLE;
+    int temp_reg;
+    switch(f) {
+    case GGL_ONE_MINUS_SRC_ALPHA:
+    case GGL_SRC_ALPHA:
+        if (component==GGLFormat::ALPHA && !isAlphaSourceNeeded()) {
+            // we're processing alpha, so we already have
+            // src-alpha in fragment, and we need src-alpha just this time.
+        } else {
+            // alpha-src will be needed for other components
+            factor = mAlphaSource;
+            factor.flags &= ~CORRUPTIBLE;
+            factor.reg = scratches.obtain();
+            //printf("mAlphaSource.offset_ebp: %d \n", mAlphaSource.offset_ebp);
+            //printf("fragment.offset_ebp: %d \n", fragment.offset_ebp);
+            //printf("factor.offset_ebp: %d \n", factor.offset_ebp);
+            MOV_MEM_TO_REG(mAlphaSource.offset_ebp, EBP, factor.reg);
+            if (!mBlendFactorCached || mBlendFactorCached==f) {
+                src_alpha = mAlphaSource;
+                // we already computed the blend factor before, nothing to do.
+                if (mBlendFactorCached)
+                    return;
+                // this is the first time, make sure to compute the blend
+                // factor properly.
+                mBlendFactorCached = f;
+                break;
+            } else {
+                // we have a cached alpha blend factor, but we want another one,
+                // this should really not happen because by construction,
+                // we cannot have BOTH source and destination
+                // blend factors use ALPHA *and* ONE_MINUS_ALPHA (because
+                // the blending stage uses the f/(1-f) optimization
+
+                // for completeness, we handle this case though. Since there
+                // are only 2 choices, this meens we want "the other one"
+                // (1-factor)
+                //factor = mAlphaSource;
+                //factor.flags &= ~CORRUPTIBLE;
+                NEG(factor.reg);
+                ADD_IMM_TO_REG((1<<factor.s), factor.reg);
+                MOV_REG_TO_MEM(factor.reg, factor.offset_ebp, EBP);
+                mBlendFactorCached = f;
+                return;
+            }
+        }
+        // fall-through...
+    case GGL_ONE_MINUS_DST_COLOR:
+    case GGL_DST_COLOR:
+    case GGL_ONE_MINUS_SRC_COLOR:
+    case GGL_SRC_COLOR:
+    case GGL_ONE_MINUS_DST_ALPHA:
+    case GGL_DST_ALPHA:
+    case GGL_SRC_ALPHA_SATURATE:
+        // help us find out what register we can use for the blend-factor
+        // CORRUPTIBLE registers are chosen first, or a new one is allocated.
+        if (fragment.flags & CORRUPTIBLE) {
+            factor.setTo(fragment.reg, 32, CORRUPTIBLE, fragment.offset_ebp);
+            fragment.flags &= ~CORRUPTIBLE;
+        } else if (fb.flags & CORRUPTIBLE) {
+            factor.setTo(fb.reg, 32, CORRUPTIBLE, fb.offset_ebp);
+            fb.flags &= ~CORRUPTIBLE;
+        } else {
+            factor.setTo(scratches.obtain(), 32, CORRUPTIBLE);
+            mCurSp = mCurSp - 4;
+            factor.offset_ebp = mCurSp;
+        }
+        break;
+    }
+
+    // XXX: doesn't work if size==1
+
+    switch(f) {
+    case GGL_ONE_MINUS_DST_COLOR:
+    case GGL_DST_COLOR:
+        factor.s = fb.s;
+        MOV_REG_TO_REG(fb.reg, factor.reg);
+        SHR(fb.s-1, factor.reg);
+        ADD_REG_TO_REG(fb.reg, factor.reg);
+        break;
+    case GGL_ONE_MINUS_SRC_COLOR:
+    case GGL_SRC_COLOR:
+        factor.s = fragment.s;
+        temp_reg = scratches.obtain();
+        MOV_REG_TO_REG(fragment.reg, temp_reg);
+        SHR(fragment.s-1, fragment.reg);
+        ADD_REG_TO_REG(temp_reg, fragment.reg);
+        scratches.recycle(temp_reg);
+        break;
+    case GGL_ONE_MINUS_SRC_ALPHA:
+    case GGL_SRC_ALPHA:
+        factor.s = src_alpha.s;
+        if (mBlendFactorCached == f) {
+            //src_alpha == factor == mAlphaSource, we need a temp reg
+            if(scratches.countFreeRegs()) {
+                temp_reg = scratches.obtain();
+                MOV_REG_TO_REG(factor.reg, temp_reg);
+                SHR(src_alpha.s-1, factor.reg);
+                ADD_REG_TO_REG(temp_reg, factor.reg);
+                scratches.recycle(temp_reg);
+            }
+            else {
+                SHR(src_alpha.s-1, factor.offset_ebp, EBP);
+                ADD_MEM_TO_REG(EBP, factor.offset_ebp, factor.reg);
+            }
+        }
+        else
+        {
+            MOV_REG_TO_REG(src_alpha.reg, factor.reg);
+            SHR(src_alpha.s-1, factor.reg);
+            ADD_REG_TO_REG(src_alpha.reg, factor.reg);
+        }
+        // we will store factor in the next switch for GGL_ONE_MINUS_SRC_ALPHA
+        if(f == GGL_SRC_ALPHA)
+            MOV_REG_TO_MEM(factor.reg, factor.offset_ebp, EBP);
+        break;
+    case GGL_ONE_MINUS_DST_ALPHA:
+    case GGL_DST_ALPHA:
+        // XXX: should be precomputed
+        extract(factor, dst_pixel, GGLFormat::ALPHA);
+        temp_reg = scratches.obtain();
+        MOV_REG_TO_REG(factor.reg, temp_reg);
+        SHR(factor.s-1, factor.reg);
+        ADD_REG_TO_REG(temp_reg, factor.reg);
+        scratches.recycle(temp_reg);
+        break;
+    case GGL_SRC_ALPHA_SATURATE:
+        // XXX: should be precomputed
+        // XXX: f = min(As, 1-Ad)
+        // btw, we're guaranteed that Ad's size is <= 8, because
+        // it's extracted from the framebuffer
+        break;
+    }
+
+    switch(f) {
+    case GGL_ONE_MINUS_DST_COLOR:
+    case GGL_ONE_MINUS_SRC_COLOR:
+    case GGL_ONE_MINUS_DST_ALPHA:
+    case GGL_ONE_MINUS_SRC_ALPHA:
+        NEG(factor.reg);
+        ADD_IMM_TO_REG(1<<factor.s, factor.reg);
+        MOV_REG_TO_MEM(factor.reg, factor.offset_ebp, EBP);
+    }
+
+    // don't need more than 8-bits for the blend factor
+    // and this will prevent overflows in the multiplies later
+    if (factor.s > 8) {
+        SHR(factor.s-8, factor.reg);
+        factor.s = 8;
+        if(f == GGL_ONE_MINUS_SRC_ALPHA || f == GGL_SRC_ALPHA)
+            MOV_REG_TO_MEM(factor.reg, factor.offset_ebp, EBP);
+    }
+    //below will be triggered on CDK for surfaceflinger
+    if(fragment.offset_ebp == mAlphaSource.offset_ebp)
+        MOV_REG_TO_REG(factor.reg, fragment.reg);
+}
+
+int GGLX86Assembler::blending_codes(int fs, int fd)
+{
+    int blending = 0;
+    switch(fs) {
+    case GGL_ONE:
+        blending |= BLEND_SRC;
+        break;
+
+    case GGL_ONE_MINUS_DST_COLOR:
+    case GGL_DST_COLOR:
+        blending |= FACTOR_DST|BLEND_SRC;
+        break;
+    case GGL_ONE_MINUS_DST_ALPHA:
+    case GGL_DST_ALPHA:
+        // no need to extract 'component' from the destination
+        // for the blend factor, because we need ALPHA only.
+        blending |= BLEND_SRC;
+        break;
+
+    case GGL_ONE_MINUS_SRC_COLOR:
+    case GGL_SRC_COLOR:
+        blending |= FACTOR_SRC|BLEND_SRC;
+        break;
+    case GGL_ONE_MINUS_SRC_ALPHA:
+    case GGL_SRC_ALPHA:
+    case GGL_SRC_ALPHA_SATURATE:
+        blending |= FACTOR_SRC|BLEND_SRC;
+        break;
+    }
+    switch(fd) {
+    case GGL_ONE:
+        blending |= BLEND_DST;
+        break;
+
+    case GGL_ONE_MINUS_DST_COLOR:
+    case GGL_DST_COLOR:
+        blending |= FACTOR_DST|BLEND_DST;
+        break;
+    case GGL_ONE_MINUS_DST_ALPHA:
+    case GGL_DST_ALPHA:
+        blending |= FACTOR_DST|BLEND_DST;
+        break;
+
+    case GGL_ONE_MINUS_SRC_COLOR:
+    case GGL_SRC_COLOR:
+        blending |= FACTOR_SRC|BLEND_DST;
+        break;
+    case GGL_ONE_MINUS_SRC_ALPHA:
+    case GGL_SRC_ALPHA:
+        // no need to extract 'component' from the source
+        // for the blend factor, because we need ALPHA only.
+        blending |= BLEND_DST;
+        break;
+    }
+    return blending;
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::build_blendFOneMinusF(
+    component_t& temp,
+    const integer_t& factor,
+    const integer_t& fragment,
+    const integer_t& fb)
+{
+    //  R = S*f + D*(1-f) = (S-D)*f + D
+    // compute S-D
+    Scratch scratches(registerFile());
+    integer_t diff(fragment.flags & CORRUPTIBLE ?
+                   fragment.reg : scratches.obtain(), fb.size(), CORRUPTIBLE);
+    const int shift = fragment.size() - fb.size();
+    if (shift>0) {
+        MOV_REG_TO_REG(fragment.reg, diff.reg);
+        SHR(shift, diff.reg);
+        SUB_REG_TO_REG(fb.reg, diff.reg);
+    } else if (shift<0) {
+        MOV_REG_TO_REG(fragment.reg, diff.reg);
+        SHL(-shift, diff.reg);
+        SUB_REG_TO_REG(fb.reg, diff.reg);
+    } else  {
+        MOV_REG_TO_REG(fragment.reg, diff.reg);
+        SUB_REG_TO_REG(fb.reg, diff.reg);
+    }
+    mul_factor_add(temp, diff, factor, component_t(fb));
+    if(!(fragment.flags & CORRUPTIBLE))
+        scratches.recycle(diff.reg);
+}
+
+void GGLX86Assembler::build_blendOneMinusFF(
+    component_t& temp,
+    const integer_t& factor,
+    const integer_t& fragment,
+    const integer_t& fb)
+{
+    //  R = S*f + D*(1-f) = (S-D)*f + D
+    Scratch scratches(registerFile());
+    // compute D-S
+    integer_t diff(fb.flags & CORRUPTIBLE ?
+                   fb.reg : scratches.obtain(), fb.size(), CORRUPTIBLE);
+    const int shift = fragment.size() - fb.size();
+    if (shift>0) {
+        SHR(shift, fragment.reg);
+        MOV_REG_TO_REG(fb.reg, diff.reg);
+        SUB_REG_TO_REG(fragment.reg, diff.reg);
+    }
+    else if (shift<0) {
+        SHR(-shift, fragment.reg);
+        MOV_REG_TO_REG(fb.reg, diff.reg);
+        SUB_REG_TO_REG(fragment.reg, diff.reg);
+    }
+    else    {
+        MOV_REG_TO_REG(fb.reg, diff.reg);
+        SUB_REG_TO_REG(fragment.reg, diff.reg);
+    }
+
+    mul_factor_add(temp, diff, factor, component_t(fragment));
+    if(!(fragment.flags & CORRUPTIBLE))
+        scratches.recycle(diff.reg);
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::mul_factor(  component_t& d,
+                                   const integer_t& v,
+                                   const integer_t& f, Scratch& scratches)
+{
+    // f can be changed
+    //
+    int vs = v.size();
+    int fs = f.size();
+    int ms = vs+fs;
+
+    // XXX: we could have special cases for 1 bit mul
+
+    // all this code below to use the best multiply instruction
+    // wrt the parameters size. We take advantage of the fact
+    // that the 16-bits multiplies allow a 16-bit shift
+    // The trick is that we just make sure that we have at least 8-bits
+    // per component (which is enough for a 8 bits display).
+
+    int xy = -1;
+    int vshift = 0;
+    int fshift = 0;
+    int smulw = 0;
+
+    int xyBB = 0;
+    int xyTB = 1;
+    int xyTT = 2;
+    int xyBT = 3;
+    if (vs<16) {
+        if (fs<16) {
+            xy = xyBB;
+        } else if (GGL_BETWEEN(fs, 24, 31)) {
+            ms -= 16;
+            xy = xyTB;
+        } else {
+            // eg: 15 * 18  ->  15 * 15
+            fshift = fs - 15;
+            ms -= fshift;
+            xy = xyBB;
+        }
+    } else if (GGL_BETWEEN(vs, 24, 31)) {
+        if (fs<16) {
+            ms -= 16;
+            xy = xyTB;
+        } else if (GGL_BETWEEN(fs, 24, 31)) {
+            ms -= 32;
+            xy = xyTT;
+        } else {
+            // eg: 24 * 18  ->  8 * 18
+            fshift = fs - 15;
+            ms -= 16 + fshift;
+            xy = xyTB;
+        }
+    } else {
+        if (fs<16) {
+            // eg: 18 * 15  ->  15 * 15
+            vshift = vs - 15;
+            ms -= vshift;
+            xy = xyBB;
+        } else if (GGL_BETWEEN(fs, 24, 31)) {
+            // eg: 18 * 24  ->  15 * 8
+            vshift = vs - 15;
+            ms -= 16 + vshift;
+            xy = xyBT;
+        } else {
+            // eg: 18 * 18  ->  (15 * 18)>>16
+            fshift = fs - 15;
+            ms -= 16 + fshift;
+            //xy = yB;    //XXX SMULWB
+            smulw = 1;
+        }
+    }
+
+    ALOGE_IF(ms>=32, "mul_factor overflow vs=%d, fs=%d", vs, fs);
+
+    int vreg = v.reg;
+    int freg = f.reg;
+    if (vshift) {
+        MOV_REG_TO_REG(vreg, d.reg);
+        SHR(vshift, d.reg);
+        vreg = d.reg;
+    }
+    if (fshift) {
+        MOV_REG_TO_REG(vreg, d.reg);
+        SHR(fshift, d.reg);
+        freg = d.reg;
+    }
+    MOV_REG_TO_REG(vreg, d.reg);
+    if (smulw) {
+        int flag_push_edx = 0;
+        int flag_reserve_edx = 0;
+        int temp_reg2 = -1;
+        int edx_offset_ebp = 0;
+        if(scratches.isUsed(EDX) == 1) {
+            if(d.reg != EDX) {
+                flag_push_edx = 1;
+                mCurSp = mCurSp - 4;
+                edx_offset_ebp = mCurSp;
+                MOV_REG_TO_MEM(EDX, edx_offset_ebp, EBP);
+                //PUSH(EDX);
+            }
+        }
+        else {
+            flag_reserve_edx = 1;
+            scratches.reserve(EDX);
+        }
+        if(scratches.isUsed(EAX)) {
+            if( freg == EAX || d.reg == EAX) {
+                MOVSX_REG_TO_REG(OpndSize_16, freg, freg);
+                if(freg == EAX)
+                    IMUL(d.reg);
+                else
+                    IMUL(freg);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, d.reg);
+            }
+            else {
+                int eax_offset_ebp = 0;
+                if(scratches.countFreeRegs() > 0) {
+                    temp_reg2 = scratches.obtain();
+                    MOV_REG_TO_REG(EAX, temp_reg2);
+                }
+                else {
+                    mCurSp = mCurSp - 4;
+                    eax_offset_ebp = mCurSp;
+                    MOV_REG_TO_MEM(EAX, eax_offset_ebp, EBP);
+                    //PUSH(EAX);
+                }
+                MOV_REG_TO_REG(freg, EAX);
+                MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                IMUL(d.reg);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, d.reg);
+                if(temp_reg2 > -1) {
+                    MOV_REG_TO_REG(temp_reg2, EAX);
+                    scratches.recycle(temp_reg2);
+                }
+                else {
+                    MOV_MEM_TO_REG(eax_offset_ebp, EBP, EAX);
+                    //POP(EAX);
+                }
+            }
+        }
+        else {
+            MOV_REG_TO_REG(freg, EAX);
+            MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+            IMUL(d.reg);
+            SHL(16, EDX);
+            SHR(16, EAX);
+            MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+            MOV_REG_TO_REG(EDX, d.reg);
+        }
+        if(flag_push_edx == 1) {
+            MOV_MEM_TO_REG(edx_offset_ebp, EBP, EDX);
+            //POP(EDX);
+        }
+        if(flag_reserve_edx ==1)
+            scratches.recycle(EDX);
+    }
+    else {
+        if(xy == xyBB) {
+            MOVSX_REG_TO_REG(OpndSize_16, d.reg, d.reg);
+            MOVSX_REG_TO_REG(OpndSize_16, freg, freg);
+            IMUL(freg, d.reg);
+        }
+        else if(xy == xyTB) {
+            SHR(16, d.reg);
+            MOVSX_REG_TO_REG(OpndSize_16, d.reg, d.reg);
+            MOVSX_REG_TO_REG(OpndSize_16, freg, freg);
+            IMUL(freg, d.reg);
+        }
+        else if(xy == xyBT) {
+            MOVSX_REG_TO_REG(OpndSize_16, d.reg, d.reg);
+            SHR(16, freg);
+            MOVSX_REG_TO_REG(OpndSize_16, freg, freg);
+            IMUL(freg, d.reg);
+        }
+        else if(xy == xyTT) {
+            SHR(16, d.reg);
+            MOVSX_REG_TO_REG(OpndSize_16, d.reg, d.reg);
+            SHR(16, freg);
+            MOVSX_REG_TO_REG(OpndSize_16, freg, freg);
+            IMUL(freg, d.reg);
+        }
+    }
+
+
+    d.h = ms;
+    if (mDithering) {
+        d.l = 0;
+    } else {
+        d.l = fs;
+        d.flags |= CLEAR_LO;
+    }
+}
+
+void GGLX86Assembler::mul_factor_add(  component_t& d,
+                                       const integer_t& v,
+                                       const integer_t& f,
+                                       const component_t& a)
+{
+    // XXX: we could have special cases for 1 bit mul
+    Scratch scratches(registerFile());
+
+    int vs = v.size();
+    int fs = f.size();
+    int as = a.h;
+    int ms = vs+fs;
+
+    ALOGE_IF(ms>=32, "mul_factor_add overflow vs=%d, fs=%d, as=%d", vs, fs, as);
+
+    integer_t add(a.reg, a.h, a.flags, a.offset_ebp);
+
+
+    // 'a' is a component_t but it is guaranteed to have
+    // its high bits set to 0. However in the dithering case,
+    // we can't get away with truncating the potentially bad bits
+    // so extraction is needed.
+
+    if ((mDithering) && (a.size() < ms)) {
+        // we need to expand a
+        if (!(a.flags & CORRUPTIBLE)) {
+            // ... but it's not corruptible, so we need to pick a
+            // temporary register.
+            // Try to uses the destination register first (it's likely
+            // to be usable, unless it aliases an input).
+            if (d.reg!=a.reg && d.reg!=v.reg && d.reg!=f.reg) {
+                add.reg = d.reg;
+            } else {
+                add.reg = scratches.obtain();
+            }
+        }
+        expand(add, a, ms); // extracts and expands
+        as = ms;
+    }
+
+    if (ms == as) {
+        MOV_REG_TO_REG(v.reg, d.reg);
+        if (vs<16 && fs<16) {
+            MOVSX_REG_TO_REG(OpndSize_16, d.reg, d.reg);
+            MOVSX_REG_TO_REG(OpndSize_16, f.reg, f.reg);
+            IMUL(f.reg, d.reg);
+        }
+        else
+            IMUL(f.reg, d.reg);
+        ADD_REG_TO_REG(add.reg, d.reg);
+    } else {
+        //int temp = d.reg;
+        //if (temp == add.reg) {
+        //    // the mul will modify add.reg, we need an intermediary reg
+        //    if (v.flags & CORRUPTIBLE)      temp = v.reg;
+        //    else if (f.flags & CORRUPTIBLE) temp = f.reg;
+        //    else                            temp = scratches.obtain();
+        //}
+
+        // below d.reg may override "temp" result, so we use a new register
+        int temp_reg;
+        int v_offset_ebp = 0;
+        if(scratches.countFreeRegs() == 0) {
+            temp_reg = v.reg;
+            mCurSp = mCurSp - 4;
+            v_offset_ebp = mCurSp;
+            MOV_REG_TO_MEM(v.reg, v_offset_ebp, EBP);
+        }
+        else {
+            temp_reg = scratches.obtain();
+            MOV_REG_TO_REG(v.reg, temp_reg);
+        }
+        if (vs<16 && fs<16) {
+            MOVSX_REG_TO_REG(OpndSize_16, temp_reg, temp_reg);
+            MOVSX_REG_TO_REG(OpndSize_16, f.reg, f.reg);
+            IMUL(f.reg, temp_reg);
+        }
+        else
+            IMUL(f.reg, temp_reg);
+
+        if (ms>as) {
+            MOV_REG_TO_REG(add.reg, d.reg);
+            SHL(ms-as, d.reg);
+            ADD_REG_TO_REG(temp_reg, d.reg);
+        } else if (ms<as) {
+            // not sure if we should expand the mul instead?
+            MOV_REG_TO_REG(add.reg, d.reg);
+            SHL(as-ms, d.reg);
+            ADD_REG_TO_REG(temp_reg, d.reg);
+        }
+        if(temp_reg == v.reg)
+            MOV_MEM_TO_REG(v_offset_ebp, EBP, v.reg);
+        else
+            scratches.recycle(temp_reg);
+    }
+
+    d.h = ms;
+    if (mDithering) {
+        d.l = a.l;
+    } else {
+        d.l = fs>a.l ? fs : a.l;
+        d.flags |= CLEAR_LO;
+    }
+}
+
+void GGLX86Assembler::component_add(component_t& d,
+                                    const integer_t& dst, const integer_t& src)
+{
+    // here we're guaranteed that fragment.size() >= fb.size()
+    const int shift = src.size() - dst.size();
+    if (!shift) {
+        MOV_REG_TO_REG(src.reg, d.reg);
+        ADD_REG_TO_REG(dst.reg, d.reg);
+    } else {
+        MOV_REG_TO_REG(dst.reg, d.reg);
+        SHL(shift, d.reg);
+        ADD_REG_TO_REG(src.reg, d.reg);
+    }
+
+    d.h = src.size();
+    if (mDithering) {
+        d.l = 0;
+    } else {
+        d.l = shift;
+        d.flags |= CLEAR_LO;
+    }
+}
+
+void GGLX86Assembler::component_sat(const component_t& v, const int temp_reg)
+{
+    const int32_t one = ((1<<v.size())-1)<<v.l;
+    MOV_IMM_TO_REG(one, temp_reg);
+    CMP_IMM_TO_REG(1<<v.h, v.reg);
+    CMOV_REG_TO_REG(Mnemonic_CMOVAE, temp_reg, v.reg);
+}
+
+// ----------------------------------------------------------------------------
+
+}; // namespace android
+
diff --git a/libpixelflinger/codeflinger/x86/load_store.cpp b/libpixelflinger/codeflinger/x86/load_store.cpp
index e69de29..a427411 100644
--- a/libpixelflinger/codeflinger/x86/load_store.cpp
+++ b/libpixelflinger/codeflinger/x86/load_store.cpp
@@ -0,0 +1,458 @@
+/* libs/pixelflinger/codeflinger/x86/load_store.cpp
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#include <assert.h>
+#include <stdio.h>
+#include <cutils/log.h>
+
+#include "codeflinger/x86/GGLX86Assembler.h"
+
+namespace android {
+
+// ----------------------------------------------------------------------------
+
+void GGLX86Assembler::store(const pointer_t& addr, const pixel_t& s, uint32_t flags)
+{
+    const int bits = addr.size;
+    const int inc = (flags & WRITE_BACK)?1:0;
+    switch (bits) {
+    case 32:
+        if (inc) {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg);
+            ADD_IMM_TO_REG(4, addr.reg);
+        } else {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg);
+        }
+        break;
+    case 24:
+        // 24 bits formats are a little special and used only for RGB
+        // 0x00BBGGRR is unpacked as R,G,B
+        MOV_REG_TO_MEM(s.reg, 0, addr.reg, OpndSize_8);
+        ROR(8, s.reg);
+        MOV_REG_TO_MEM(s.reg, 1, addr.reg, OpndSize_8);
+        ROR(8, s.reg);
+        MOV_REG_TO_MEM(s.reg, 2, addr.reg, OpndSize_8);
+        if (!(s.flags & CORRUPTIBLE)) {
+            ROR(16, s.reg);
+        }
+        if (inc) {
+            ADD_IMM_TO_REG(3, addr.reg);
+        }
+        break;
+    case 16:
+        if (inc) {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg,OpndSize_16);
+            ADD_IMM_TO_REG(2, addr.reg);
+        } else {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg,OpndSize_16);
+        }
+        break;
+    case  8:
+        if (inc) {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg,OpndSize_8);
+            ADD_IMM_TO_REG(1, addr.reg);
+        } else {
+            MOV_REG_TO_MEM(s.reg, 0, addr.reg,OpndSize_8);
+        }
+        break;
+    }
+}
+
+void GGLX86Assembler::load(pointer_t& addr, const pixel_t& s, uint32_t flags)
+{
+    Scratch scratches(registerFile());
+    int s0;
+
+    const int bits = addr.size;
+    // WRITE_BACK indicates that the base register will also be updated after loading the data
+    const int inc = (flags & WRITE_BACK)?1:0;
+    switch (bits) {
+    case 32:
+        if (inc) {
+            MOV_MEM_TO_REG(0, addr.reg, s.reg);
+            ADD_IMM_TO_REG(4, addr.reg);
+
+        } else        MOV_MEM_TO_REG(0, addr.reg, s.reg);
+        break;
+    case 24:
+        // 24 bits formats are a little special and used only for RGB
+        // R,G,B is packed as 0x00BBGGRR
+        s0 = scratches.obtain();
+        if (s.reg != addr.reg) {
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 0, s.reg); //R
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 1, s0);   //G
+            SHL(8, s0);
+            OR_REG_TO_REG(s0, s.reg);
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 2, s0); //B
+            SHL(16, s0);
+            OR_REG_TO_REG(s0, s.reg);
+        } else {
+            int s1 = scratches.obtain();
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 0, s1); //R
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 1, s0); //G
+            SHL(8, s0);
+            OR_REG_TO_REG(s0, s1);
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 2, s0); //B
+            SHL(16, s0);
+            OR_REG_TO_REG(s0, s1);
+            MOV_REG_TO_REG(s1, s.reg);
+            scratches.recycle(s1);
+
+        }
+        scratches.recycle(s0);
+        if (inc)
+            ADD_IMM_TO_REG(3, addr.reg);
+        break;
+    case 16:
+        if (inc) {
+            MOVZX_MEM_TO_REG(OpndSize_16, addr.reg, 0, s.reg);
+            ADD_IMM_TO_REG(2, addr.reg);
+        }
+        else  MOVZX_MEM_TO_REG(OpndSize_16, addr.reg, 0, s.reg);
+        break;
+    case  8:
+        if (inc) {
+            MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 0, s.reg);
+            ADD_IMM_TO_REG(1, addr.reg);
+        }
+        else        MOVZX_MEM_TO_REG(OpndSize_8, addr.reg, 0, s.reg);
+        break;
+    }
+    if (inc) MOV_REG_TO_MEM(addr.reg, addr.offset_ebp, PhysicalReg_EBP);
+}
+
+void GGLX86Assembler::extract(integer_t& d, int s, int h, int l, int bits)
+{
+    const int maskLen = h-l;
+
+    assert(maskLen<=8);
+    assert(h);
+
+
+    if (h != bits) {
+        const int mask = ((1<<maskLen)-1) << l;
+        MOV_REG_TO_REG(s, d.reg);
+        AND_IMM_TO_REG(mask, d.reg);// component = packed & mask;
+        s = d.reg;
+    }
+
+    if (l) {
+        MOV_REG_TO_REG(s, d.reg);
+        SHR(l, d.reg);// component = packed >> l;
+        s = d.reg;
+    }
+
+    if (s != d.reg) {
+        MOV_REG_TO_REG(s, d.reg);
+    }
+
+    d.s = maskLen;
+}
+
+void GGLX86Assembler::extract(integer_t& d, const pixel_t& s, int component)
+{
+    extract(d,  s.reg,
+            s.format.c[component].h,
+            s.format.c[component].l,
+            s.size());
+}
+
+void GGLX86Assembler::extract(component_t& d, const pixel_t& s, int component)
+{
+    integer_t r(d.reg, 32, d.flags, d.offset_ebp);
+    extract(r,  s.reg,
+            s.format.c[component].h,
+            s.format.c[component].l,
+            s.size());
+    d = component_t(r);
+}
+
+
+void GGLX86Assembler::expand(integer_t& d, const component_t& s, int dbits)
+{
+    if (s.l || (s.flags & CLEAR_HI)) {
+        extract(d, s.reg, s.h, s.l, 32);
+        expand(d, d, dbits);
+    } else {
+        expand(d, integer_t(s.reg, s.size(), s.flags, s.offset_ebp), dbits);
+    }
+}
+
+void GGLX86Assembler::expand(component_t& d, const component_t& s, int dbits)
+{
+    integer_t r(d.reg, 32, d.flags, d.offset_ebp);
+    expand(r, s, dbits);
+    d = component_t(r);
+}
+
+void GGLX86Assembler::expand(integer_t& dst, const integer_t& src, int dbits)
+{
+    assert(src.size());
+
+    Scratch scratches(registerFile());
+    int sbits = src.size();
+    int s = src.reg;
+    int d = dst.reg;
+
+    // be sure to set 'dst' after we read 'src' as they may be identical
+    dst.s = dbits;
+    dst.flags = 0;
+
+    if (dbits<=sbits) {
+        if (s != d) {
+            MOV_REG_TO_REG(s, d);
+        }
+        return;
+    }
+
+    if (sbits == 1) {
+        MOV_REG_TO_REG(s, d);
+        SHL(dbits, d);
+        SUB_REG_TO_REG(s, d);
+        // d = (s<<dbits) - s;
+        return;
+    }
+
+    if (dbits % sbits) {
+        MOV_REG_TO_REG(s, d);
+        SHL(dbits-sbits, d);
+        // d = s << (dbits-sbits);
+        dbits -= sbits;
+        int temp = scratches.obtain();
+        do {
+            MOV_REG_TO_REG(d, temp);
+            SHR(sbits, temp);
+            OR_REG_TO_REG(temp, d);
+            // d |= d >> sbits;
+            dbits -= sbits;
+            sbits *= 2;
+        } while(dbits>0);
+        return;
+    }
+
+    dbits -= sbits;
+    do {
+        MOV_REG_TO_REG(s, d);
+        SHL(sbits, d);
+        OR_REG_TO_REG(s, d);
+        // d |= d<<sbits;
+        s = d;
+        dbits -= sbits;
+        if (sbits*2 < dbits) {
+            sbits *= 2;
+        }
+    } while(dbits>0);
+}
+
+void GGLX86Assembler::downshift(
+    pixel_t& d, int component, component_t s, reg_t& dither)
+{
+    const needs_t& needs = mBuilderContext.needs;
+    Scratch scratches(registerFile());
+    // s(temp) is loaded in build_blending
+    s.reg = scratches.obtain();
+    MOV_MEM_TO_REG(s.offset_ebp, EBP, s.reg);
+
+    int sh = s.h;
+    int sl = s.l;
+    int maskHiBits = (sh!=32) ? ((s.flags & CLEAR_HI)?1:0) : 0;
+    int maskLoBits = (sl!=0)  ? ((s.flags & CLEAR_LO)?1:0) : 0;
+    int sbits = sh - sl;
+
+    int dh = d.format.c[component].h;
+    int dl = d.format.c[component].l;
+    int dbits = dh - dl;
+    int dithering = 0;
+
+    ALOGE_IF(sbits<dbits, "sbits (%d) < dbits (%d) in downshift", sbits, dbits);
+
+    if (sbits>dbits) {
+        // see if we need to dither
+        dithering = mDithering;
+    }
+
+    int ireg = d.reg;
+    if (!(d.flags & FIRST)) {
+        if (s.flags & CORRUPTIBLE)  {
+            ireg = s.reg;
+        } else {
+            ireg = scratches.obtain();
+        }
+    }
+    d.flags &= ~FIRST;
+
+    if (maskHiBits) {
+        // we need to mask the high bits (and possibly the lowbits too)
+        // and we might be able to use immediate mask.
+        if (!dithering) {
+            // we don't do this if we only have maskLoBits because we can
+            // do it more efficiently below (in the case where dl=0)
+            const int offset = sh - dbits;
+            if (dbits<=8 && offset >= 0) {
+                const uint32_t mask = ((1<<dbits)-1) << offset;
+                build_and_immediate(ireg, s.reg, mask, 32);
+                s.reg = ireg;
+                sl = offset;
+                sbits = dbits;
+                maskLoBits = maskHiBits = 0;
+            }
+        } else {
+            // in the dithering case though, we need to preserve the lower bits
+            const uint32_t mask = ((1<<sbits)-1) << sl;
+            build_and_immediate(ireg, s.reg, mask, 32);
+            s.reg = ireg;
+            maskLoBits = maskHiBits = 0;
+        }
+    }
+
+    // XXX: we could special case (maskHiBits & !maskLoBits)
+    // like we do for maskLoBits below, but it happens very rarely
+    // that we have maskHiBits only and the conditions necessary to lead
+    // to better code (like doing d |= s << 24)
+
+    if (maskHiBits) {
+        MOV_REG_TO_REG(s.reg, ireg);
+        SHL(32-sh, ireg);
+        sl += 32-sh;
+        sh = 32;
+        s.reg = ireg;
+        maskHiBits = 0;
+    }
+
+    //  Downsampling should be performed as follows:
+    //  V * ((1<<dbits)-1) / ((1<<sbits)-1)
+    //  V * [(1<<dbits)/((1<<sbits)-1) - 1/((1<<sbits)-1)]
+    //  V * [1/((1<<sbits)-1)>>dbits - 1/((1<<sbits)-1)]
+    //  V/((1<<(sbits-dbits))-(1>>dbits)) - (V>>sbits)/((1<<sbits)-1)>>sbits
+    //  V/((1<<(sbits-dbits))-(1>>dbits)) - (V>>sbits)/(1-(1>>sbits))
+    //
+    //  By approximating (1>>dbits) and (1>>sbits) to 0:
+    //
+    //  V>>(sbits-dbits) - V>>sbits
+    //
+    //  A good approximation is V>>(sbits-dbits),
+    //  but better one (needed for dithering) is:
+    //
+    //  (V>>(sbits-dbits)<<sbits - V)>>sbits
+    //  (V<<dbits - V)>>sbits
+    //  (V - V>>dbits)>>(sbits-dbits)
+
+    // Dithering is done here
+    if (dithering) {
+        comment("dithering");
+        if (sl) {
+            MOV_REG_TO_REG(s.reg, ireg);
+            SHR(sl, ireg);
+            sh -= sl;
+            sl = 0;
+            s.reg = ireg;
+        }
+        // scaling (V-V>>dbits)
+        int temp_reg = scratches.obtain();
+        MOV_REG_TO_REG(s.reg, temp_reg);
+        SHR(dbits, temp_reg);
+        MOV_REG_TO_REG(s.reg, ireg);
+        SUB_REG_TO_REG(temp_reg, ireg);
+        scratches.recycle(temp_reg);
+        const int shift = (GGL_DITHER_BITS - (sbits-dbits));
+        dither.reg = scratches.obtain();
+        MOV_MEM_TO_REG(dither.offset_ebp, EBP, dither.reg);
+        if (shift>0)  {
+            temp_reg = scratches.obtain();
+            MOV_REG_TO_REG(dither.reg, temp_reg);
+            SHR(shift, temp_reg);
+            ADD_REG_TO_REG(temp_reg, ireg);
+            scratches.recycle(temp_reg);
+        }
+        else if (shift<0) {
+            temp_reg = scratches.obtain();
+            MOV_REG_TO_REG(dither.reg, temp_reg);
+            SHL(-shift, temp_reg);
+            ADD_REG_TO_REG(temp_reg, ireg);
+            scratches.recycle(temp_reg);
+        }
+        else {
+            ADD_REG_TO_REG(dither.reg, ireg);
+        }
+        scratches.recycle(dither.reg);
+        s.reg = ireg;
+    }
+
+    if ((maskLoBits|dithering) && (sh > dbits)) {
+        int shift = sh-dbits;
+        if (dl) {
+            MOV_REG_TO_REG(s.reg, ireg);
+            SHR(shift, ireg);
+            if (ireg == d.reg) {
+                MOV_REG_TO_REG(ireg, d.reg);
+                SHL(dl, d.reg);
+            } else {
+                int temp_reg = scratches.obtain();
+                MOV_REG_TO_REG(ireg, temp_reg);
+                SHL(dl, temp_reg);
+                OR_REG_TO_REG(temp_reg, d.reg);
+                scratches.recycle(temp_reg);
+            }
+        } else {
+            if (ireg == d.reg) {
+                MOV_REG_TO_REG(s.reg, d.reg);
+                SHR(shift, d.reg);
+            } else {
+                int temp_reg = scratches.obtain();
+                MOV_REG_TO_REG(s.reg, temp_reg);
+                SHR(shift, temp_reg);
+                OR_REG_TO_REG(temp_reg, d.reg);
+                scratches.recycle(temp_reg);
+            }
+        }
+    } else {
+        int shift = sh-dh;
+        if (shift>0) {
+            if (ireg == d.reg) {
+                MOV_REG_TO_REG(s.reg, d.reg);
+                SHR(shift, d.reg);
+            } else {
+                int temp_reg = scratches.obtain();
+                MOV_REG_TO_REG(s.reg, temp_reg);
+                SHR(shift, temp_reg);
+                OR_REG_TO_REG(temp_reg, d.reg);
+                scratches.recycle(temp_reg);
+            }
+        } else if (shift<0) {
+            if (ireg == d.reg) {
+                MOV_REG_TO_REG(s.reg, d.reg);
+                SHL(-shift, d.reg);
+            } else {
+                int temp_reg = scratches.obtain();
+                MOV_REG_TO_REG(s.reg, temp_reg);
+                SHL(-shift, temp_reg);
+                OR_REG_TO_REG(temp_reg, d.reg);
+                scratches.recycle(temp_reg);
+            }
+        } else {
+            if (ireg == d.reg) {
+                if (s.reg != d.reg) {
+                    MOV_REG_TO_REG(s.reg, d.reg);
+                }
+            } else {
+                OR_REG_TO_REG(s.reg, d.reg);
+            }
+        }
+    }
+}
+
+}; // namespace android
diff --git a/libpixelflinger/codeflinger/x86/texturing.cpp b/libpixelflinger/codeflinger/x86/texturing.cpp
index e69de29..43a56ef 100644
--- a/libpixelflinger/codeflinger/x86/texturing.cpp
+++ b/libpixelflinger/codeflinger/x86/texturing.cpp
@@ -0,0 +1,1800 @@
+/* libs/pixelflinger/codeflinger/x86/texturing.cpp
+**
+** Copyright 2006, The Android Open Source Project
+**
+** Licensed under the Apache License, Version 2.0 (the "License");
+** you may not use this file except in compliance with the License.
+** You may obtain a copy of the License at
+**
+**     http://www.apache.org/licenses/LICENSE-2.0
+**
+** Unless required by applicable law or agreed to in writing, software
+** distributed under the License is distributed on an "AS IS" BASIS,
+** WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+** See the License for the specific language governing permissions and
+** limitations under the License.
+*/
+
+#include <assert.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <sys/types.h>
+
+#include <cutils/log.h>
+
+#include "codeflinger/x86/GGLX86Assembler.h"
+
+
+namespace android {
+
+// ---------------------------------------------------------------------------
+
+// iterators are initialized like this:
+// (intToFixedCenter(x) * dx)>>16 + x0
+// ((x<<16 + 0x8000) * dx)>>16 + x0
+// ((x<<16)*dx + (0x8000*dx))>>16 + x0
+// ( (x*dx) + dx>>1 ) + x0
+// (x*dx) + (dx>>1 + x0)
+
+void GGLX86Assembler::init_iterated_color(fragment_parts_t& parts, const reg_t& x)
+{
+    context_t const* c = mBuilderContext.c;
+    const needs_t& needs = mBuilderContext.needs;
+    int temp_reg;
+
+    if (mSmooth) {
+        // NOTE: we could take this case in the mDithering + !mSmooth case,
+        // but this would use up to 4 more registers for the color components
+        // for only a little added quality.
+        // Currently, this causes the system to run out of registers in
+        // some case (see issue #719496)
+
+        comment("compute initial iterated color (smooth and/or dither case)");
+
+        parts.iterated_packed = 0;
+        parts.packed = 0;
+
+        // 0x1: color component
+        // 0x2: iterators
+        //parts.reload = 3;
+        const int optReload = mOptLevel >> 1;
+        if (optReload >= 3)         parts.reload = 0; // reload nothing
+        else if (optReload == 2)    parts.reload = 2; // reload iterators
+        else if (optReload == 1)    parts.reload = 1; // reload colors
+        else if (optReload <= 0)    parts.reload = 3; // reload both
+
+        if (!mSmooth) {
+            // we're not smoothing (just dithering), we never have to
+            // reload the iterators
+            parts.reload &= ~2;
+        }
+
+        Scratch scratches(registerFile());
+        const int t0 = (parts.reload & 1) ? scratches.obtain() : 0;
+        const int t1 = (parts.reload & 2) ? scratches.obtain() : 0;
+        for (int i=0 ; i<4 ; i++) {
+            if (!mInfo[i].iterated)
+                continue;
+            // this component exists in the destination and is not replaced
+            // by a texture unit.
+            const int c = (parts.reload & 1) ? t0 : obtainReg();
+            if (i==0) CONTEXT_LOAD(c, iterators.ydady);
+            if (i==1) CONTEXT_LOAD(c, iterators.ydrdy);
+            if (i==2) CONTEXT_LOAD(c, iterators.ydgdy);
+            if (i==3) CONTEXT_LOAD(c, iterators.ydbdy);
+            parts.argb[i].reg = c;
+
+            if (mInfo[i].smooth) {
+                parts.argb_dx[i].reg = (parts.reload & 2) ? t1 : obtainReg();
+                const int dvdx = parts.argb_dx[i].reg;
+                temp_reg = scratches.obtain();
+                CONTEXT_LOAD(dvdx, generated_vars.argb[i].dx);
+                MOV_REG_TO_REG(dvdx, temp_reg);
+                IMUL(x.reg, temp_reg);
+                ADD_REG_TO_REG(temp_reg, c);
+                scratches.recycle(temp_reg);
+
+                // adjust the color iterator to make sure it won't overflow
+                if (!mAA) {
+                    // this is not needed when we're using anti-aliasing
+                    // because we will (have to) clamp the components
+                    // anyway.
+                    int end = scratches.obtain();
+                    MOV_MEM_TO_REG(parts.count.offset_ebp, PhysicalReg_EBP, end);
+                    SHR(16, end);
+                    IMUL(end, dvdx);
+                    temp_reg = end;
+                    // c - (dvdx*end + c) = -(dvdx*end)
+                    MOV_REG_TO_REG(dvdx, temp_reg);
+                    NEG(temp_reg);
+                    ADD_REG_TO_REG(c, dvdx);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, temp_reg, c);
+                    /*
+                                        SUB_REG_TO_REG(dvdx, temp_reg);
+                                        switch(i) {
+                                        case 0:
+                                            JCC(Mnemonic_JNS, "1f_init_iterated_color");
+                                            SUB_REG_TO_REG(dvdx, c);
+                                            label("1f_init_iterated_color");
+                                            break;
+                                        case 1:
+                                            JCC(Mnemonic_JNS, "2f_init_iterated_color");
+                                            SUB_REG_TO_REG(dvdx, c);
+                                            label("2f_init_iterated_color");
+                                            break;
+                                        case 2:
+                                            JCC(Mnemonic_JNS, "3f_init_iterated_color");
+                                            SUB_REG_TO_REG(dvdx, c);
+                                            label("3f_init_iterated_color");
+                                            break;
+                                        case 3:
+                                            JCC(Mnemonic_JNS, "4f_init_iterated_color");
+                                            SUB_REG_TO_REG(dvdx, c);
+                                            label("4f_init_iterated_color");
+                                            break;
+                                        }
+                    */
+
+                    MOV_REG_TO_REG(c, temp_reg);
+                    SAR(31, temp_reg);
+                    NOT(temp_reg);
+                    AND_REG_TO_REG(temp_reg, c);
+                    scratches.recycle(end);
+                }
+                if(parts.reload & 2)
+                    scratches.recycle(dvdx);
+                else
+                    recycleReg(dvdx);
+            }
+            CONTEXT_STORE(c, generated_vars.argb[i].c);
+            if(parts.reload & 1)
+                scratches.recycle(parts.argb[i].reg);
+            else
+                recycleReg(parts.argb[i].reg);
+
+            parts.argb[i].reg = -1;
+            //if (parts.reload & 1) {
+            //    //MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+            //}
+        }
+    } else {
+        // We're not smoothed, so we can
+        // just use a packed version of the color and extract the
+        // components as needed (or not at all if we don't blend)
+
+        // figure out if we need the iterated color
+        int load = 0;
+        for (int i=0 ; i<4 ; i++) {
+            component_info_t& info = mInfo[i];
+            if ((info.inDest || info.needed) && !info.replaced)
+                load |= 1;
+        }
+
+        parts.iterated_packed = 1;
+        parts.packed = (!mTextureMachine.mask && !mBlending
+                        && !mFog && !mDithering);
+        parts.reload = 0;
+        if (load || parts.packed) {
+            if (mBlending || mDithering || mInfo[GGLFormat::ALPHA].needed) {
+                comment("load initial iterated color (8888 packed)");
+                parts.iterated.setTo(obtainReg(),
+                                     &(c->formats[GGL_PIXEL_FORMAT_RGBA_8888]));
+                CONTEXT_LOAD(parts.iterated.reg, packed8888);
+            } else {
+                comment("load initial iterated color (dest format packed)");
+
+                parts.iterated.setTo(obtainReg(), &mCbFormat);
+
+                // pre-mask the iterated color
+                const int bits = parts.iterated.size();
+                const uint32_t size = ((bits>=32) ? 0 : (1LU << bits)) - 1;
+                uint32_t mask = 0;
+                if (mMasking) {
+                    for (int i=0 ; i<4 ; i++) {
+                        const int component_mask = 1<<i;
+                        const int h = parts.iterated.format.c[i].h;
+                        const int l = parts.iterated.format.c[i].l;
+                        if (h && (!(mMasking & component_mask))) {
+                            mask |= ((1<<(h-l))-1) << l;
+                        }
+                    }
+                }
+
+                if (mMasking && ((mask & size)==0)) {
+                    // none of the components are present in the mask
+                } else {
+                    CONTEXT_LOAD(parts.iterated.reg, packed);
+                    if (mCbFormat.size == 1) {
+                        int imm = 0xFF;
+                        AND_IMM_TO_REG(imm, parts.iterated.reg);
+                    } else if (mCbFormat.size == 2) {
+                        SHR(16, parts.iterated.reg);
+                    }
+                }
+
+                // pre-mask the iterated color
+                if (mMasking) {
+                    //AND_IMM_TO_REG(mask, parts.iterated.reg);
+                    build_and_immediate(parts.iterated.reg, parts.iterated.reg,
+                                        mask, bits);
+                }
+            }
+            mCurSp = mCurSp - 4;
+            parts.iterated.offset_ebp = mCurSp;
+            MOV_REG_TO_MEM(parts.iterated.reg, parts.iterated.offset_ebp, EBP);
+            //PUSH(parts.iterated.reg);
+            recycleReg(parts.iterated.reg);
+            parts.iterated.reg=-1;
+        }
+    }
+}
+
+void GGLX86Assembler::build_iterated_color(
+    component_t& fragment,
+    fragment_parts_t& parts,
+    int component,
+    Scratch& regs)
+{
+
+    if (!mInfo[component].iterated)
+        return;
+
+    if (parts.iterated_packed) {
+        // iterated colors are packed, extract the one we need
+        parts.iterated.reg = regs.obtain();
+        MOV_MEM_TO_REG(parts.iterated.offset_ebp, EBP, parts.iterated.reg);
+        extract(fragment, parts.iterated, component);
+        regs.recycle(parts.iterated.reg);
+    } else {
+        fragment.h = GGL_COLOR_BITS;
+        fragment.l = GGL_COLOR_BITS - 8;
+        fragment.flags |= CLEAR_LO;
+        // iterated colors are held in their own register,
+        // (smooth and/or dithering case)
+        Scratch scratches(registerFile());
+        mBuilderContext.Rctx = scratches.obtain();
+        MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+        if (parts.reload==3) {
+            // this implies mSmooth
+            int dx = scratches.obtain();
+            CONTEXT_LOAD(fragment.reg, generated_vars.argb[component].c);
+            CONTEXT_LOAD(dx, generated_vars.argb[component].dx);
+            ADD_REG_TO_REG(fragment.reg, dx);
+            CONTEXT_STORE(dx, generated_vars.argb[component].c);
+            scratches.recycle(dx);
+        } else if (parts.reload & 1) {
+            //MOV_MEM_TO_REG(parts.argb[component].offset_ebp, EBP, fragment.reg);
+            CONTEXT_LOAD(fragment.reg, generated_vars.argb[component].c);
+        } else {
+            // we don't reload, so simply rename the register and mark as
+            // non CORRUPTIBLE so that the texture env or blending code
+            // won't modify this (renamed) register
+            //regs.recycle(fragment.reg);
+            //MOV_MEM_TO_REG(parts.argb[component].offset_ebp, EBP, fragment.reg);
+            // it will also be used in build_smooth_shade
+            CONTEXT_LOAD(fragment.reg, generated_vars.argb[component].c);
+            //fragment.reg = parts.argb[component].reg;
+            //fragment.flags &= ~CORRUPTIBLE;
+        }
+        scratches.recycle(mBuilderContext.Rctx);
+        if (mInfo[component].smooth && mAA) {
+            // when using smooth shading AND anti-aliasing, we need to clamp
+            // the iterators because there is always an extra pixel on the
+            // edges, which most of the time will cause an overflow
+            // (since technically its outside of the domain).
+            int temp = scratches.obtain();
+            MOV_REG_TO_REG(fragment.reg, temp);
+            SAR(31, temp);
+            NOT(temp);
+            OR_REG_TO_REG(temp, fragment.reg);
+            component_sat(fragment, temp);
+            scratches.recycle(temp);
+        }
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::decodeLogicOpNeeds(const needs_t& needs)
+{
+    // gather some informations about the components we need to process...
+    const int opcode = GGL_READ_NEEDS(LOGIC_OP, needs.n) | GGL_CLEAR;
+    switch(opcode) {
+    case GGL_COPY:
+        mLogicOp = 0;
+        break;
+    case GGL_CLEAR:
+    case GGL_SET:
+        mLogicOp = LOGIC_OP;
+        break;
+    case GGL_AND:
+    case GGL_AND_REVERSE:
+    case GGL_AND_INVERTED:
+    case GGL_XOR:
+    case GGL_OR:
+    case GGL_NOR:
+    case GGL_EQUIV:
+    case GGL_OR_REVERSE:
+    case GGL_OR_INVERTED:
+    case GGL_NAND:
+        mLogicOp = LOGIC_OP|LOGIC_OP_SRC|LOGIC_OP_DST;
+        break;
+    case GGL_NOOP:
+    case GGL_INVERT:
+        mLogicOp = LOGIC_OP|LOGIC_OP_DST;
+        break;
+    case GGL_COPY_INVERTED:
+        mLogicOp = LOGIC_OP|LOGIC_OP_SRC;
+        break;
+    };
+}
+
+void GGLX86Assembler::decodeTMUNeeds(const needs_t& needs, context_t const* c)
+{
+    uint8_t replaced=0;
+    mTextureMachine.mask = 0;
+    mTextureMachine.activeUnits = 0;
+    for (int i=GGL_TEXTURE_UNIT_COUNT-1 ; i>=0 ; i--) {
+        texture_unit_t& tmu = mTextureMachine.tmu[i];
+        if (replaced == 0xF) {
+            // all components are replaced, skip this TMU.
+            tmu.format_idx = 0;
+            tmu.mask = 0;
+            tmu.replaced = replaced;
+            continue;
+        }
+        tmu.format_idx = GGL_READ_NEEDS(T_FORMAT, needs.t[i]);
+        tmu.format = c->formats[tmu.format_idx];
+        tmu.bits = tmu.format.size*8;
+        tmu.swrap = GGL_READ_NEEDS(T_S_WRAP, needs.t[i]);
+        tmu.twrap = GGL_READ_NEEDS(T_T_WRAP, needs.t[i]);
+        tmu.env = ggl_needs_to_env(GGL_READ_NEEDS(T_ENV, needs.t[i]));
+        tmu.pot = GGL_READ_NEEDS(T_POT, needs.t[i]);
+        tmu.linear = GGL_READ_NEEDS(T_LINEAR, needs.t[i])
+                     && tmu.format.size!=3; // XXX: only 8, 16 and 32 modes for now
+
+        // 5551 linear filtering is not supported
+        if (tmu.format_idx == GGL_PIXEL_FORMAT_RGBA_5551)
+            tmu.linear = 0;
+
+        tmu.mask = 0;
+        tmu.replaced = replaced;
+
+        if (tmu.format_idx) {
+            mTextureMachine.activeUnits++;
+            if (tmu.format.c[0].h)    tmu.mask |= 0x1;
+            if (tmu.format.c[1].h)    tmu.mask |= 0x2;
+            if (tmu.format.c[2].h)    tmu.mask |= 0x4;
+            if (tmu.format.c[3].h)    tmu.mask |= 0x8;
+            if (tmu.env == GGL_REPLACE) {
+                replaced |= tmu.mask;
+            } else if (tmu.env == GGL_DECAL) {
+                if (!tmu.format.c[GGLFormat::ALPHA].h) {
+                    // if we don't have alpha, decal does nothing
+                    tmu.mask = 0;
+                } else {
+                    // decal always ignores At
+                    tmu.mask &= ~(1<<GGLFormat::ALPHA);
+                }
+            }
+        }
+        mTextureMachine.mask |= tmu.mask;
+        ////printf("%d: mask=%08lx, replaced=%08lx\n",
+        //    i, int(tmu.mask), int(tmu.replaced));
+    }
+    mTextureMachine.replaced = replaced;
+    mTextureMachine.directTexture = 0;
+    ////printf("replaced=%08lx\n", mTextureMachine.replaced);
+}
+
+
+void GGLX86Assembler::init_textures(
+    tex_coord_t* coords,
+    const reg_t& x, const reg_t& y)
+{
+    context_t const* c = mBuilderContext.c;
+    const needs_t& needs = mBuilderContext.needs;
+    reg_t temp_reg_t;
+    int Rx = x.reg;
+    int Ry = y.reg;
+
+    if (mTextureMachine.mask) {
+        comment("compute texture coordinates");
+    }
+
+    // init texture coordinates for each tmu
+    const int cb_format_idx = GGL_READ_NEEDS(CB_FORMAT, needs.n);
+    const bool multiTexture = mTextureMachine.activeUnits > 1;
+    for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT; i++) {
+        const texture_unit_t& tmu = mTextureMachine.tmu[i];
+        if (tmu.format_idx == 0)
+            continue;
+        if ((tmu.swrap == GGL_NEEDS_WRAP_11) &&
+                (tmu.twrap == GGL_NEEDS_WRAP_11))
+        {
+            Scratch scratches(registerFile());
+            // 1:1 texture
+            pointer_t& txPtr = coords[i].ptr;
+            txPtr.setTo(obtainReg(), tmu.bits);
+            CONTEXT_LOAD(txPtr.reg, state.texture[i].iterators.ydsdy);
+            SAR(16, txPtr.reg);
+            ADD_REG_TO_REG(txPtr.reg, Rx);
+            CONTEXT_LOAD(txPtr.reg, state.texture[i].iterators.ydtdy);
+            SAR(16, txPtr.reg);
+            ADD_REG_TO_REG(txPtr.reg, Ry);
+            // Rx and Ry are changed
+            // Rx = Rx + ti.iterators.ydsdy>>16
+            // Ry = Ry + ti.iterators.ydtdy>>16
+            // Rx = Ry * ti.stide + Rx
+
+            // merge base & offset
+            CONTEXT_LOAD(txPtr.reg, generated_vars.texture[i].stride);
+            IMUL(Ry, txPtr.reg);
+            ADD_REG_TO_REG(txPtr.reg, Rx);
+
+            CONTEXT_LOAD(txPtr.reg, generated_vars.texture[i].data);
+            temp_reg_t.setTo(Rx);
+            base_offset(txPtr, txPtr, temp_reg_t);
+            //PUSH(txPtr.reg);
+            mCurSp = mCurSp - 4;
+            txPtr.offset_ebp = mCurSp; //ebx, esi, edi, parts.count.reg, parts.cbPtr.reg, parts.z.reg
+            MOV_REG_TO_MEM(txPtr.reg, txPtr.offset_ebp, EBP);
+            recycleReg(txPtr.reg);
+            txPtr.reg=-1;
+        } else {
+            Scratch scratches(registerFile());
+            reg_t& s = coords[i].s;
+            reg_t& t = coords[i].t;
+            // s = (x * dsdx)>>16 + ydsdy
+            // s = (x * dsdx)>>16 + (y*dsdy)>>16 + s0
+            // t = (x * dtdx)>>16 + ydtdy
+            // t = (x * dtdx)>>16 + (y*dtdy)>>16 + t0
+            const int need_w = GGL_READ_NEEDS(W, needs.n);
+            MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+            if (need_w) {
+                s.setTo(obtainReg());
+                t.setTo(obtainReg());
+                CONTEXT_LOAD(s.reg, state.texture[i].iterators.ydsdy);
+                CONTEXT_LOAD(t.reg, state.texture[i].iterators.ydtdy);
+                CONTEXT_STORE(s.reg, generated_vars.texture[i].spill[0]);
+                CONTEXT_STORE(t.reg, generated_vars.texture[i].spill[1]);
+                recycleReg(s.reg);
+                recycleReg(t.reg);
+            } else {
+                int ydsdy = scratches.obtain();
+                int dsdx = scratches.obtain();
+                CONTEXT_LOAD(ydsdy, state.texture[i].iterators.ydsdy);
+                CONTEXT_LOAD(dsdx, generated_vars.texture[i].dsdx);
+                IMUL(Rx, dsdx);
+                ADD_REG_TO_REG(dsdx, ydsdy);
+                CONTEXT_STORE(ydsdy, generated_vars.texture[i].spill[0]);
+                scratches.recycle(ydsdy);
+                scratches.recycle(dsdx);
+
+                int ydtdy = scratches.obtain();
+                int dtdx = scratches.obtain();
+                CONTEXT_LOAD(ydtdy, state.texture[i].iterators.ydtdy);
+                CONTEXT_LOAD(dtdx, generated_vars.texture[i].dtdx);
+                IMUL(Rx, dtdx);
+                ADD_REG_TO_REG(dtdx, ydtdy);
+                CONTEXT_STORE(ydtdy, generated_vars.texture[i].spill[1]);
+                scratches.recycle(ydtdy);
+                scratches.recycle(dtdx);
+
+                // s.reg = Rx * ti.dsdx + ydsdy
+                // t.reg = Rx * ti.dtdx + ydtdy
+            }
+        }
+
+        // direct texture?
+        if (!multiTexture && !mBlending && !mDithering && !mFog &&
+                cb_format_idx == tmu.format_idx && !tmu.linear &&
+                mTextureMachine.replaced == tmu.mask)
+        {
+            mTextureMachine.directTexture = i + 1;
+        }
+    }
+}
+
+void GGLX86Assembler::build_textures(  fragment_parts_t& parts,
+                                       Scratch& regs)
+{
+    context_t const* c = mBuilderContext.c;
+    const needs_t& needs = mBuilderContext.needs;
+    reg_t temp_reg_t;
+    //int Rctx = mBuilderContext.Rctx;
+
+
+    const bool multiTexture = mTextureMachine.activeUnits > 1;
+    for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT; i++) {
+        const texture_unit_t& tmu = mTextureMachine.tmu[i];
+        if (tmu.format_idx == 0)
+            continue;
+
+        pointer_t& txPtr = parts.coords[i].ptr;
+        pixel_t& texel = parts.texel[i];
+
+        // repeat...
+        if ((tmu.swrap == GGL_NEEDS_WRAP_11) &&
+                (tmu.twrap == GGL_NEEDS_WRAP_11))
+        {   // 1:1 textures
+            comment("fetch texel");
+            texel.setTo(regs.obtain(), &tmu.format);
+            txPtr.reg = regs.obtain();
+            MOV_MEM_TO_REG(txPtr.offset_ebp, EBP, txPtr.reg);
+            mCurSp = mCurSp - 4;
+            texel.offset_ebp = mCurSp;
+            load(txPtr, texel, WRITE_BACK);
+            MOV_REG_TO_MEM(texel.reg, texel.offset_ebp, EBP);
+            regs.recycle(texel.reg);
+            regs.recycle(txPtr.reg);
+        } else {
+            Scratch scratches(registerFile());
+            reg_t& s = parts.coords[i].s;
+            reg_t& t = parts.coords[i].t;
+            comment("reload s/t (multitexture or linear filtering)");
+            s.reg = scratches.obtain();
+            t.reg = scratches.obtain();
+            mBuilderContext.Rctx = scratches.obtain();
+            MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+            CONTEXT_LOAD(s.reg, generated_vars.texture[i].spill[0]);
+            CONTEXT_LOAD(t.reg, generated_vars.texture[i].spill[1]);
+
+            comment("compute repeat/clamp");
+            int width   = scratches.obtain();
+            int height  = scratches.obtain();
+            int U = 0;
+            int V = 0;
+            // U and V will be stored onto the stack due to the limited register
+            reg_t reg_U, reg_V;
+
+            CONTEXT_LOAD(width,  generated_vars.texture[i].width);
+            CONTEXT_LOAD(height, generated_vars.texture[i].height);
+            scratches.recycle(mBuilderContext.Rctx);
+
+            int FRAC_BITS = 0;
+            if (tmu.linear) {
+                // linear interpolation
+                if (tmu.format.size == 1) {
+                    // for 8-bits textures, we can afford
+                    // 7 bits of fractional precision at no
+                    // additional cost (we can't do 8 bits
+                    // because filter8 uses signed 16 bits muls)
+                    FRAC_BITS = 7;
+                } else if (tmu.format.size == 2) {
+                    // filter16() is internally limited to 4 bits, so:
+                    // FRAC_BITS=2 generates less instructions,
+                    // FRAC_BITS=3,4,5 creates unpleasant artifacts,
+                    // FRAC_BITS=6+ looks good
+                    FRAC_BITS = 6;
+                } else if (tmu.format.size == 4) {
+                    // filter32() is internally limited to 8 bits, so:
+                    // FRAC_BITS=4 looks good
+                    // FRAC_BITS=5+ looks better, but generates 3 extra ipp
+                    FRAC_BITS = 6;
+                } else {
+                    // for all other cases we use 4 bits.
+                    FRAC_BITS = 4;
+                }
+            }
+            int u       = scratches.obtain();
+            // s.reg and t.reg are recycled in wrapping
+            wrapping(u, s.reg, width,  tmu.swrap, FRAC_BITS, scratches);
+            int v       = scratches.obtain();
+            wrapping(v, t.reg, height, tmu.twrap, FRAC_BITS, scratches);
+
+
+            if (tmu.linear) {
+
+                //mBuilderContext.Rctx = scratches.obtain();
+                //MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+                //CONTEXT_LOAD(width,  generated_vars.texture[i].width);
+                //CONTEXT_LOAD(height, generated_vars.texture[i].height);
+                //scratches.recycle(mBuilderContext.Rctx);
+
+                comment("compute linear filtering offsets");
+                // pixel size scale
+                const int shift = 31 - gglClz(tmu.format.size);
+                U = scratches.obtain();
+                V = scratches.obtain();
+
+
+                // sample the texel center
+                SUB_IMM_TO_REG(1<<(FRAC_BITS-1), u);
+                SUB_IMM_TO_REG(1<<(FRAC_BITS-1), v);
+
+                // get the fractionnal part of U,V
+                MOV_REG_TO_REG(u, U);
+                AND_IMM_TO_REG((1<<FRAC_BITS)-1, U);
+                MOV_REG_TO_REG(v, V);
+                AND_IMM_TO_REG((1<<FRAC_BITS)-1, V);
+
+                // below we will pop U and V in the filter function
+                mCurSp = mCurSp - 4;
+                MOV_REG_TO_MEM(U, mCurSp, EBP);
+                reg_U.offset_ebp = mCurSp;
+                mCurSp = mCurSp - 4;
+                MOV_REG_TO_MEM(V, mCurSp, EBP);
+                reg_V.offset_ebp = mCurSp;
+
+                scratches.recycle(U);
+                scratches.recycle(V);
+
+                // compute width-1 and height-1
+                SUB_IMM_TO_REG(1, width);
+                SUB_IMM_TO_REG(1, height);
+
+                // the registers are used up
+                int temp1 = scratches.obtain();
+                int temp2 = scratches.obtain();
+                // get the integer part of U,V and clamp/wrap
+                // and compute offset to the next texel
+                if (tmu.swrap == GGL_NEEDS_WRAP_REPEAT) {
+                    // u has already been REPEATed
+                    SAR(FRAC_BITS, u);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, width, u);
+                    MOV_IMM_TO_REG(1<<shift, temp1);
+                    MOV_REG_TO_REG(width, temp2);
+                    // SHL may pollute the CF flag
+                    SHL(shift, temp2);
+                    mCurSp = mCurSp - 4;
+                    int width_offset_ebp = mCurSp;
+                    // width will be changed after the first comparison
+                    MOV_REG_TO_MEM(width, width_offset_ebp, EBP);
+                    CMP_REG_TO_REG(width, u);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVL, temp1, width);
+                    if (shift) {
+                        CMOV_REG_TO_REG(Mnemonic_CMOVGE, temp2, width);
+                    }
+                    MOV_REG_TO_REG(width, temp1);
+                    NEG(temp1);
+                    // width is actually changed
+                    CMP_MEM_TO_REG(EBP, width_offset_ebp, u);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVGE, temp1, width);
+                } else {
+                    // u has not been CLAMPed yet
+                    // algorithm:
+                    // if ((u>>4) >= width)
+                    //      u = width<<4
+                    //      width = 0
+                    // else
+                    //      width = 1<<shift
+                    // u = u>>4; // get integer part
+                    // if (u<0)
+                    //      u = 0
+                    //      width = 0
+                    // generated_vars.rt = width
+
+                    MOV_REG_TO_REG(width, temp2);
+                    SHL(FRAC_BITS, temp2);
+                    MOV_REG_TO_REG(u, temp1);
+                    SAR(FRAC_BITS, temp1);
+                    CMP_REG_TO_REG(temp1, width);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVLE, temp2, u);
+                    // mov doesn't affect the flags
+                    MOV_IMM_TO_REG(0, temp2);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVLE, temp2, width);
+                    MOV_IMM_TO_REG(1 << shift, temp2);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVG, temp2, width);
+
+                    MOV_IMM_TO_REG(0, temp2);
+                    SAR(FRAC_BITS, u);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, temp2, u);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, temp2, width);
+                }
+                scratches.recycle(temp1);
+                scratches.recycle(temp2);
+                mBuilderContext.Rctx = scratches.obtain();
+                MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+                CONTEXT_STORE(width, generated_vars.rt);
+
+                const int stride = width;
+                CONTEXT_LOAD(stride, generated_vars.texture[i].stride);
+                scratches.recycle(mBuilderContext.Rctx);
+
+                temp1 = scratches.obtain();
+                temp2 = scratches.obtain();
+
+                int height_offset_ebp;
+                if (tmu.twrap == GGL_NEEDS_WRAP_REPEAT) {
+                    // v has already been REPEATed
+                    SAR(FRAC_BITS, v);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, height, v);
+                    MOV_IMM_TO_REG(1<<shift, temp1);
+                    MOV_REG_TO_REG(height, temp2);
+                    SHL(shift, temp2);
+                    mCurSp = mCurSp - 4;
+                    height_offset_ebp = mCurSp;
+                    // height will be changed after the first comparison
+                    MOV_REG_TO_MEM(height, height_offset_ebp, EBP);
+                    CMP_REG_TO_REG(height, v);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVL, temp1, height);
+                    if (shift) {
+                        CMOV_REG_TO_REG(Mnemonic_CMOVGE, temp2, height);
+                    }
+                    MOV_REG_TO_REG(height, temp1);
+                    NEG(temp1);
+                    // height is actually changed
+                    CMP_MEM_TO_REG(EBP, height_offset_ebp, v);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVGE, temp1, height);
+                    IMUL(stride, height);
+                } else {
+                    // u has not been CLAMPed yet
+                    MOV_REG_TO_REG(height, temp2);
+                    SHL(FRAC_BITS, temp2);
+                    MOV_REG_TO_REG(v, temp1);
+                    SAR(FRAC_BITS, temp1);
+
+                    mCurSp = mCurSp - 4;
+                    height_offset_ebp = mCurSp;
+                    // height may be changed after the first comparison
+                    MOV_REG_TO_MEM(height, height_offset_ebp, EBP);
+
+                    CMP_REG_TO_REG(temp1, height);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVLE, temp2, v);
+                    MOV_IMM_TO_REG(0, temp2);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVLE, temp2, height);
+
+                    if (shift) {
+                        // stride = width. It's not used
+                        // shift may pollute the flags
+                        SHL(shift, stride);
+                        // height may be changed to 0
+                        CMP_REG_TO_MEM(temp1, height_offset_ebp, EBP);
+                        CMOV_REG_TO_REG(Mnemonic_CMOVG, stride, height);
+                    } else {
+                        CMOV_REG_TO_REG(Mnemonic_CMOVG, stride, height);
+                    }
+                    MOV_IMM_TO_REG(0, temp2);
+                    SAR(FRAC_BITS, v);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, temp2, v);
+                    CMOV_REG_TO_REG(Mnemonic_CMOVS, temp2, height);
+                }
+                scratches.recycle(temp1);
+                scratches.recycle(temp2);
+                mBuilderContext.Rctx = scratches.obtain();
+                MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+                CONTEXT_STORE(height, generated_vars.lb);
+                scratches.recycle(mBuilderContext.Rctx);
+            }
+
+            scratches.recycle(width);
+            scratches.recycle(height);
+
+            // iterate texture coordinates...
+            comment("iterate s,t");
+            int dsdx = scratches.obtain();
+            s.reg = scratches.obtain();
+            mBuilderContext.Rctx = scratches.obtain();
+            MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+            CONTEXT_LOAD(dsdx, generated_vars.texture[i].dsdx);
+            CONTEXT_LOAD(s.reg, generated_vars.texture[i].spill[0]);
+            ADD_REG_TO_REG(dsdx, s.reg);
+            CONTEXT_STORE(s.reg, generated_vars.texture[i].spill[0]);
+            scratches.recycle(s.reg);
+            scratches.recycle(dsdx);
+            int dtdx = scratches.obtain();
+            t.reg = scratches.obtain();
+            CONTEXT_LOAD(dtdx, generated_vars.texture[i].dtdx);
+            CONTEXT_LOAD(t.reg, generated_vars.texture[i].spill[1]);
+            ADD_REG_TO_REG(dtdx, t.reg);
+            CONTEXT_STORE(t.reg, generated_vars.texture[i].spill[1]);
+            scratches.recycle(dtdx);
+            scratches.recycle(t.reg);
+
+            // merge base & offset...
+            comment("merge base & offset");
+            texel.setTo(scratches.obtain(), &tmu.format);
+            //txPtr.setTo(texel.reg, tmu.bits);
+            txPtr.setTo(scratches.obtain(), tmu.bits);
+            int stride = scratches.obtain();
+            CONTEXT_LOAD(stride,    generated_vars.texture[i].stride);
+            CONTEXT_LOAD(txPtr.reg, generated_vars.texture[i].data);
+            scratches.recycle(mBuilderContext.Rctx);
+            MOVSX_REG_TO_REG(OpndSize_16, v, v);
+            MOVSX_REG_TO_REG(OpndSize_16, stride, stride);
+            IMUL(v, stride);
+            ADD_REG_TO_REG(stride, u);// u+v*stride
+            temp_reg_t.setTo(u);
+            base_offset(txPtr, txPtr, temp_reg_t);
+
+            // recycle registers we don't need anymore
+            scratches.recycle(u);
+            scratches.recycle(v);
+            scratches.recycle(stride);
+
+            mCurSp = mCurSp - 4;
+            texel.offset_ebp = mCurSp;
+            // load texel
+            if (!tmu.linear) {
+                comment("fetch texel in building texture");
+                load(txPtr, texel, 0);
+                MOV_REG_TO_MEM(texel.reg, texel.offset_ebp, EBP);
+                scratches.recycle(texel.reg);
+                scratches.recycle(txPtr.reg);
+            } else {
+                comment("fetch texel, bilinear");
+                // the registes are not enough. We spill texel and previous U and V
+                // texel.reg is recycled in the following functions since there are more than one code path
+                switch (tmu.format.size) {
+                case 1:
+                    filter8(parts, texel, tmu, reg_U, reg_V, txPtr, FRAC_BITS, scratches);
+                    break;
+                case 2:
+                    filter16(parts, texel, tmu, reg_U, reg_V, txPtr, FRAC_BITS, scratches);
+                    break;
+                case 3:
+                    filter24(parts, texel, tmu, U, V, txPtr, FRAC_BITS);
+                    break;
+                case 4:
+                    filter32(parts, texel, tmu, reg_U, reg_V, txPtr, FRAC_BITS, scratches);
+                    break;
+                }
+            }
+        }
+    }
+}
+
+void GGLX86Assembler::build_iterate_texture_coordinates(
+    const fragment_parts_t& parts)
+{
+    const bool multiTexture = mTextureMachine.activeUnits > 1;
+    for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT; i++) {
+        const texture_unit_t& tmu = mTextureMachine.tmu[i];
+        if (tmu.format_idx == 0)
+            continue;
+
+        if ((tmu.swrap == GGL_NEEDS_WRAP_11) &&
+                (tmu.twrap == GGL_NEEDS_WRAP_11))
+        {   // 1:1 textures
+            const pointer_t& txPtr = parts.coords[i].ptr;
+            ADD_IMM_TO_MEM(txPtr.size>>3, txPtr.offset_ebp, EBP);
+        } else {
+            Scratch scratches(registerFile());
+            int s = parts.coords[i].s.reg;
+            int t = parts.coords[i].t.reg;
+            mBuilderContext.Rctx = scratches.obtain();
+            MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+            s = scratches.obtain();
+            int dsdx = scratches.obtain();
+            CONTEXT_LOAD(s, generated_vars.texture[i].spill[0]);
+            CONTEXT_LOAD(dsdx, generated_vars.texture[i].dsdx);
+            ADD_REG_TO_REG(dsdx, s);
+            CONTEXT_STORE(s, generated_vars.texture[i].spill[0]);
+            scratches.recycle(s);
+            scratches.recycle(dsdx);
+            int dtdx = scratches.obtain();
+            t = scratches.obtain();
+            CONTEXT_LOAD(t, generated_vars.texture[i].spill[1]);
+            CONTEXT_LOAD(dtdx, generated_vars.texture[i].dtdx);
+            ADD_REG_TO_REG(dtdx, t);
+            CONTEXT_STORE(t, generated_vars.texture[i].spill[1]);
+            scratches.recycle(t);
+            scratches.recycle(dtdx);
+        }
+    }
+}
+
+void GGLX86Assembler::filter8(
+    const fragment_parts_t& parts,
+    pixel_t& texel, const texture_unit_t& tmu,
+    reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+    int FRAC_BITS, Scratch& scratches)
+{
+    if (tmu.format.components != GGL_ALPHA &&
+            tmu.format.components != GGL_LUMINANCE)
+    {
+        // this is a packed format, and we don't support
+        // linear filtering (it's probably RGB 332)
+        // Should not happen with OpenGL|ES
+        MOVZX_MEM_TO_REG(OpndSize_8, txPtr.reg, 0, texel.reg);
+        MOV_REG_TO_MEM(texel.reg, texel.offset_ebp, EBP);
+        scratches.recycle(texel.reg);
+        scratches.recycle(txPtr.reg);
+        return;
+    }
+
+    // ------------------------
+
+    //int d    = scratches.obtain();
+    //int u    = scratches.obtain();
+    //int k    = scratches.obtain();
+
+    scratches.recycle(texel.reg);
+    int rt   = scratches.obtain();
+    int lb   = scratches.obtain();
+
+    // RB -> U * V
+
+    mBuilderContext.Rctx = scratches.obtain();
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(rt, generated_vars.rt);
+    CONTEXT_LOAD(lb, generated_vars.lb);
+    scratches.recycle(mBuilderContext.Rctx);
+    int pixel= scratches.obtain();
+
+    int offset = pixel;
+
+    MOV_REG_TO_REG(rt, offset);
+    ADD_REG_TO_REG(lb, offset);
+
+    int temp_reg1 = scratches.obtain();
+    int temp_reg2 = scratches.obtain();
+    // it seems that the address mode with base and scale reg cannot be encoded correctly
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, temp_reg1, OpndSize_8);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOVZX_MEM_TO_REG(OpndSize_8, offset, 0, temp_reg1);
+    // pixel is only 8-bits
+    MOV_REG_TO_REG(temp_reg1, pixel);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_U.offset_ebp, temp_reg1);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg2);
+    IMUL(temp_reg2, temp_reg1);
+    MOVSX_REG_TO_REG(OpndSize_16, pixel, pixel);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg1, temp_reg2);
+    IMUL(temp_reg2, pixel);
+    NEG(temp_reg1);
+    ADD_IMM_TO_REG(1<<(FRAC_BITS*2), temp_reg1);
+    mCurSp = mCurSp - 4;
+    int d_offset_ebp = mCurSp;
+    MOV_REG_TO_MEM(pixel, d_offset_ebp, EBP);
+    mCurSp = mCurSp - 4;
+    int k_offset_ebp = mCurSp;
+    MOV_REG_TO_MEM(temp_reg1, k_offset_ebp, EBP);
+
+
+    // LB -> (1-U) * V
+    MOV_MEM_TO_REG(reg_U.offset_ebp, EBP, temp_reg2);
+    NEG(temp_reg2);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg2);
+    MOV_REG_TO_MEM(temp_reg2, reg_U.offset_ebp, EBP);
+
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, lb, 1, pixel, OpndSize_8);
+    ADD_REG_TO_REG(txPtr.reg, lb);
+    MOVZX_MEM_TO_REG(OpndSize_8, lb, 0, pixel);
+
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg2, temp_reg2);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg1);
+    IMUL(temp_reg1, temp_reg2);
+    MOVSX_REG_TO_REG(OpndSize_16, pixel, pixel);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg2, temp_reg1);
+    IMUL(pixel, temp_reg1);
+    ADD_REG_TO_MEM(temp_reg1, EBP, d_offset_ebp);
+    SUB_REG_TO_MEM(temp_reg2, EBP, k_offset_ebp);
+
+
+    // LT -> (1-U)*(1-V)
+    MOV_MEM_TO_REG(reg_V.offset_ebp, EBP, temp_reg2);
+    NEG(temp_reg2);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg2);
+    MOV_REG_TO_MEM(temp_reg2, reg_V.offset_ebp, EBP);
+
+    MOVZX_MEM_TO_REG(OpndSize_8, txPtr.reg, 0, pixel);
+
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_U.offset_ebp, temp_reg1);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg2, temp_reg2);
+    IMUL(temp_reg1, temp_reg2);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg2, temp_reg1);
+    MOVSX_REG_TO_REG(OpndSize_16, pixel, pixel);
+    IMUL(pixel, temp_reg1);
+    ADD_REG_TO_MEM(temp_reg1, EBP, d_offset_ebp);
+
+    // RT -> U*(1-V)
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, rt, 1, pixel, OpndSize_8);
+    ADD_REG_TO_REG(txPtr.reg, rt);
+    MOVZX_MEM_TO_REG(OpndSize_8, rt, 0, pixel);
+
+    int k = rt;
+    MOV_MEM_TO_REG(k_offset_ebp, EBP, k);
+    SUB_REG_TO_REG(temp_reg2, k);
+    MOVSX_REG_TO_REG(OpndSize_16, pixel, pixel);
+    MOVSX_REG_TO_REG(OpndSize_16, k, k);
+    IMUL(pixel, k);
+    ADD_MEM_TO_REG(EBP, d_offset_ebp, k);
+    MOV_REG_TO_MEM(k, texel.offset_ebp, EBP);
+    scratches.recycle(rt);
+    scratches.recycle(lb);
+    scratches.recycle(pixel);
+    scratches.recycle(txPtr.reg);
+    scratches.recycle(temp_reg1);
+    scratches.recycle(temp_reg2);
+    for (int i=0 ; i<4 ; i++) {
+        if (!texel.format.c[i].h) continue;
+        texel.format.c[i].h = FRAC_BITS*2+8;
+        texel.format.c[i].l = FRAC_BITS*2; // keeping 8 bits in enough
+    }
+    texel.format.size = 4;
+    texel.format.bitsPerPixel = 32;
+    texel.flags |= CLEAR_LO;
+}
+
+void GGLX86Assembler::filter16(
+    const fragment_parts_t& parts,
+    pixel_t& texel, const texture_unit_t& tmu,
+    reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+    int FRAC_BITS, Scratch& scratches)
+{
+    // compute the mask
+    // XXX: it would be nice if the mask below could be computed
+    // automatically.
+    uint32_t mask = 0;
+    int shift = 0;
+    int prec = 0;
+    switch (tmu.format_idx) {
+    case GGL_PIXEL_FORMAT_RGB_565:
+        // source: 00000ggg.ggg00000 | rrrrr000.000bbbbb
+        // result: gggggggg.gggrrrrr | rrrrr0bb.bbbbbbbb
+        mask = 0x07E0F81F;
+        shift = 16;
+        prec = 5;
+        break;
+    case GGL_PIXEL_FORMAT_RGBA_4444:
+        // 0000,1111,0000,1111 | 0000,1111,0000,1111
+        mask = 0x0F0F0F0F;
+        shift = 12;
+        prec = 4;
+        break;
+    case GGL_PIXEL_FORMAT_LA_88:
+        // 0000,0000,1111,1111 | 0000,0000,1111,1111
+        // AALL -> 00AA | 00LL
+        mask = 0x00FF00FF;
+        shift = 8;
+        prec = 8;
+        break;
+    default:
+        // unsupported format, do something sensical...
+        ALOGE("Unsupported 16-bits texture format (%d)", tmu.format_idx);
+        MOVZX_MEM_TO_REG(OpndSize_16, txPtr.reg, 0, texel.reg);
+        MOV_REG_TO_MEM(texel.reg, texel.offset_ebp, EBP);
+        scratches.recycle(texel.reg);
+        scratches.recycle(txPtr.reg);
+        return;
+    }
+
+    const int adjust = FRAC_BITS*2 - prec;
+    const int round  = 0;
+
+    // update the texel format
+    texel.format.size = 4;
+    texel.format.bitsPerPixel = 32;
+    texel.flags |= CLEAR_HI|CLEAR_LO;
+    for (int i=0 ; i<4 ; i++) {
+        if (!texel.format.c[i].h) continue;
+        const uint32_t offset = (mask & tmu.format.mask(i)) ? 0 : shift;
+        texel.format.c[i].h = tmu.format.c[i].h + offset + prec;
+        texel.format.c[i].l = texel.format.c[i].h - (tmu.format.bits(i) + prec);
+    }
+
+    // ------------------------
+
+    scratches.recycle(texel.reg);
+
+    int pixel= scratches.obtain();
+    int u    = scratches.obtain();
+    int temp_reg1 = scratches.obtain();
+
+    // RB -> U * V
+    //printf("RB ->  U * V \n");
+    int offset = pixel;
+    mBuilderContext.Rctx = scratches.obtain();
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(offset, generated_vars.rt);
+    CONTEXT_LOAD(u, generated_vars.lb);
+    ADD_REG_TO_REG(u, offset);
+
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, temp_reg1, OpndSize_16);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOVZX_MEM_TO_REG(OpndSize_16, offset, 0, temp_reg1);
+
+    MOV_REG_TO_REG(temp_reg1, pixel);
+
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_U.offset_ebp, u);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg1);
+    IMUL(temp_reg1, u);
+    MOV_REG_TO_REG(pixel, temp_reg1);
+    SHL(shift, temp_reg1);
+    OR_REG_TO_REG(temp_reg1, pixel);
+    build_and_immediate(pixel, pixel, mask, 32);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), u);
+        SHR(adjust, u);
+    }
+    int d = scratches.obtain();
+    MOV_REG_TO_REG(u, d);
+    IMUL(pixel, d);
+    NEG(u);
+    ADD_IMM_TO_REG(1<<prec, u);
+
+
+    // LB -> (1-U) * V
+    //printf("LB -> (1- U) * V \n");
+    MOV_MEM_TO_REG(reg_U.offset_ebp, EBP, temp_reg1);
+    NEG(temp_reg1);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg1);
+    MOV_REG_TO_MEM(temp_reg1, reg_U.offset_ebp, EBP);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg1, temp_reg1);
+
+    CONTEXT_LOAD(offset, generated_vars.lb);
+    scratches.recycle(mBuilderContext.Rctx);
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, pixel, OpndSize_16);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOVZX_MEM_TO_REG(OpndSize_16, offset, 0, pixel);
+
+    int temp_reg2 = scratches.obtain();
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg2);
+    IMUL(temp_reg1, temp_reg2);
+    MOV_REG_TO_REG(pixel, temp_reg1);
+    SHL(shift, temp_reg1);
+    OR_REG_TO_REG(temp_reg1, pixel);
+    build_and_immediate(pixel, pixel, mask, 32);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), temp_reg2);
+        SHR(adjust, temp_reg2);
+    }
+    IMUL(temp_reg2, pixel);
+    ADD_REG_TO_REG(pixel, d);
+    SUB_REG_TO_REG(temp_reg2, u);
+
+
+    // LT -> (1-U)*(1-V)
+    //printf("LT -> (1- U)*(1-V) \n");
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg2);
+    NEG(temp_reg2);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg2);
+    MOV_REG_TO_MEM(temp_reg2, reg_V.offset_ebp, EBP);
+    MOVZX_MEM_TO_REG(OpndSize_16, txPtr.reg, 0, pixel);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_U.offset_ebp, temp_reg1);
+    IMUL(temp_reg1, temp_reg2);
+    MOV_REG_TO_REG(pixel, temp_reg1);
+    SHL(shift, temp_reg1);
+    OR_REG_TO_REG(temp_reg1, pixel);
+    build_and_immediate(pixel, pixel, mask, 32);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), temp_reg2);
+        SHR(adjust, temp_reg2);
+    }
+    IMUL(temp_reg2, pixel);
+    ADD_REG_TO_REG(pixel, d);
+
+
+    // RT -> U*(1-V)
+    //printf("RT -> U*(1-V) \n");
+    SUB_REG_TO_REG(temp_reg2, u);
+    mBuilderContext.Rctx = temp_reg2;
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(temp_reg1, generated_vars.rt);
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, temp_reg1, 1, pixel, OpndSize_16);
+    ADD_REG_TO_REG(txPtr.reg, temp_reg1);
+    MOVZX_MEM_TO_REG(OpndSize_16, temp_reg1, 0, pixel);
+
+    MOV_REG_TO_REG(pixel, temp_reg1);
+    SHL(shift, temp_reg1);
+    OR_REG_TO_REG(temp_reg1, pixel);
+    build_and_immediate(pixel, pixel, mask, 32);
+    IMUL(u, pixel);
+    ADD_REG_TO_REG(pixel, d);
+    MOV_REG_TO_MEM(d, texel.offset_ebp, EBP);
+    scratches.recycle(d);
+    scratches.recycle(pixel);
+    scratches.recycle(u);
+    scratches.recycle(txPtr.reg);
+    scratches.recycle(temp_reg1);
+    scratches.recycle(temp_reg2);
+}
+
+void GGLX86Assembler::filter24(
+    const fragment_parts_t& parts,
+    pixel_t& texel, const texture_unit_t& tmu,
+    int U, int V, pointer_t& txPtr,
+    int FRAC_BITS)
+{
+    // not supported yet (currently disabled)
+    load(txPtr, texel, 0);
+}
+
+void GGLX86Assembler::filter32(
+    const fragment_parts_t& parts,
+    pixel_t& texel, const texture_unit_t& tmu,
+    reg_t reg_U, reg_t reg_V, pointer_t& txPtr,
+    int FRAC_BITS, Scratch& scratches)
+{
+    const int adjust = FRAC_BITS*2 - 8;
+    const int round  = 0;
+
+    // ------------------------
+    scratches.recycle(texel.reg);
+    int mask = scratches.obtain();
+    int pixel= scratches.obtain();
+    int u    = scratches.obtain();
+
+    //int dh   = scratches.obtain();
+    //int k    = scratches.obtain();
+    //int temp = scratches.obtain();
+    //int dl   = scratches.obtain();
+
+    MOV_IMM_TO_REG(0xFF, mask);
+    OR_IMM_TO_REG(0xFF0000, mask);
+
+    // RB -> U * V
+    int offset = pixel;
+    mBuilderContext.Rctx = scratches.obtain();
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(offset, generated_vars.rt);
+    CONTEXT_LOAD(u, generated_vars.lb);
+    ADD_REG_TO_REG(u, offset);
+    scratches.recycle(mBuilderContext.Rctx);
+
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, u);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOV_MEM_TO_REG(0, offset, u);
+
+    MOV_REG_TO_REG(u, pixel);
+
+    int temp_reg1  = scratches.obtain();
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_U.offset_ebp, temp_reg1);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, u);
+    IMUL(temp_reg1, u);
+    MOV_REG_TO_REG(mask, temp_reg1);
+    AND_REG_TO_REG(pixel, temp_reg1);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), u);
+        SHR(adjust, u);
+    }
+    int temp_reg2  = scratches.obtain();
+    MOV_REG_TO_REG(temp_reg1, temp_reg2);
+    IMUL(u, temp_reg2);
+    SHR(8, pixel);
+    AND_REG_TO_REG(mask, pixel);
+    IMUL(u, pixel);
+    NEG(u);
+    ADD_IMM_TO_REG(0x100, u);
+    mCurSp = mCurSp - 4;
+    int dh_offset_ebp = mCurSp;
+    MOV_REG_TO_MEM(temp_reg2, dh_offset_ebp, EBP);
+    mCurSp = mCurSp - 4;
+    int dl_offset_ebp = mCurSp;
+    MOV_REG_TO_MEM(pixel, dl_offset_ebp, EBP);
+
+    // LB -> (1-U) * V
+    mBuilderContext.Rctx = temp_reg2;
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(offset, generated_vars.lb);
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, temp_reg2);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOV_MEM_TO_REG(0, offset, temp_reg2);
+
+    MOV_REG_TO_REG(temp_reg2, pixel);
+    MOV_MEM_TO_REG(reg_U.offset_ebp, EBP, temp_reg1);
+    NEG(temp_reg1);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg1);
+    MOV_REG_TO_MEM(temp_reg1, reg_U.offset_ebp, EBP);
+    MOVSX_REG_TO_REG(OpndSize_16, temp_reg1, temp_reg1);
+    MOVSX_MEM_TO_REG(OpndSize_16, EBP, reg_V.offset_ebp, temp_reg2);
+    IMUL(temp_reg2, temp_reg1);
+    MOV_REG_TO_REG(mask, temp_reg2);
+    AND_REG_TO_REG(pixel, temp_reg2);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), temp_reg1);
+        SHR(adjust, temp_reg1);
+    }
+    // if we use push and pop txPtr.reg later, It will cause the bad locality, since the esp is already been subtracted before the loop.
+    // we will spill txPtr.reg due to the limited register
+    mCurSp = mCurSp - 4;
+    int txPtr_offset_ebp = mCurSp;
+    MOV_REG_TO_MEM(txPtr.reg, txPtr_offset_ebp, EBP);
+    //PUSH(txPtr.reg);
+
+    int temp_reg3 = txPtr.reg;
+    MOV_REG_TO_REG(temp_reg2, temp_reg3);
+    IMUL(temp_reg1, temp_reg3);
+    ADD_REG_TO_MEM(temp_reg3, EBP, dh_offset_ebp);
+    SHR(8, pixel);
+    AND_REG_TO_REG(mask, pixel);
+    IMUL(temp_reg1, pixel);
+    ADD_REG_TO_MEM(pixel, EBP, dl_offset_ebp);
+    SUB_REG_TO_REG(temp_reg1, u);
+
+
+    // LT -> (1-U)*(1-V)
+    MOV_MEM_TO_REG(reg_V.offset_ebp, EBP, temp_reg1);
+    NEG(temp_reg1);
+    ADD_IMM_TO_REG(1<<FRAC_BITS, temp_reg1);
+    MOV_REG_TO_MEM(temp_reg1, reg_V.offset_ebp, EBP);
+    MOV_MEM_TO_REG(reg_U.offset_ebp, EBP, temp_reg2);
+
+    MOV_MEM_TO_REG(txPtr_offset_ebp, EBP, txPtr.reg);
+    //POP(txPtr.reg);
+
+    MOV_MEM_TO_REG(0, txPtr.reg, pixel);
+    IMUL(temp_reg2, temp_reg1);
+    //we have already saved txPtr.reg
+    temp_reg3 = txPtr.reg;
+    MOV_REG_TO_REG(pixel, temp_reg3);
+    AND_REG_TO_REG(mask, temp_reg3);
+    if (adjust) {
+        if (round)
+            ADD_IMM_TO_REG(1<<(adjust-1), temp_reg1);
+        SHR(adjust, temp_reg1);
+    }
+    IMUL(temp_reg1, temp_reg3);
+    ADD_REG_TO_MEM(temp_reg3, EBP, dh_offset_ebp);
+    SHR(8, pixel);
+    AND_REG_TO_REG(mask, pixel);
+    IMUL(temp_reg1, pixel);
+    ADD_REG_TO_MEM(pixel, EBP, dl_offset_ebp);
+
+    // RT -> U*(1-V)
+    SUB_REG_TO_REG(temp_reg1, u);
+    mBuilderContext.Rctx = temp_reg2;
+    MOV_MEM_TO_REG(8, EBP, mBuilderContext.Rctx);
+    CONTEXT_LOAD(offset, generated_vars.rt);
+
+    MOV_MEM_TO_REG(txPtr_offset_ebp, EBP, txPtr.reg);
+    //POP(txPtr.reg);
+
+    //MOV_MEM_SCALE_TO_REG(txPtr.reg, offset, 1, temp_reg2);
+    ADD_REG_TO_REG(txPtr.reg, offset);
+    MOV_MEM_TO_REG(0, offset, temp_reg2);
+
+    MOV_REG_TO_REG(temp_reg2, pixel);
+    AND_REG_TO_REG(mask, temp_reg2);
+    IMUL(u, temp_reg2);
+    ADD_REG_TO_MEM(temp_reg2, EBP, dh_offset_ebp);
+    SHR(8, pixel);
+    AND_REG_TO_REG(mask, pixel);
+    IMUL(u, pixel);
+    ADD_REG_TO_MEM(pixel, EBP, dl_offset_ebp);
+    MOV_MEM_TO_REG(dh_offset_ebp, EBP, temp_reg1);
+    MOV_MEM_TO_REG(dl_offset_ebp, EBP, temp_reg2);
+    SHR(8, temp_reg1);
+    AND_REG_TO_REG(mask, temp_reg1);
+    SHL(8, mask);
+    AND_REG_TO_REG(mask, temp_reg2);
+    OR_REG_TO_REG(temp_reg1, temp_reg2);
+    MOV_REG_TO_MEM(temp_reg2, texel.offset_ebp, EBP);
+    scratches.recycle(u);
+    scratches.recycle(mask);
+    scratches.recycle(pixel);
+    scratches.recycle(txPtr.reg);
+    scratches.recycle(temp_reg1);
+    scratches.recycle(temp_reg2);
+
+}
+
+void GGLX86Assembler::build_texture_environment(
+    component_t& fragment,
+    fragment_parts_t& parts,
+    int component,
+    Scratch& regs)
+{
+    const uint32_t component_mask = 1<<component;
+    const bool multiTexture = mTextureMachine.activeUnits > 1;
+    Scratch scratches(registerFile());
+    for (int i=0 ; i<GGL_TEXTURE_UNIT_COUNT ; i++) {
+        texture_unit_t& tmu = mTextureMachine.tmu[i];
+
+        if (tmu.mask & component_mask) {
+            // replace or modulate with this texture
+            if ((tmu.replaced & component_mask) == 0) {
+                // not replaced by a later tmu...
+
+                pixel_t texel(parts.texel[i]);
+                if (multiTexture &&
+                        tmu.swrap == GGL_NEEDS_WRAP_11 &&
+                        tmu.twrap == GGL_NEEDS_WRAP_11)
+                {
+                    texel.reg = scratches.obtain();
+                    texel.flags |= CORRUPTIBLE;
+                    mCurSp = mCurSp - 4;
+                    texel.offset_ebp = mCurSp;
+                    comment("fetch texel (multitexture 1:1)");
+                    parts.coords[i].ptr.reg = scratches.obtain();
+                    MOV_MEM_TO_REG(parts.coords[i].ptr.offset_ebp, EBP, parts.coords[i].ptr.reg);
+                    load(parts.coords[i].ptr, texel, WRITE_BACK);
+                    MOV_REG_TO_MEM(texel.reg, texel.offset_ebp, EBP);
+                    scratches.recycle(parts.coords[i].ptr.reg);
+                } else {
+                    // the texel is already loaded in building textures
+                    texel.reg = scratches.obtain();
+                    MOV_MEM_TO_REG(texel.offset_ebp, EBP, texel.reg);
+                }
+
+                component_t incoming(fragment);
+                modify(fragment, regs);
+
+                switch (tmu.env) {
+                case GGL_REPLACE:
+                    extract(fragment, texel, component);
+                    break;
+                case GGL_MODULATE:
+                    modulate(fragment, incoming, texel, component);
+                    break;
+                case GGL_DECAL:
+                    decal(fragment, incoming, texel, component);
+                    break;
+                case GGL_BLEND:
+                    blend(fragment, incoming, texel, component, i);
+                    break;
+                case GGL_ADD:
+                    add(fragment, incoming, texel, component);
+                    break;
+                }
+                scratches.recycle(texel.reg);
+            }
+        }
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::wrapping(
+    int d,
+    int coord, int size,
+    int tx_wrap, int tx_linear, Scratch& scratches)
+{
+    // coord is recycled after return, so it can be written.
+    // notes:
+    // if tx_linear is set, we need 4 extra bits of precision on the result
+    // SMULL/UMULL is 3 cycles
+    // coord is actually s.reg or t.reg which will not be used
+    int c = coord;
+    if (tx_wrap == GGL_NEEDS_WRAP_REPEAT) {
+        // UMULL takes 4 cycles (interlocked), and we can get away with
+        // 2 cycles using SMULWB, but we're loosing 16 bits of precision
+        // out of 32 (this is not a problem because the iterator keeps
+        // its full precision)
+        // UMULL(AL, 0, size, d, c, size);
+        // note: we can't use SMULTB because it's signed.
+        MOV_REG_TO_REG(c, d);
+        SHR(16-tx_linear, d);
+        int temp_reg;
+        if(c != EDX)
+            temp_reg = c;
+        else {
+            temp_reg = scratches.obtain();
+            scratches.recycle(c);
+        }
+        int flag_push_edx = -1;
+        int flag_reserve_edx = -1;
+        int edx_offset_ebp = 0;
+        if(scratches.isUsed(EDX) == 1) { //not indicates that the registers are used up. Probably, previous allocated registers are recycled
+            if((d != EDX) && (size != EDX)) {
+                flag_push_edx = 1;
+                mCurSp = mCurSp - 4;
+                edx_offset_ebp = mCurSp;
+                MOV_REG_TO_MEM(EDX, edx_offset_ebp, EBP);
+                //PUSH(EDX);
+            }
+        }
+        else {
+            flag_reserve_edx = 1;
+            scratches.reserve(EDX);
+        }
+        if(scratches.isUsed(EAX)) {
+            if( size == EAX || d == EAX) {
+                // size is actually width and height, which will probably be used after wrapping
+                MOV_REG_TO_REG(size, temp_reg);
+                MOVSX_REG_TO_REG(OpndSize_16, size, size);
+                if(size == EAX)
+                    IMUL(d);
+                else
+                    IMUL(size);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, d);
+
+                MOV_REG_TO_REG(temp_reg, size);
+            }
+            else {
+                if(temp_reg != EAX)
+                    MOV_REG_TO_REG(EAX, temp_reg);
+                MOV_REG_TO_REG(size, EAX);
+                MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+                IMUL(d);
+                SHL(16, EDX);
+                SHR(16, EAX);
+                MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+                MOV_REG_TO_REG(EDX, d);
+                if(temp_reg != EAX)
+                    MOV_REG_TO_REG(temp_reg, EAX);
+            }
+        }
+        else {
+            MOV_REG_TO_REG(size, EAX);
+            MOVSX_REG_TO_REG(OpndSize_16, EAX, EAX);
+            IMUL(d);
+            SHL(16, EDX);
+            SHR(16, EAX);
+            MOV_REG_TO_REG(EAX, EDX, OpndSize_16);
+            MOV_REG_TO_REG(EDX, d);
+        }
+        if(flag_push_edx == 1) {
+            MOV_MEM_TO_REG(edx_offset_ebp, EBP, EDX);
+            //POP(EDX);
+        }
+        if(flag_reserve_edx ==1)
+            scratches.recycle(EDX);
+
+        scratches.recycle(temp_reg);
+        //IMUL(size, d) will cause segmentation fault with GlobalTime
+    } else if (tx_wrap == GGL_NEEDS_WRAP_CLAMP_TO_EDGE) {
+        if (tx_linear) {
+            // 1 cycle
+            MOV_REG_TO_REG(coord, d);
+            SAR(16-tx_linear, d);
+        } else {
+            SAR(16, coord);
+            MOV_REG_TO_REG(coord, d);
+            SAR(31, coord);
+            NOT(coord);
+            AND_REG_TO_REG(coord, d);
+
+            MOV_REG_TO_REG(size, coord);
+            SUB_IMM_TO_REG(1, coord);
+
+            CMP_REG_TO_REG(size, d);
+            CMOV_REG_TO_REG(Mnemonic_CMOVGE, coord, d);
+
+        }
+        scratches.recycle(coord);
+    }
+}
+
+// ---------------------------------------------------------------------------
+
+void GGLX86Assembler::modulate(
+    component_t& dest,
+    const component_t& incoming,
+    const pixel_t& incomingTexel, int component)
+{
+    Scratch locals(registerFile());
+    integer_t texel(locals.obtain(), 32, CORRUPTIBLE);
+    extract(texel, incomingTexel, component);
+
+    const int Nt = texel.size();
+    // Nt should always be less than 10 bits because it comes
+    // from the TMU.
+
+    int Ni = incoming.size();
+    // Ni could be big because it comes from previous MODULATEs
+
+    if (Nt == 1) {
+        // texel acts as a bit-mask
+        // dest = incoming & ((texel << incoming.h)-texel)
+        MOV_REG_TO_REG(texel.reg, dest.reg);
+        SHL(incoming.h, dest.reg);
+        SUB_REG_TO_REG(texel.reg, dest.reg);
+        dest.l = incoming.l;
+        dest.h = incoming.h;
+        dest.flags |= (incoming.flags & CLEAR_LO);
+    } else if (Ni == 1) {
+        SHL(31-incoming.h, incoming.reg);
+        MOV_REG_TO_REG(incoming.reg, dest.reg);
+        SAR(31, dest.reg);
+        AND_REG_TO_REG(texel.reg, dest.reg);
+        dest.l = 0;
+        dest.h = Nt;
+    } else {
+        int inReg = incoming.reg;
+        int shift = incoming.l;
+        if ((Nt + Ni) > 32) {
+            // we will overflow, reduce the precision of Ni to 8 bits
+            // (Note Nt cannot be more than 10 bits which happens with
+            // 565 textures and GGL_LINEAR)
+            shift += Ni-8;
+            Ni = 8;
+        }
+
+        // modulate by the component with the lowest precision
+        if (Nt >= Ni) {
+            if (shift) {
+                // XXX: we should be able to avoid this shift
+                // when shift==16 && Nt<16 && Ni<16, in which
+                // we could use SMULBT below.
+                MOV_REG_TO_REG(inReg, dest.reg);
+                SHR(shift, inReg);
+                inReg = dest.reg;
+                shift = 0;
+            }
+            int temp_reg = locals.obtain();
+            // operation:           (Cf*Ct)/((1<<Ni)-1)
+            // approximated with:   Cf*(Ct + Ct>>(Ni-1))>>Ni
+            // this operation doesn't change texel's size
+            MOV_REG_TO_REG(inReg, temp_reg);
+            SHR(Ni-1, temp_reg);
+            MOV_REG_TO_REG(inReg, dest.reg);
+            ADD_REG_TO_REG(temp_reg, dest.reg);
+            locals.recycle(temp_reg);
+            if (Nt<16 && Ni<16) {
+                MOVSX_REG_TO_REG(OpndSize_16, texel.reg, texel.reg);
+                MOVSX_REG_TO_REG(OpndSize_16, dest.reg, dest.reg);
+                IMUL(texel.reg, dest.reg);
+            }
+            else
+                IMUL(texel.reg, dest.reg);
+            dest.l = Ni;
+            dest.h = Nt + Ni;
+        } else {
+            if (shift && (shift != 16)) {
+                // if shift==16, we can use 16-bits mul instructions later
+                MOV_REG_TO_REG(inReg, dest.reg);
+                SHR(shift, dest.reg);
+                inReg = dest.reg;
+                shift = 0;
+            }
+            // operation:           (Cf*Ct)/((1<<Nt)-1)
+            // approximated with:   Ct*(Cf + Cf>>(Nt-1))>>Nt
+            // this operation doesn't change incoming's size
+            Scratch scratches(registerFile());
+            int temp_reg = locals.obtain();
+            int t = (texel.flags & CORRUPTIBLE) ? texel.reg : dest.reg;
+            if (t == inReg)
+                t = scratches.obtain();
+
+            MOV_REG_TO_REG(texel.reg, temp_reg);
+            SHR(Nt-1, temp_reg);
+            ADD_REG_TO_REG(temp_reg, texel.reg);
+            MOV_REG_TO_REG(texel.reg, t);
+            locals.recycle(temp_reg);
+            MOV_REG_TO_REG(inReg, dest.reg);
+            if (Nt<16 && Ni<16) {
+                if (shift==16) {
+                    MOVSX_REG_TO_REG(OpndSize_16, t, t);
+                    SHR(16, dest.reg);
+                    MOVSX_REG_TO_REG(OpndSize_16, dest.reg, dest.reg);
+                    IMUL(t, dest.reg);
+                }
+                else {
+                    MOVSX_REG_TO_REG(OpndSize_16, dest.reg, dest.reg);
+                    MOVSX_REG_TO_REG(OpndSize_16, t, t);
+                    IMUL(t, dest.reg);
+                }
+            } else
+                IMUL(t, dest.reg);
+            dest.l = Nt;
+            dest.h = Nt + Ni;
+        }
+
+        // low bits are not valid
+        dest.flags |= CLEAR_LO;
+
+        // no need to keep more than 8 bits/component
+        if (dest.size() > 8)
+            dest.l = dest.h-8;
+    }
+}
+
+void GGLX86Assembler::decal(
+    component_t& dest,
+    const component_t& incoming,
+    const pixel_t& incomingTexel, int component)
+{
+    // RGBA:
+    // Cv = Cf*(1 - At) + Ct*At = Cf + (Ct - Cf)*At
+    // Av = Af
+    Scratch locals(registerFile());
+    integer_t texel(locals.obtain(), 32, CORRUPTIBLE);
+    integer_t factor(locals.obtain(), 32, CORRUPTIBLE);
+    extract(texel, incomingTexel, component);
+    extract(factor, incomingTexel, GGLFormat::ALPHA);
+
+    // no need to keep more than 8-bits for decal
+    int Ni = incoming.size();
+    int shift = incoming.l;
+    if (Ni > 8) {
+        shift += Ni-8;
+        Ni = 8;
+    }
+    integer_t incomingNorm(incoming.reg, Ni, incoming.flags);
+    if (shift) {
+        SHR(shift, incomingNorm.reg);
+        MOV_REG_TO_REG(incomingNorm.reg, dest.reg);
+        incomingNorm.reg = dest.reg;
+        incomingNorm.flags |= CORRUPTIBLE;
+    }
+    int temp = locals.obtain();
+    MOV_REG_TO_REG(factor.reg, temp);
+    SHR(factor.s-1, temp);
+    ADD_REG_TO_REG(temp, factor.reg);
+    locals.recycle(temp);
+    build_blendOneMinusFF(dest, factor, incomingNorm, texel);
+}
+
+void GGLX86Assembler::blend(
+    component_t& dest,
+    const component_t& incoming,
+    const pixel_t& incomingTexel, int component, int tmu)
+{
+    // RGBA:
+    // Cv = (1 - Ct)*Cf + Ct*Cc = Cf + (Cc - Cf)*Ct
+    // Av = At*Af
+
+    if (component == GGLFormat::ALPHA) {
+        modulate(dest, incoming, incomingTexel, component);
+        return;
+    }
+
+    Scratch locals(registerFile());
+    int temp = locals.obtain();
+    integer_t color(locals.obtain(), 8, CORRUPTIBLE);
+    integer_t factor(locals.obtain(), 32, CORRUPTIBLE);
+    mBuilderContext.Rctx = temp;
+    MOV_MEM_TO_REG(8, PhysicalReg_EBP, mBuilderContext.Rctx);
+    MOVZX_MEM_TO_REG(OpndSize_8, mBuilderContext.Rctx, GGL_OFFSETOF(state.texture[tmu].env_color[component]), color.reg);
+    extract(factor, incomingTexel, component);
+
+    // no need to keep more than 8-bits for blend
+    int Ni = incoming.size();
+    int shift = incoming.l;
+    if (Ni > 8) {
+        shift += Ni-8;
+        Ni = 8;
+    }
+    integer_t incomingNorm(incoming.reg, Ni, incoming.flags);
+    if (shift) {
+        MOV_REG_TO_REG(incomingNorm.reg, dest.reg);
+        SHR(shift, dest.reg);
+        incomingNorm.reg = dest.reg;
+        incomingNorm.flags |= CORRUPTIBLE;
+    }
+    MOV_REG_TO_REG(factor.reg, temp);
+    SHR(factor.s-1, temp);
+    ADD_REG_TO_REG(temp, factor.reg);
+    locals.recycle(temp);
+    build_blendOneMinusFF(dest, factor, incomingNorm, color);
+}
+
+void GGLX86Assembler::add(
+    component_t& dest,
+    const component_t& incoming,
+    const pixel_t& incomingTexel, int component)
+{
+    // RGBA:
+    // Cv = Cf + Ct;
+    Scratch locals(registerFile());
+
+    component_t incomingTemp(incoming);
+
+    // use "dest" as a temporary for extracting the texel, unless "dest"
+    // overlaps "incoming".
+    integer_t texel(dest.reg, 32, CORRUPTIBLE);
+    if (dest.reg == incomingTemp.reg)
+        texel.reg = locals.obtain();
+    extract(texel, incomingTexel, component);
+
+    if (texel.s < incomingTemp.size()) {
+        expand(texel, texel, incomingTemp.size());
+    } else if (texel.s > incomingTemp.size()) {
+        if (incomingTemp.flags & CORRUPTIBLE) {
+            expand(incomingTemp, incomingTemp, texel.s);
+        } else {
+            incomingTemp.reg = locals.obtain();
+            expand(incomingTemp, incoming, texel.s);
+        }
+    }
+
+    if (incomingTemp.l) {
+        MOV_REG_TO_REG(incomingTemp.reg, dest.reg);
+        SHR(incomingTemp.l, dest.reg);
+        ADD_REG_TO_REG(texel.reg, dest.reg);
+    } else {
+        MOV_REG_TO_REG(incomingTemp.reg, dest.reg);
+        ADD_REG_TO_REG(texel.reg, dest.reg);
+    }
+    dest.l = 0;
+    dest.h = texel.size();
+    int temp_reg = locals.obtain();
+    component_sat(dest, temp_reg);
+    locals.recycle(temp_reg);
+}
+
+// ----------------------------------------------------------------------------
+
+}; // namespace android
+
diff --git a/libpixelflinger/pixelflinger.cpp b/libpixelflinger/pixelflinger.cpp
index ea5bc8e..fde6ed9 100644
--- a/libpixelflinger/pixelflinger.cpp
+++ b/libpixelflinger/pixelflinger.cpp
@@ -32,7 +32,11 @@
 #include "scanline.h"
 #include "trap.h"
 
+#if defined(__i386__)
+#include "codeflinger/x86/GGLX86Assembler.h"
+#else
 #include "codeflinger/GGLAssembler.h"
+#endif
 #include "codeflinger/CodeCache.h"
 
 #include <stdio.h> 
diff --git a/libpixelflinger/scanline.cpp b/libpixelflinger/scanline.cpp
index 26b9a3e..7b80580 100644
--- a/libpixelflinger/scanline.cpp
+++ b/libpixelflinger/scanline.cpp
@@ -34,7 +34,12 @@
 #include "scanline.h"
 
 #include "codeflinger/CodeCache.h"
+#if defined(__i386__)
+#include "codeflinger/x86/GGLX86Assembler.h"
+#include "codeflinger/x86/X86Assembler.h"
+#else
 #include "codeflinger/GGLAssembler.h"
+#endif
 #if defined(__arm__)
 #include "codeflinger/ARMAssembler.h"
 #elif defined(__aarch64__)
@@ -61,6 +66,8 @@
 
 #if defined(__arm__) || (defined(__mips__) && !defined(__LP64__)) || defined(__aarch64__)
 #   define ANDROID_ARM_CODEGEN  1
+#elif defined (__i386__)
+#   define ANDROID_IA32_CODEGEN 1
 #else
 #   define ANDROID_ARM_CODEGEN  0
 #endif
@@ -284,7 +291,7 @@ static  const needs_filter_t fill16noblend = {
 
 // ----------------------------------------------------------------------------
 
-#if ANDROID_ARM_CODEGEN
+#if ANDROID_ARM_CODEGEN || ANDROID_IA32_CODEGEN
 
 #if defined(__mips__)
 static CodeCache gCodeCache(32 * 1024);
@@ -316,7 +323,7 @@ void ggl_uninit_scanline(context_t* c)
 {
     if (c->state.buffers.coverage)
         free(c->state.buffers.coverage);
-#if ANDROID_ARM_CODEGEN
+#if ANDROID_ARM_CODEGEN || ANDROID_IA32_CODEGEN
     if (c->scanline_as)
         c->scanline_as->decStrong(c);
 #endif
@@ -436,6 +443,39 @@ static void pick_scanline(context_t* c)
     c->scanline_as = assembly.get();
     c->scanline_as->incStrong(c); //  hold on to assembly
     c->scanline = (void(*)(context_t* c))assembly->base();
+#elif ANDROID_IA32_CODEGEN
+    const AssemblyKey<needs_t> key(c->state.needs);
+    sp<Assembly> assembly = gCodeCache.lookup(key);
+    if (assembly == 0) {
+        // create a new assembly region
+        sp<ScanlineAssembly> a = new ScanlineAssembly(c->state.needs,
+                ASSEMBLY_SCRATCH_SIZE);
+        // initialize our assembler
+        GGLX86Assembler assembler( a );
+        // generate the scanline code for the given needs
+        int err = assembler.scanline(c->state.needs, c);
+        if (ggl_likely(!err)) {
+            // finally, cache this assembly
+            err = gCodeCache.cache(a->key(), a);
+        }
+        if (ggl_unlikely(err)) {
+            ALOGE("error generating or caching assembly. Reverting to NOP.  cache_err: %d \n", err);
+            c->scanline = scanline_noop;
+            c->init_y = init_y_noop;
+            c->step_y = step_y__nop;
+            return;
+        }
+        assembly = a;
+    }
+
+    // release the previous assembly
+    if (c->scanline_as) {
+        c->scanline_as->decStrong(c);
+    }
+
+    c->scanline_as = assembly.get();
+    c->scanline_as->incStrong(c); //  hold on to assembly
+    c->scanline = (void(*)(context_t* c))assembly->base();
 #else
 //    ALOGW("using generic (slow) pixel-pipeline");
     c->scanline = scanline;
@@ -464,7 +504,7 @@ static void blend_factor(context_t* c, pixel_t* r, uint32_t factor,
         const pixel_t* src, const pixel_t* dst);
 static void rescale(uint32_t& u, uint8_t& su, uint32_t& v, uint8_t& sv);
 
-#if ANDROID_ARM_CODEGEN && (ANDROID_CODEGEN == ANDROID_CODEGEN_GENERATED)
+#if (ANDROID_ARM_CODEGEN || ANDROID_IA32_CODEGEN) && (ANDROID_CODEGEN == ANDROID_CODEGEN_GENERATED)
 
 // no need to compile the generic-pipeline, it can't be reached
 void scanline(context_t*)
@@ -939,7 +979,7 @@ discard:
 	}
 }
 
-#endif // ANDROID_ARM_CODEGEN && (ANDROID_CODEGEN == ANDROID_CODEGEN_GENERATED)
+#endif // (ANDROID_ARM_CODEGEN || ANDROID_IA32_CODEGEN) && (ANDROID_CODEGEN == ANDROID_CODEGEN_GENERATED)
 
 // ----------------------------------------------------------------------------
 #if 0
diff --git a/libpixelflinger/tests/codegen/Android.mk b/libpixelflinger/tests/codegen/Android.mk
index bc07015..4bf15b4 100644
--- a/libpixelflinger/tests/codegen/Android.mk
+++ b/libpixelflinger/tests/codegen/Android.mk
@@ -1,8 +1,13 @@
 LOCAL_PATH:= $(call my-dir)
 include $(CLEAR_VARS)
 
+ifeq ($(TARGET_ARCH),x86)
 LOCAL_SRC_FILES:= \
-	codegen.cpp.arm
+    codegen.cpp
+else
+LOCAL_SRC_FILES:= \
+    codegen.cpp.arm
+endif
 
 LOCAL_SHARED_LIBRARIES := \
 	libcutils \
@@ -11,6 +16,11 @@ LOCAL_SHARED_LIBRARIES := \
 LOCAL_C_INCLUDES := \
 	system/core/libpixelflinger
 
+ifeq ($(TARGET_ARCH),x86)
+LOCAL_C_INCLUDES += \
+    $(TARGET_OUT_HEADERS)/libenc
+endif
+
 LOCAL_MODULE:= test-opengl-codegen
 
 LOCAL_MODULE_TAGS := tests
diff --git a/libpixelflinger/tests/codegen/codegen.cpp b/libpixelflinger/tests/codegen/codegen.cpp
index 46c1ccc..40ed5ab 100644
--- a/libpixelflinger/tests/codegen/codegen.cpp
+++ b/libpixelflinger/tests/codegen/codegen.cpp
@@ -7,13 +7,20 @@
 #include "scanline.h"
 
 #include "codeflinger/CodeCache.h"
+#if defined (__i386__)
+#include "codeflinger/x86/GGLX86Assembler.h"
+#include "codeflinger/x86/X86Assembler.h"
+#else
 #include "codeflinger/GGLAssembler.h"
 #include "codeflinger/ARMAssembler.h"
 #include "codeflinger/MIPSAssembler.h"
 #include "codeflinger/Arm64Assembler.h"
+#endif
 
 #if defined(__arm__) || defined(__mips__) || defined(__aarch64__)
 #   define ANDROID_ARM_CODEGEN  1
+#elif defined (__i386__)
+#   define ANDROID_IA32_CODEGEN 1
 #else
 #   define ANDROID_ARM_CODEGEN  0
 #endif
@@ -38,7 +45,6 @@ public:
 
 static void ggl_test_codegen(uint32_t n, uint32_t p, uint32_t t0, uint32_t t1)
 {
-#if ANDROID_ARM_CODEGEN
     GGLContext* c;
     gglInit(&c);
     needs_t needs;
@@ -46,6 +52,7 @@ static void ggl_test_codegen(uint32_t n, uint32_t p, uint32_t t0, uint32_t t1)
     needs.p = p;
     needs.t[0] = t0;
     needs.t[1] = t1;
+#if ANDROID_ARM_CODEGEN
     sp<ScanlineAssembly> a(new ScanlineAssembly(needs, ASSEMBLY_SCRATCH_SIZE));
 
 #if defined(__arm__)
@@ -64,10 +71,15 @@ static void ggl_test_codegen(uint32_t n, uint32_t p, uint32_t t0, uint32_t t1)
     if (err != 0) {
         printf("error %08x (%s)\n", err, strerror(-err));
     }
-    gglUninit(c);
-#else
-    printf("This test runs only on ARM, Arm64 or MIPS\n");
+#elif ANDROID_IA32_CODEGEN
+    sp<ScanlineAssembly> a(new ScanlineAssembly(needs, ASSEMBLY_SCRATCH_SIZE));
+    GGLX86Assembler assembler( a );
+    int err = assembler.scanline(needs, (context_t*)c);
+    if (err != 0) {
+        printf("error %08x (%s)\n", err, strerror(-err));
+    }
 #endif
+    gglUninit(c);
 }
 
 int main(int argc, char** argv)
